{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0017f312",
   "metadata": {
    "id": "0017f312"
   },
   "source": [
    "# SSVEP Character Classification with DiffE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "WnZeypG6gsYU",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "WnZeypG6gsYU",
    "outputId": "97ecbbad-d616-4794-b73d-184bb9e3a8a6"
   },
   "outputs": [],
   "source": [
    "#!pip install ema-pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "s8yKwaRCQxyM",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "s8yKwaRCQxyM",
    "outputId": "6db7da5b-c0d8-4691-9dd9-39dc83ae5a8e"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "#!git clone https://github.com/diffe2023/Diff-E.git\n",
    "#sys.path.append('/content/Diff-E')\n",
    "sys.path.append(r'C:\\LTI 11785 Introduction to deep learning\\project\\SSVEP\\Diff-E')\n",
    "sys.path.append(r'C:\\LTI 11785 Introduction to deep learning\\project\\SSVEP\\arl-eegmodels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4MhV2qw9hu-m",
   "metadata": {
    "id": "4MhV2qw9hu-m"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from einops import reduce\n",
    "from functools import partial\n",
    "import random\n",
    "import argparse\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchaudio.transforms as T\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from ema_pytorch import EMA\n",
    "#from tqdm.notebook import tqdm\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    f1_score,\n",
    "    roc_auc_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    top_k_accuracy_score,\n",
    ")\n",
    "\n",
    "from EEGModels import EEGNet, ShallowConvNet, DeepConvNet\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import KFold\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.initializers import RandomNormal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bc7c99e6-2046-4f2d-8323-813c144db7f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'Name': 'Yucheng Shao',\n",
    "    #'subset': 1,\n",
    "    'batch_size': 384,\n",
    "    'eeg_lr': 0.0012,\n",
    "    \n",
    "    'epochs_AEs': 500,\n",
    "    'alpha': 0,\n",
    "    'beta': 1,\n",
    "    'num_classes': 26,\n",
    "    'ddpm_dim': 128,\n",
    "    'encoder_dim': 256,\n",
    "    'fc_dim': 512,\n",
    "    'n_T': 1000,\n",
    "    'AE_base_lr': 9e-5,\n",
    "    'AE_max_lr': 1.5e-3,\n",
    "\n",
    "    'epochs_EEG': 500,\n",
    "    'eeg_F1': 16,\n",
    "    'eeg_F2': 32,\n",
    "    'eeg_D': 2,\n",
    "    'eeg_droupout': 0.2,\n",
    "    'eeg_channels': 64,\n",
    "    'eeg_samples': 528,\n",
    "    'eeg_kernel_length': 64,\n",
    "    'eeg_pool_1': 4,\n",
    "    'eeg_pool_2': 8,\n",
    "\n",
    "    'checkpoint_dir': r'C:\\LTI 11785 Introduction to deep learning\\project\\SSVEP\\checkpoints_separated',\n",
    "    'augument': False,\n",
    "    #'num_workers': 0\n",
    "    # Include other parameters as needed.\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "WjdWMaswQjkV",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "WjdWMaswQjkV",
    "outputId": "13515977-d235-41b3-aaa4-fef32a38c060"
   },
   "outputs": [],
   "source": [
    "# Uncomment this if you want to use Google Drive\n",
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Do2JYBiZZp1z",
   "metadata": {
    "id": "Do2JYBiZZp1z"
   },
   "source": [
    "#load and preprocess data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dPlrZ7Lez1XQ",
   "metadata": {
    "id": "dPlrZ7Lez1XQ"
   },
   "source": [
    "We retain the original SSVEP data preprocessing procedures, including:\n",
    "\n",
    "    Chebyshev bandpass filtering (6–90 Hz),\n",
    "    Noise removal and segmentation using a sliding window of 250 samples, followed by manual channel-wise standardization (subtracting the mean and dividing by the standard deviation),\n",
    "    Downsampling from 1000 Hz to 250 Hz.\n",
    "\n",
    "In this work, we directly load the preprocessed character-level files \"S#_chars.npy\" for subsequent training.\n",
    "\n",
    "Unlike the original setup where models are trained separately for each subject, we train a single model jointly across all subjects by concatenating all subjects' data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "FD4sHQTI0fua",
   "metadata": {
    "id": "FD4sHQTI0fua"
   },
   "outputs": [],
   "source": [
    "# Define a function to perform z-score normalization on the data\n",
    "def zscore_norm(data):\n",
    "    # Calculate the mean and standard deviation for each channel in each batch\n",
    "    mean = torch.mean(data, dim=(1, 2))\n",
    "    std = torch.std(data, dim=(1, 2))\n",
    "\n",
    "    # Subtract the mean from each channel in each batch and divide by the standard deviation\n",
    "    norm_data = (data - mean[:, None, None]) / std[:, None, None]\n",
    "\n",
    "    return norm_data\n",
    "\n",
    "\n",
    "# Define a function to perform min-max normalization on the data\n",
    "def minmax_norm(data):\n",
    "    # Calculate the minimum and maximum values for each channel and sequence in the batch\n",
    "    min_vals = torch.min(data, dim=-1)[0]\n",
    "    max_vals = torch.max(data, dim=-1)[0]\n",
    "\n",
    "    # Scale the data to the range [0, 1]\n",
    "    norm_data = (data - min_vals.unsqueeze(-1)) / (\n",
    "        max_vals.unsqueeze(-1) - min_vals.unsqueeze(-1)\n",
    "    )\n",
    "\n",
    "    return norm_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "kpv9Xa910cDi",
   "metadata": {
    "id": "kpv9Xa910cDi"
   },
   "outputs": [],
   "source": [
    "class EEGDataset(Dataset):\n",
    "    \"Characterizes a dataset for PyTorch\"\n",
    "\n",
    "    def __init__(self, X, Y, transform=None):\n",
    "        \"Initialization\"\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        \"Denotes the total number of samples\"\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"Generates one sample of data\"\n",
    "        # Load data and get label\n",
    "        x = self.X[index]\n",
    "        y = self.Y[index]\n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "        return x.squeeze(), y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7Z4LtRwCGwmm",
   "metadata": {
    "id": "7Z4LtRwCGwmm"
   },
   "outputs": [],
   "source": [
    "#define spectrogram\n",
    "#time_frames = 1 + (signal_length - n_fft) // hop_length\n",
    "#1 + (250 - 64) // 16 = 12 time frame\n",
    "spectrogram_transform = T.Spectrogram(\n",
    "    n_fft=64,       # width of window. original: 128, but then the time_frame will become 0 when downsampling\n",
    "    win_length=None, # default: n_fft\n",
    "    hop_length=16, # default: win_length // 2\n",
    "    normalized=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bpIIK-nqQEXL",
   "metadata": {
    "id": "bpIIK-nqQEXL"
   },
   "outputs": [],
   "source": [
    "def load_data_by_session(root_dir, subject_id, session_idx_list):\n",
    "    data = np.load(os.path.join(root_dir, f\"S{subject_id}_chars.npy\"))  # [26, 6, 64, 250]\n",
    "    data = data[:, session_idx_list]  # [26, len(session), 64, 250]\n",
    "    X = data.reshape(-1, 64, 250)  # [samples, 64, 250]\n",
    "    Y = np.repeat(np.arange(26), len(session_idx_list))\n",
    "\n",
    "    # transform to spectrogram\n",
    "    X_spectrogram = []\n",
    "    for sample in X:  # sample: [64, 250]\n",
    "        spec_per_channel = []\n",
    "        for ch in sample:\n",
    "            ch_tensor = torch.tensor(ch, dtype=torch.float32)\n",
    "            spec = spectrogram_transform(ch_tensor)  # [freq, time]\n",
    "            spec_per_channel.append(spec)\n",
    "        spec_tensor = torch.stack(spec_per_channel)  # [64, freq, time]\n",
    "        X_spectrogram.append(spec_tensor)\n",
    "\n",
    "    X_spectrogram = torch.stack(X_spectrogram)  # [samples, 64, freq, time]\n",
    "    Y = torch.tensor(Y, dtype=torch.long)\n",
    "\n",
    "    print(\"X shape after Spectrogram:\", X_spectrogram.shape)  # [B, 64, freq≈33, time≈12]\n",
    "\n",
    "    return X_spectrogram, Y\n",
    "\n",
    "\n",
    "def load_split_dataset(root_dir, num_seen=25, seed=43, cache_dir='processed_by_session'):\n",
    "    os.makedirs(cache_dir, exist_ok=True)\n",
    "    random.seed(seed)\n",
    "\n",
    "    all_subjects = list(range(1, 36))\n",
    "    seen_subjects = random.sample(all_subjects, num_seen)#25 seen\n",
    "    unseen_subjects = [sid for sid in all_subjects if sid not in seen_subjects]#10 unseen\n",
    "\n",
    "    split_cfg = {\n",
    "        \"train\":  [(sid, [0, 1, 2, 3]) for sid in seen_subjects],\n",
    "        \"val\":    [(sid, [4]) for sid in seen_subjects],\n",
    "        \"test1\":  [(sid, [5]) for sid in seen_subjects],\n",
    "        \"test2\":  [(sid, [0, 1, 2, 3, 4, 5]) for sid in unseen_subjects]\n",
    "    }\n",
    "\n",
    "    print(f\"[Split] Seen subjects (train/val/test1): {seen_subjects}\")\n",
    "    print(f\"[Split] Unseen subjects (test2): {unseen_subjects}\")\n",
    "\n",
    "    loaders = {}\n",
    "    for split, sid_sess in split_cfg.items():\n",
    "        X_all, Y_all = [], []\n",
    "        for sid, sess in sid_sess:\n",
    "            sess_str = ''.join(str(s) for s in sess)\n",
    "            \n",
    "            cache_X_path = os.path.join(cache_dir, f\"S{sid:02d}_sess_{sess_str}_X.npy\")\n",
    "            cache_Y_path = os.path.join(cache_dir, f\"S{sid:02d}_sess_{sess_str}_Y.npy\")\n",
    "\n",
    "            if os.path.exists(cache_X_path) and os.path.exists(cache_Y_path):\n",
    "                X = torch.tensor(np.load(cache_X_path))\n",
    "                Y = torch.tensor(np.load(cache_Y_path))\n",
    "                print(f\"[Cache] Loaded S{sid:02d} sessions {sess_str} from cache\")\n",
    "            else:\n",
    "                X, Y = load_data_by_session(root_dir, sid, sess)\n",
    "                np.save(cache_X_path, X.cpu().numpy())\n",
    "                np.save(cache_Y_path, Y.cpu().numpy())\n",
    "                print(f\"[Cache] Saved S{sid:02d} sessions {sess_str} to cache\")\n",
    "                \n",
    "            X_all.append(X)\n",
    "            Y_all.append(Y)\n",
    "        X_all = torch.cat(X_all, dim=0)\n",
    "        Y_all = torch.cat(Y_all, dim=0)\n",
    "        dataset = EEGDataset(X_all, Y_all)\n",
    "        loaders[split] = DataLoader(dataset, batch_size=32, shuffle=(split == \"train\"))\n",
    "    return loaders\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "R1jZy_mPsxNI",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R1jZy_mPsxNI",
    "outputId": "7f6b04f7-34a4-481b-e459-92f8db552739"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Seed:  44\n"
     ]
    }
   ],
   "source": [
    "seed = 44\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "print(\"Random Seed: \", seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "seWeK5dArjjM",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "seWeK5dArjjM",
    "outputId": "cabe5a3f-7cd8-440a-a870-5fa634af8a8d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Split] Seen subjects (train/val/test1): [27, 34, 8, 12, 13, 33, 10, 1, 30, 4, 19, 28, 26, 6, 17, 29, 31, 25, 11, 32]\n",
      "[Split] Unseen subjects (test2): [2, 3, 5, 7, 9, 14, 15, 16, 18, 20, 21, 22, 23, 24, 35]\n",
      "[Cache] Loaded S27 sessions 0123 from cache\n",
      "[Cache] Loaded S34 sessions 0123 from cache\n",
      "[Cache] Loaded S08 sessions 0123 from cache\n",
      "[Cache] Loaded S12 sessions 0123 from cache\n",
      "[Cache] Loaded S13 sessions 0123 from cache\n",
      "[Cache] Loaded S33 sessions 0123 from cache\n",
      "[Cache] Loaded S10 sessions 0123 from cache\n",
      "[Cache] Loaded S01 sessions 0123 from cache\n",
      "[Cache] Loaded S30 sessions 0123 from cache\n",
      "[Cache] Loaded S04 sessions 0123 from cache\n",
      "[Cache] Loaded S19 sessions 0123 from cache\n",
      "[Cache] Loaded S28 sessions 0123 from cache\n",
      "[Cache] Loaded S26 sessions 0123 from cache\n",
      "[Cache] Loaded S06 sessions 0123 from cache\n",
      "[Cache] Loaded S17 sessions 0123 from cache\n",
      "[Cache] Loaded S29 sessions 0123 from cache\n",
      "[Cache] Loaded S31 sessions 0123 from cache\n",
      "[Cache] Loaded S25 sessions 0123 from cache\n",
      "[Cache] Loaded S11 sessions 0123 from cache\n",
      "[Cache] Loaded S32 sessions 0123 from cache\n",
      "[Cache] Loaded S27 sessions 4 from cache\n",
      "[Cache] Loaded S34 sessions 4 from cache\n",
      "[Cache] Loaded S08 sessions 4 from cache\n",
      "[Cache] Loaded S12 sessions 4 from cache\n",
      "[Cache] Loaded S13 sessions 4 from cache\n",
      "[Cache] Loaded S33 sessions 4 from cache\n",
      "[Cache] Loaded S10 sessions 4 from cache\n",
      "[Cache] Loaded S01 sessions 4 from cache\n",
      "[Cache] Loaded S30 sessions 4 from cache\n",
      "[Cache] Loaded S04 sessions 4 from cache\n",
      "[Cache] Loaded S19 sessions 4 from cache\n",
      "[Cache] Loaded S28 sessions 4 from cache\n",
      "[Cache] Loaded S26 sessions 4 from cache\n",
      "[Cache] Loaded S06 sessions 4 from cache\n",
      "[Cache] Loaded S17 sessions 4 from cache\n",
      "[Cache] Loaded S29 sessions 4 from cache\n",
      "[Cache] Loaded S31 sessions 4 from cache\n",
      "[Cache] Loaded S25 sessions 4 from cache\n",
      "[Cache] Loaded S11 sessions 4 from cache\n",
      "[Cache] Loaded S32 sessions 4 from cache\n",
      "[Cache] Loaded S27 sessions 5 from cache\n",
      "[Cache] Loaded S34 sessions 5 from cache\n",
      "[Cache] Loaded S08 sessions 5 from cache\n",
      "[Cache] Loaded S12 sessions 5 from cache\n",
      "[Cache] Loaded S13 sessions 5 from cache\n",
      "[Cache] Loaded S33 sessions 5 from cache\n",
      "[Cache] Loaded S10 sessions 5 from cache\n",
      "[Cache] Loaded S01 sessions 5 from cache\n",
      "[Cache] Loaded S30 sessions 5 from cache\n",
      "[Cache] Loaded S04 sessions 5 from cache\n",
      "[Cache] Loaded S19 sessions 5 from cache\n",
      "[Cache] Loaded S28 sessions 5 from cache\n",
      "[Cache] Loaded S26 sessions 5 from cache\n",
      "[Cache] Loaded S06 sessions 5 from cache\n",
      "[Cache] Loaded S17 sessions 5 from cache\n",
      "[Cache] Loaded S29 sessions 5 from cache\n",
      "[Cache] Loaded S31 sessions 5 from cache\n",
      "[Cache] Loaded S25 sessions 5 from cache\n",
      "[Cache] Loaded S11 sessions 5 from cache\n",
      "[Cache] Loaded S32 sessions 5 from cache\n",
      "[Cache] Loaded S02 sessions 012345 from cache\n",
      "[Cache] Loaded S03 sessions 012345 from cache\n",
      "[Cache] Loaded S05 sessions 012345 from cache\n",
      "[Cache] Loaded S07 sessions 012345 from cache\n",
      "[Cache] Loaded S09 sessions 012345 from cache\n",
      "[Cache] Loaded S14 sessions 012345 from cache\n",
      "[Cache] Loaded S15 sessions 012345 from cache\n",
      "[Cache] Loaded S16 sessions 012345 from cache\n",
      "[Cache] Loaded S18 sessions 012345 from cache\n",
      "[Cache] Loaded S20 sessions 012345 from cache\n",
      "[Cache] Loaded S21 sessions 012345 from cache\n",
      "[Cache] Loaded S22 sessions 012345 from cache\n",
      "[Cache] Loaded S23 sessions 012345 from cache\n",
      "[Cache] Loaded S24 sessions 012345 from cache\n",
      "[Cache] Loaded S35 sessions 012345 from cache\n"
     ]
    }
   ],
   "source": [
    "#subject_chars_dir = '/content/drive/MyDrive/project/dataset/ssvep/chars' # Where to store all character data\n",
    "subject_chars_dir = r'C:\\LTI 11785 Introduction to deep learning\\project\\SSVEP\\chars' # Where to store all character data\n",
    "subject_ids = list(range(1, 36)) #35 samples + 1\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "batch_size = 256\n",
    "batch_size2 = 256\n",
    "seed = 44\n",
    "shuffle = True\n",
    "\n",
    "# load all subjects' data\n",
    "loaders = load_split_dataset(subject_chars_dir, num_seen=20, seed=seed)\n",
    "train_loader = loaders[\"train\"]\n",
    "val_loader   = loaders[\"val\"]\n",
    "test1_loader = loaders[\"test1\"]\n",
    "test2_loader = loaders[\"test2\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f0JmYf8_7foW",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f0JmYf8_7foW",
    "outputId": "451a29b4-453b-4649-e095-e829c058ff1b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2080 520 520 2340\n",
      "X shape: torch.Size([32, 64, 33, 16])\n",
      "Y shape: torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "print(len(train_loader.dataset), len(val_loader.dataset), len(test1_loader.dataset), len(test2_loader.dataset))\n",
    "\n",
    "for x, y in train_loader:\n",
    "    print(\"X shape:\", x.shape)\n",
    "    print(\"Y shape:\", y.shape)\n",
    "    break  # the first batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "zuXPlL3nol6U",
   "metadata": {
    "id": "zuXPlL3nol6U"
   },
   "source": [
    "train: 20 seen subjects × 4 session × 26 = 2080\n",
    "\n",
    "val: 20 × 1 × 26 = 520\n",
    "\n",
    "test1: 20 × 1 × 26 = 520\n",
    "\n",
    "test2: 15 unseen × 6 session × 26 = 2340"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tEjThr9xpong",
   "metadata": {
    "id": "tEjThr9xpong"
   },
   "source": [
    "random choose 25 subjects as seen, the rest 10 as unseen\n",
    "\n",
    "| Group Name | Subject Range   | Sessions Used         | Purpose                                  |\n",
    "|------------|------------------|------------------------|-------------------------------------------|\n",
    "| train      | seen subjects    | sessions [0, 1, 2, 3]  | Training (4 sessions)                     |\n",
    "| val        | seen subjects    | session [4]            | Validation                                |\n",
    "| test1      | seen subjects    | session [5]            | Test on unseen session of seen subjects  |\n",
    "| test2      | unseen subjects  | sessions [0–5] (all)   | Test on unseen subjects                   |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "D-8XtejnbSv2",
   "metadata": {
    "id": "D-8XtejnbSv2"
   },
   "source": [
    "#model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mV-wGFCs2hpy",
   "metadata": {
    "id": "mV-wGFCs2hpy"
   },
   "source": [
    "The overall model follows the original GitHub implementation, with a few necessary adjustments to align tensor time dimensions. Specifically, we aligned the temporal dimensions between up2 + temb and down1, up3 and x, and x_hat and x. These changes are clearly marked with comments in the code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "M67sftFNJGiL",
   "metadata": {
    "id": "M67sftFNJGiL"
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "```\n",
    "#transform from 1D to 2D:\n",
    "current input: [batch_size, channels, freq_bins, time_frames]\n",
    "original input: [batch_size, channels, time]\n",
    "\n",
    "Replace all instances of Conv1d, MaxPool1d, and AvgPool1d with Conv2d, MaxPool2d, and AvgPool2d, respectively.\n",
    "Also modify the AvgPool1d and MaxPool1d used in the Encoder and Decoder.\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "iIaSnHewQDNY",
   "metadata": {
    "id": "iIaSnHewQDNY"
   },
   "outputs": [],
   "source": [
    "def safe_cat_and_add(a, b, temb):\n",
    "    \"\"\"\n",
    "    Interpolates `a` to align with `b`, then adds `temb` (broadcasted) to `a`, and finally concatenates with `b`.\n",
    "\n",
    "    Inputs:\n",
    "        a: decoder upsampled output [B, C, F, T]\n",
    "        b: encoder skip connection [B, C, F, T]\n",
    "        temb: time embedding output, supports [B, C], [B, C, 1], etc., auto-adapts shape\n",
    "\n",
    "    Output:\n",
    "        Concatenated result [B, C_concat, F, T]\n",
    "    \"\"\"\n",
    "    # Align spatial dimensions (freq, time)\n",
    "    if a.shape[2:] != b.shape[2:]:\n",
    "        a = F.interpolate(a, size=b.shape[2:], mode='nearest')\n",
    "\n",
    "    # Normalize temb to shape [B, C, 1, 1]\n",
    "    while temb.dim() < 4:\n",
    "        temb = temb.unsqueeze(-1)\n",
    "    if temb.dim() > 4:\n",
    "        temb = temb.squeeze(-1)\n",
    "\n",
    "    # Broadcast to shape [B, C, F, T]\n",
    "    temb = temb.expand(-1, -1, a.shape[2], a.shape[3])\n",
    "\n",
    "    # Element-wise addition and concatenation\n",
    "    return torch.cat([a + temb, b], dim=1)\n",
    "\n",
    "def safe_align_2d(x1, x2):\n",
    "    \"\"\"\n",
    "    Align two 4D tensors in frequency and time dimensions (2D spatial alignment).\n",
    "\n",
    "    Inputs:\n",
    "        x1, x2: [B, C, F, T]\n",
    "\n",
    "    Returns:\n",
    "        Aligned x1, x2\n",
    "    \"\"\"\n",
    "    target_freq = min(x1.shape[2], x2.shape[2])\n",
    "    target_time = min(x1.shape[3], x2.shape[3])\n",
    "    x1 = F.interpolate(x1, size=(target_freq, target_time), mode='nearest')\n",
    "    x2 = F.interpolate(x2, size=(target_freq, target_time), mode='nearest')\n",
    "    return x1, x2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ceb479ce",
   "metadata": {
    "id": "ceb479ce"
   },
   "outputs": [],
   "source": [
    "#define diffE model\n",
    "def get_padding(kernel_size, dilation=1):\n",
    "    return int((kernel_size * dilation - dilation) / 2)\n",
    "\n",
    "\n",
    "# Swish activation function\n",
    "class Swish(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x * self.sigmoid(x)\n",
    "\n",
    "\n",
    "class SinusoidalPosEmb(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        device = x.device\n",
    "        half_dim = self.dim // 2\n",
    "        emb = math.log(10000) / (half_dim - 1)\n",
    "        emb = torch.exp(torch.arange(half_dim, device=device) * -emb)\n",
    "        emb = x[:, None] * emb[None, :]\n",
    "        emb = torch.cat((emb.sin(), emb.cos()), dim=-1)\n",
    "        return emb\n",
    "\n",
    "\n",
    "class WeightStandardizedConv2d(nn.Conv2d):\n",
    "    def forward(self, x):\n",
    "        eps = 1e-5 if x.dtype == torch.float32 else 1e-3\n",
    "        weight = self.weight\n",
    "        mean = weight.mean(dim=(1,2,3), keepdim=True)\n",
    "        var = weight.var(dim=(1,2,3), keepdim=True, unbiased=False)\n",
    "        weight = (weight - mean) / (var + eps).sqrt()\n",
    "        return F.conv2d(x, weight, self.bias, self.stride, self.padding, self.dilation, self.groups)\n",
    "\n",
    "\n",
    "class ResidualConvBlock(nn.Module):\n",
    "    def __init__(self, inc, outc, kernel_size, stride=1, gn=8):\n",
    "        super().__init__()\n",
    "        self.same_channels = inc == outc\n",
    "        padding = kernel_size // 2\n",
    "        self.conv = nn.Sequential(\n",
    "            WeightStandardizedConv2d(inc, outc, kernel_size, stride, padding),\n",
    "            nn.GroupNorm(gn, outc),\n",
    "            nn.PReLU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.conv(x)\n",
    "        return (x + x1) / 2 if self.same_channels else x1\n",
    "\n",
    "\n",
    "class UnetDown(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, gn=8, factor=2):\n",
    "        super().__init__()\n",
    "        self.pool = nn.MaxPool2d(kernel_size=factor)\n",
    "        self.layer = ResidualConvBlock(in_channels, out_channels, kernel_size, gn=gn)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer(x)\n",
    "        return self.pool(x)\n",
    "\n",
    "\n",
    "class UnetUp(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, gn=8, factor=2):\n",
    "        super().__init__()\n",
    "        self.upsample = nn.Upsample(scale_factor=factor, mode='nearest')\n",
    "        self.layer = ResidualConvBlock(in_channels, out_channels, kernel_size, gn=gn)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layer(self.upsample(x))\n",
    "\n",
    "\n",
    "class EmbedFC(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim):\n",
    "        super(EmbedFC, self).__init__()\n",
    "        \"\"\"\n",
    "        generic one layer FC NN for embedding things\n",
    "        \"\"\"\n",
    "        self.input_dim = input_dim\n",
    "        layers = [\n",
    "            nn.Linear(input_dim, emb_dim),\n",
    "            nn.PReLU(),\n",
    "            nn.Linear(emb_dim, emb_dim),\n",
    "        ]\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, self.input_dim)\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "class ConditionalUNet(nn.Module):\n",
    "    def __init__(self, in_channels, n_feat=256):\n",
    "        super(ConditionalUNet, self).__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.n_feat = n_feat\n",
    "\n",
    "        self.d1_out = n_feat * 1\n",
    "        self.d2_out = n_feat * 2\n",
    "        self.d3_out = n_feat * 3\n",
    "        self.d4_out = n_feat * 4\n",
    "\n",
    "        self.u1_out = n_feat\n",
    "        self.u2_out = n_feat\n",
    "        self.u3_out = n_feat\n",
    "        self.u4_out = in_channels\n",
    "\n",
    "        self.sin_emb = SinusoidalPosEmb(n_feat)\n",
    "        # self.timeembed1 = EmbedFC(n_feat, self.u1_out)\n",
    "        # self.timeembed2 = EmbedFC(n_feat, self.u2_out)\n",
    "        # self.timeembed3 = EmbedFC(n_feat, self.u3_out)\n",
    "\n",
    "        self.down1 = UnetDown(in_channels, self.d1_out, 1, gn=8, factor=2)\n",
    "        self.down2 = UnetDown(self.d1_out, self.d2_out, 1, gn=8, factor=2)\n",
    "        self.down3 = UnetDown(self.d2_out, self.d3_out, 1, gn=8, factor=2)\n",
    "\n",
    "        self.up2 = UnetUp(self.d3_out, self.u2_out, 1, gn=8, factor=2)\n",
    "        self.up3 = UnetUp(self.u2_out + self.d2_out, self.u3_out, 1, gn=8, factor=2)\n",
    "        self.up4 = UnetUp(self.u3_out + self.d1_out, self.u4_out, 1, gn=8, factor=2)\n",
    "        self.out = nn.Conv2d(self.u4_out + in_channels, in_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        down1 = self.down1(x)  # 2000 -> 1000\n",
    "        down2 = self.down2(down1)  # 1000 -> 500\n",
    "        down3 = self.down3(down2)  # 500 -> 250\n",
    "\n",
    "        temb = self.sin_emb(t).view(-1, self.n_feat, 1)  # [b, n_feat, 1]\n",
    "\n",
    "        up1 = self.up2(down3)  # 250 -> 500\n",
    "        #up2 = self.up3(torch.cat([up1 + temb, down2], 1))  # 500 -> 1000\n",
    "        up2 = self.up3(safe_cat_and_add(up1, down2, temb))\n",
    "\n",
    "        #up3 = self.up4(torch.cat([up2 + temb, down1], 1))  # 1000 -> 2000\n",
    "        up3 = self.up4(safe_cat_and_add(up2, down1, temb))\n",
    "\n",
    "        # Align the temporal dimension of up3 and x\n",
    "        if up3.shape[-1] != x.shape[-1]:\n",
    "            target_len = min(up3.shape[-1], x.shape[-1])\n",
    "            up3 = F.interpolate(up3, size=target_len)\n",
    "            x = F.interpolate(x, size=target_len)\n",
    "\n",
    "        #out = self.out(torch.cat([up3, x], 1))  # 2000 -> 2000\n",
    "        if x.shape[2:] != up3.shape[2:]:\n",
    "            x = F.interpolate(x, size=up3.shape[2:], mode='nearest')\n",
    "\n",
    "        out = self.out(torch.cat([up3, x], 1))\n",
    "\n",
    "        down = (down1, down2, down3)\n",
    "        up = (up1, up2, up3)\n",
    "        return out, down, up\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, in_channels, dim=512):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.e1_out = dim\n",
    "        self.e2_out = dim\n",
    "        self.e3_out = dim\n",
    "\n",
    "        self.down1 = UnetDown(in_channels, self.e1_out, 1, gn=8, factor=2)\n",
    "        self.down2 = UnetDown(self.e1_out, self.e2_out, 1, gn=8, factor=2)\n",
    "        self.down3 = UnetDown(self.e2_out, self.e3_out, 1, gn=8, factor=2)\n",
    "\n",
    "        self.avg_pooling = nn.AdaptiveAvgPool2d((1,1))\n",
    "        self.max_pooling = nn.AdaptiveMaxPool2d((1,1))\n",
    "        self.act = nn.Tanh()\n",
    "\n",
    "    def forward(self, x0):\n",
    "        # Down sampling\n",
    "        dn1 = self.down1(x0)  # 2048 -> 1024\n",
    "        dn2 = self.down2(dn1)  # 1024 -> 512\n",
    "        dn3 = self.down3(dn2)  # 512 -> 256\n",
    "        z = self.avg_pooling(dn3).view(-1, self.e3_out)  # [b, features]\n",
    "        down = (dn1, dn2, dn3)\n",
    "        out = (down, z)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, in_channels, n_feat=256, encoder_dim=512, n_classes=config['num_classes']):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.n_feat = n_feat\n",
    "        self.n_classes = n_classes\n",
    "        self.e1_out = encoder_dim\n",
    "        self.e2_out = encoder_dim\n",
    "        self.e3_out = encoder_dim\n",
    "        self.d1_out = n_feat\n",
    "        self.d2_out = n_feat * 2\n",
    "        self.d3_out = n_feat * 3\n",
    "        self.u1_out = n_feat\n",
    "        self.u2_out = n_feat\n",
    "        self.u3_out = n_feat\n",
    "        self.u4_out = in_channels\n",
    "\n",
    "        # self.sin_emb = SinusoidalPosEmb(n_feat)\n",
    "        # self.timeembed1 = EmbedFC(n_feat, self.e3_out)\n",
    "        # self.timeembed2 = EmbedFC(n_feat, self.u2_out)\n",
    "        # self.timeembed3 = EmbedFC(n_feat, self.u3_out)\n",
    "        # self.contextembed1 = EmbedFC(self.e3_out, self.e3_out)\n",
    "        # self.contextembed2 = EmbedFC(self.e3_out, self.u2_out)\n",
    "        # self.contextembed3 = EmbedFC(self.e3_out, self.u3_out)\n",
    "\n",
    "        # Unet up sampling\n",
    "        self.up1 = UnetUp(self.d3_out + self.e3_out, self.u2_out, 1, gn=8, factor=2)\n",
    "        self.up2 = UnetUp(self.d2_out + self.u2_out, self.u3_out, 1, gn=8, factor=2)\n",
    "        self.up3 = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2, mode=\"nearest\"),\n",
    "            nn.Conv2d(self.d1_out + self.u3_out + in_channels * 2, in_channels, kernel_size=1),\n",
    "        )\n",
    "\n",
    "        # self.out = nn.Conv1d(self.u4_out+in_channels, in_channels, 1)\n",
    "        self.pool = nn.AvgPool2d(kernel_size=2)\n",
    "\n",
    "    def forward(self, x0, encoder_out, diffusion_out):\n",
    "        # Encoder output\n",
    "        down, z = encoder_out\n",
    "        dn1, dn2, dn3 = down\n",
    "\n",
    "        # DDPM output\n",
    "        x_hat, down_ddpm, up, t = diffusion_out\n",
    "        dn11, dn22, dn33 = down_ddpm\n",
    "\n",
    "        # embed context, time step\n",
    "        # temb = self.sin_emb(t).view(-1, self.n_feat, 1) # [b, n_feat, 1]\n",
    "        # temb1 = self.timeembed1(temb).view(-1, self.e3_out, 1) # [b, features]\n",
    "        # temb2 = self.timeembed2(temb).view(-1, self.u2_out, 1) # [b, features]\n",
    "        # temb3 = self.timeembed3(temb).view(-1, self.u3_out, 1) # [b, features]\n",
    "        # ct2 = self.contextembed2(z).view(-1, self.u2_out, 1) # [b, n_feat, 1]\n",
    "        # ct3 = self.contextembed3(z).view(-1, self.u3_out, 1) # [b, n_feat, 1]\n",
    "\n",
    "        # Up sampling\n",
    "        up1 = self.up1(torch.cat([dn3, dn33.detach()], 1))\n",
    "        up2 = self.up2(torch.cat([up1, dn22.detach()], 1))\n",
    "        out = self.up3(\n",
    "            torch.cat([self.pool(x0), self.pool(x_hat.detach()), up2, dn11.detach()], 1)\n",
    "        )\n",
    "        return out\n",
    "\n",
    "\n",
    "class DiffE(nn.Module):\n",
    "    def __init__(self, encoder, decoder, fc):\n",
    "        super(DiffE, self).__init__()\n",
    "\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.fc = fc\n",
    "\n",
    "    def forward(self, x0, ddpm_out):\n",
    "        encoder_out = self.encoder(x0)\n",
    "        decoder_out = self.decoder(x0, encoder_out, ddpm_out)\n",
    "        fc_out = self.fc(encoder_out[1])\n",
    "        return decoder_out, fc_out\n",
    "\n",
    "\n",
    "class DecoderNoDiff(nn.Module):\n",
    "    def __init__(self, in_channels, n_feat=256, encoder_dim=512, n_classes=config['num_classes']):\n",
    "        super(DecoderNoDiff, self).__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.n_feat = n_feat\n",
    "        self.n_classes = n_classes\n",
    "        self.e1_out = encoder_dim\n",
    "        self.e2_out = encoder_dim\n",
    "        self.e3_out = encoder_dim\n",
    "        self.u1_out = n_feat\n",
    "        self.u2_out = n_feat\n",
    "        self.u3_out = n_feat\n",
    "        self.u4_out = n_feat\n",
    "\n",
    "        self.sin_emb = SinusoidalPosEmb(n_feat)\n",
    "        self.timeembed1 = EmbedFC(n_feat, self.e3_out)\n",
    "        self.timeembed2 = EmbedFC(n_feat, self.u2_out)\n",
    "        self.timeembed3 = EmbedFC(n_feat, self.u3_out)\n",
    "        self.contextembed1 = EmbedFC(self.e3_out, self.e3_out)\n",
    "        self.contextembed2 = EmbedFC(self.e3_out, self.u2_out)\n",
    "        self.contextembed3 = EmbedFC(self.e3_out, self.u3_out)\n",
    "\n",
    "        # Unet up sampling\n",
    "        self.up2 = UnetUp(self.e3_out, self.u2_out, 1, gn=8, factor=2)\n",
    "        self.up3 = UnetUp(self.e2_out + self.u2_out, self.u3_out, 1, gn=8, factor=2)\n",
    "        # self.up4 = UnetUp(self.e1_out+self.u3_out, self.u4_out, 1, 1, gn=in_channels, factor=2, is_res=True)\n",
    "        self.up4 = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=(2, 1), mode=\"nearest\"),  # freq\n",
    "            nn.Conv2d(self.u3_out + self.e1_out + in_channels, in_channels, kernel_size=1)\n",
    "        )\n",
    "\n",
    "        self.out = nn.Conv2d(self.u4_out, in_channels, kernel_size=1)\n",
    "        self.pool = nn.AvgPool2d(kernel_size=(2, 1))  #  freq\n",
    "\n",
    "    def forward(self, x0, x_hat, encoder_out, t):\n",
    "        down, z = encoder_out\n",
    "        dn1, dn2, dn3 = down\n",
    "        tembd = self.sin_emb(t).view(-1, self.n_feat, 1)  # [b, n_feat, 1]\n",
    "        tembd1 = self.timeembed1(self.sin_emb(t)).view(\n",
    "            -1, self.e3_out, 1\n",
    "        )  # [b, n_feat, 1]\n",
    "        tembd2 = self.timeembed2(self.sin_emb(t)).view(\n",
    "            -1, self.u2_out, 1\n",
    "        )  # [b, n_feat, 1]\n",
    "        tembd3 = self.timeembed3(self.sin_emb(t)).view(\n",
    "            -1, self.u3_out, 1\n",
    "        )  # [b, n_feat, 1]\n",
    "\n",
    "        # Up sampling\n",
    "        ddpm_loss = F.l1_loss(x0, x_hat, reduction=\"none\")\n",
    "\n",
    "        up2 = self.up2(dn3)  # 256 -> 512\n",
    "        up3 = self.up3(torch.cat([up2, dn2], 1))  # 512 -> 1024\n",
    "        out = self.up4(\n",
    "            torch.cat([self.pool(x0), self.pool(x_hat), up3, dn1], 1)\n",
    "        )  # 1024 -> 2048\n",
    "        # out = self.out(torch.cat([out, x_hat], 1)) # 2048 -> 2048\n",
    "        # out = self.out(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class LinearClassifier(nn.Module):\n",
    "    def __init__(self, in_dim, latent_dim, emb_dim):\n",
    "        super().__init__()\n",
    "        self.linear_out = nn.Sequential(\n",
    "            nn.Linear(in_features=in_dim, out_features=latent_dim),\n",
    "            nn.GroupNorm(4, latent_dim),\n",
    "            nn.PReLU(),\n",
    "            nn.Linear(in_features=latent_dim, out_features=latent_dim),\n",
    "            nn.GroupNorm(4, latent_dim),\n",
    "            nn.PReLU(),\n",
    "            nn.Linear(in_features=latent_dim, out_features=emb_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear_out(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def cosine_beta_schedule(timesteps, s=0.008):\n",
    "    \"\"\"\n",
    "    cosine schedule\n",
    "    as proposed in https://openreview.net/forum?id=-NEXDKk8gZ\n",
    "    \"\"\"\n",
    "    steps = timesteps + 1\n",
    "    t = torch.linspace(0, timesteps, steps, dtype=torch.float64) / timesteps\n",
    "    alphas_cumprod = torch.cos((t + s) / (1 + s) * math.pi * 0.5) ** 2\n",
    "    alphas_cumprod = alphas_cumprod / alphas_cumprod[0]\n",
    "    betas = 1 - (alphas_cumprod[1:] / alphas_cumprod[:-1])\n",
    "    return torch.clip(betas, 0, 0.999)\n",
    "\n",
    "\n",
    "def sigmoid_beta_schedule(timesteps, start=-3, end=3, tau=1, clamp_min=1e-5):\n",
    "    \"\"\"\n",
    "    sigmoid schedule\n",
    "    proposed in https://arxiv.org/abs/2212.11972 - Figure 8\n",
    "    \"\"\"\n",
    "    steps = timesteps + 1\n",
    "    t = torch.linspace(0, timesteps, steps, dtype=torch.float64) / timesteps\n",
    "    v_start = torch.tensor(start / tau).sigmoid()\n",
    "    v_end = torch.tensor(end / tau).sigmoid()\n",
    "    alphas_cumprod = (-((t * (end - start) + start) / tau).sigmoid() + v_end) / (\n",
    "        v_end - v_start\n",
    "    )\n",
    "    alphas_cumprod = alphas_cumprod / alphas_cumprod[0]\n",
    "    betas = 1 - (alphas_cumprod[1:] / alphas_cumprod[:-1])\n",
    "    return torch.clip(betas, 0, 0.999)\n",
    "\n",
    "\n",
    "def ddpm_schedules(beta1, beta2, T):\n",
    "    # assert beta1 < beta2 < 1.0, \"beta1 and beta2 must be in (0, 1)\"\n",
    "    # beta_t = (beta2 - beta1) * torch.arange(0, T + 1, dtype=torch.float32) / T + beta1\n",
    "    beta_t = cosine_beta_schedule(T, s=0.008).float()\n",
    "    # beta_t = sigmoid_beta_schedule(T).float()\n",
    "\n",
    "    alpha_t = 1 - beta_t\n",
    "\n",
    "    log_alpha_t = torch.log(alpha_t)\n",
    "    alphabar_t = torch.cumsum(log_alpha_t, dim=0).exp()\n",
    "\n",
    "    sqrtab = torch.sqrt(alphabar_t)\n",
    "\n",
    "    sqrtmab = torch.sqrt(1 - alphabar_t)\n",
    "\n",
    "    return {\n",
    "        \"sqrtab\": sqrtab,  # \\sqrt{\\bar{\\alpha_t}}\n",
    "        \"sqrtmab\": sqrtmab,  # \\sqrt{1-\\bar{\\alpha_t}}\n",
    "    }\n",
    "\n",
    "\n",
    "class DDPM(nn.Module):\n",
    "    def __init__(self, nn_model, betas, n_T, device):\n",
    "        super(DDPM, self).__init__()\n",
    "        self.nn_model = nn_model.to(device)\n",
    "\n",
    "        for k, v in ddpm_schedules(betas[0], betas[1], n_T).items():\n",
    "            self.register_buffer(k, v)\n",
    "\n",
    "        self.n_T = n_T\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, x):\n",
    "        _ts = torch.randint(1, self.n_T, (x.shape[0],)).to(\n",
    "            self.device\n",
    "        )  # t ~ Uniform(0, n_T)\n",
    "        noise = torch.randn_like(x)  # eps ~ N(0, 1)\n",
    "        #x_t = self.sqrtab[_ts, None, None] * x + self.sqrtmab[_ts, None, None] * noise\n",
    "        x_t = self.sqrtab[_ts].view(-1, 1, 1, 1) * x + self.sqrtmab[_ts].view(-1, 1, 1, 1) * noise\n",
    "        times = _ts / self.n_T\n",
    "        output, down, up = self.nn_model(x_t, times)\n",
    "        return output, down, up, noise, times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "T4i5FvbBauXV",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T4i5FvbBauXV",
    "outputId": "e81a162a-3775-4a63-b0fd-53a02611b01c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample input shape: torch.Size([32, 64, 33, 16])  → Channels: 64, Timepoints: 33\n"
     ]
    }
   ],
   "source": [
    "for x, y in train_loader:\n",
    "    channels = x.shape[1]\n",
    "    timepoints = x.shape[2]\n",
    "    print(f\"Sample input shape: {x.shape}  → Channels: {channels}, Timepoints: {timepoints}\")\n",
    "    break\n",
    "\n",
    "num_classes = config['num_classes']\n",
    "ddpm_dim = config['ddpm_dim']\n",
    "encoder_dim = config['encoder_dim']\n",
    "fc_dim = config['fc_dim']\n",
    "n_T = config['n_T']\n",
    "\n",
    "ddpm_model = ConditionalUNet(in_channels=channels, n_feat=ddpm_dim).to(device)\n",
    "ddpm = DDPM(nn_model=ddpm_model, betas=(1e-6, 1e-2), n_T=n_T, device=device).to(device)\n",
    "\n",
    "encoder = Encoder(in_channels=channels, dim=encoder_dim).to(device)\n",
    "decoder = Decoder(in_channels=channels, n_feat=ddpm_dim, encoder_dim=encoder_dim).to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "42acd14a-7754-4fba-ba6a-8f7a4f609e17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Model initialized with:\n",
      " - Input channels       : 64\n",
      " - Timepoints           : 33\n",
      " - ddpm total params    : 265478\n",
      " - encoder total params : 149763\n",
      " - decoder total params : 156482\n",
      " - classifier params    : 409628\n",
      " - total DiffE params   : 715873\n"
     ]
    }
   ],
   "source": [
    "fc = LinearClassifier(encoder_dim, fc_dim, emb_dim=num_classes).to(device)\n",
    "\n",
    "diffe = DiffE(encoder, decoder, fc).to(device)\n",
    "\n",
    "print(\" Model initialized with:\")\n",
    "print(\" - Input channels       :\", channels)\n",
    "print(\" - Timepoints           :\", timepoints)\n",
    "print(\" - ddpm total params    :\", sum(p.numel() for p in ddpm.parameters()))\n",
    "print(\" - encoder total params :\", sum(p.numel() for p in encoder.parameters()))\n",
    "print(\" - decoder total params :\", sum(p.numel() for p in decoder.parameters()))\n",
    "print(\" - classifier params    :\", sum(p.numel() for p in fc.parameters()))\n",
    "print(\" - total DiffE params   :\", sum(p.numel() for p in diffe.parameters()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gNFmH1U1cUfr",
   "metadata": {
    "id": "gNFmH1U1cUfr"
   },
   "source": [
    "#train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e6b9kOFsmNm2",
   "metadata": {
    "id": "e6b9kOFsmNm2"
   },
   "outputs": [],
   "source": [
    "# Criterion\n",
    "criterion = nn.L1Loss()\n",
    "criterion_class = nn.MSELoss()\n",
    "\n",
    "# Define optimizer\n",
    "base_lr, lr = config['AE_base_lr'], config['AE_max_lr']\n",
    "optim1 = optim.RMSprop(ddpm.parameters(), lr=base_lr)\n",
    "optim2 = optim.RMSprop(diffe.parameters(), lr=base_lr)\n",
    "\n",
    "# EMAs\n",
    "fc_ema = EMA(diffe.fc, beta=0.95, update_after_step=100, update_every=10,)\n",
    "\n",
    "step_size = 150\n",
    "scheduler1 = optim.lr_scheduler.CyclicLR(\n",
    "    optimizer=optim1,\n",
    "    base_lr=base_lr,\n",
    "    max_lr=lr,\n",
    "    step_size_up=step_size,\n",
    "    mode=\"exp_range\",\n",
    "    cycle_momentum=False,\n",
    "    gamma=0.9998,\n",
    ")\n",
    "scheduler2 = optim.lr_scheduler.CyclicLR(\n",
    "    optimizer=optim2,\n",
    "    base_lr=base_lr,\n",
    "    max_lr=lr,\n",
    "    step_size_up=step_size,\n",
    "    mode=\"exp_range\",\n",
    "    cycle_momentum=False,\n",
    "    gamma=0.9998,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "DEi7SkZfvNbe",
   "metadata": {
    "id": "DEi7SkZfvNbe"
   },
   "outputs": [],
   "source": [
    "# Evaluate function\n",
    "def evaluate(encoder, fc, generator, device):\n",
    "    labels = np.arange(0, 26)\n",
    "    Y = []\n",
    "    Y_hat = []\n",
    "    for x, y in generator:\n",
    "        x, y = x.to(device), y.type(torch.LongTensor).to(device)\n",
    "        encoder_out = encoder(x)\n",
    "        y_hat = fc(encoder_out[1])\n",
    "        y_hat = F.softmax(y_hat, dim=1)\n",
    "\n",
    "        Y.append(y.detach().cpu())\n",
    "        Y_hat.append(y_hat.detach().cpu())\n",
    "\n",
    "    # List of tensors to tensor to numpy\n",
    "    Y = torch.cat(Y, dim=0).numpy()  # (N, )\n",
    "    Y_hat = torch.cat(Y_hat, dim=0).numpy()  # (N, 13): has to sum to 1 for each row\n",
    "\n",
    "    # Accuracy and Confusion Matrix\n",
    "    accuracy = top_k_accuracy_score(Y, Y_hat, k=1, labels=labels)\n",
    "    f1 = f1_score(Y, Y_hat.argmax(axis=1), average=\"macro\", labels=labels)\n",
    "    recall = recall_score(Y, Y_hat.argmax(axis=1), average=\"macro\", labels=labels)\n",
    "    precision = precision_score(Y, Y_hat.argmax(axis=1), average=\"macro\", labels=labels, zero_division=0)\n",
    "    auc = roc_auc_score(Y, Y_hat, average=\"macro\", multi_class=\"ovo\", labels=labels)\n",
    "\n",
    "    metrics = {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"f1\": f1,\n",
    "        \"recall\": recall,\n",
    "        \"precision\": precision,\n",
    "        \"auc\": auc,\n",
    "    }\n",
    "    # df_cm = pd.DataFrame(confusion_matrix(Y, Y_hat.argmax(axis=1)))\n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "635ff91f-7ac3-44aa-8054-d6f9d0e038d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shaoy\\anaconda3\\envs\\pytorch_env_p11\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">528</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)          │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">528</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)         │           <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ batch_normalization                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">528</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)         │              <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ depthwise_conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">528</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │           <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ batch_normalization_1                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">528</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ activation (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">528</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ average_pooling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">AveragePooling2D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">132</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">132</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ separable_conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SeparableConv2D</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">132</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │           <span style=\"color: #00af00; text-decoration-color: #00af00\">1,536</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ batch_normalization_2                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">132</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ activation_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">132</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ average_pooling2d_1                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)           │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">AveragePooling2D</span>)                   │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)           │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>)                  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">13,338</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ softmax (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m528\u001b[0m, \u001b[38;5;34m1\u001b[0m)          │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m528\u001b[0m, \u001b[38;5;34m16\u001b[0m)         │           \u001b[38;5;34m1,024\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ batch_normalization                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m528\u001b[0m, \u001b[38;5;34m16\u001b[0m)         │              \u001b[38;5;34m64\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ depthwise_conv2d (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m528\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │           \u001b[38;5;34m2,048\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ batch_normalization_1                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m528\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │             \u001b[38;5;34m128\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ activation (\u001b[38;5;33mActivation\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m528\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ average_pooling2d (\u001b[38;5;33mAveragePooling2D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m132\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m132\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ separable_conv2d (\u001b[38;5;33mSeparableConv2D\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m132\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │           \u001b[38;5;34m1,536\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ batch_normalization_2                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m132\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │             \u001b[38;5;34m128\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ activation_1 (\u001b[38;5;33mActivation\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m132\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ average_pooling2d_1                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m32\u001b[0m)           │               \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mAveragePooling2D\u001b[0m)                   │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m32\u001b[0m)           │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m26\u001b[0m)                  │          \u001b[38;5;34m13,338\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ softmax (\u001b[38;5;33mActivation\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m26\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">18,266</span> (71.35 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m18,266\u001b[0m (71.35 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">18,106</span> (70.73 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m18,106\u001b[0m (70.73 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">160</span> (640.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m160\u001b[0m (640.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "eegnet_model = EEGNet(nb_classes=config['num_classes'], \n",
    "                      Chans=config['eeg_channels'], \n",
    "                      Samples=config['eeg_samples'], \n",
    "                      dropoutRate=config['eeg_droupout'], \n",
    "                      kernLength=config['eeg_kernel_length'], \n",
    "                      F1=config['eeg_F1'], \n",
    "                      D=config['eeg_D'], \n",
    "                      F2=config['eeg_F2'], \n",
    "                      dropoutType='Dropout')\n",
    "\n",
    "eegnet_model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "eegnet_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ec9a3b47-5c70-4e89-94d8-a1155fa3541f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2080, 64, 528)\n",
      "(2080,)\n"
     ]
    }
   ],
   "source": [
    "def x_transform(x):\n",
    "    return x.reshape(x.shape[0], 64, -1)\n",
    "\n",
    "\n",
    "def dataloader_to_numpy(dataloader):\n",
    "    X_all, y_all = [], []\n",
    "    for X_batch, y_batch in dataloader:\n",
    "        X_all.append(X_batch.numpy())\n",
    "        y_all.append(y_batch.numpy())\n",
    "    X_all = np.concatenate(X_all, axis=0)\n",
    "    y_all = np.concatenate(y_all, axis=0)\n",
    "    X_eegnet = x_transform(X_all)\n",
    "    return X_eegnet, y_all\n",
    "\n",
    "\n",
    "X_train_np,  y_train_np  = dataloader_to_numpy(train_loader)\n",
    "X_val_np,    y_val_np    = dataloader_to_numpy(val_loader)\n",
    "X_test1_np,  y_test1_np  = dataloader_to_numpy(test1_loader)\n",
    "X_test2_np,  y_test2_np  = dataloader_to_numpy(test2_loader)\n",
    "\n",
    "print(X_train_np.shape)\n",
    "print(y_train_np.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "669995da-058b-4428-a873-bb8c3721c948",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Initiate Wandb.'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Initiate Wandb.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "410fb9bd-de77-46e3-a781-75ed17251e7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "wandb: WARNING If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "wandb: WARNING Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "wandb: Appending key for api.wandb.ai to your netrc file: C:\\Users\\shaoy\\_netrc\n",
      "wandb: W&B API key is configured. Use `wandb login --relogin` to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "creating run (1.3m)<br>  <strong style=\"color:red\">ERROR</strong> retrying: Post \"https://api.wandb.ai/graphql\": net/http: request canceled (Client.Timeout exceeded while awaiting headers)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\LTI 11785 Introduction to deep learning\\project\\SSVEP\\wandb\\run-20250418_165422-u0fwht0f</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/yucheng_shao-carnegie-mellon-university/project_ssvep-ablations/runs/u0fwht0f' target=\"_blank\">Ben_project_base_line_EEGNet_with_spectrogram</a></strong> to <a href='https://wandb.ai/yucheng_shao-carnegie-mellon-university/project_ssvep-ablations' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/yucheng_shao-carnegie-mellon-university/project_ssvep-ablations' target=\"_blank\">https://wandb.ai/yucheng_shao-carnegie-mellon-university/project_ssvep-ablations</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/yucheng_shao-carnegie-mellon-university/project_ssvep-ablations/runs/u0fwht0f' target=\"_blank\">https://wandb.ai/yucheng_shao-carnegie-mellon-university/project_ssvep-ablations/runs/u0fwht0f</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "# Use wandb? Resume Training?\n",
    "USE_WANDB = True\n",
    "\n",
    "RESUME_LOGGING = False # Set this to true if you are resuming training from a previous run\n",
    "\n",
    "# Create your wandb run\n",
    "\n",
    "run_name = '{}_project_base_line_EEGNet_with_spectrogram'.format('Ben')\n",
    "\n",
    "# If you are resuming an old run\n",
    "if USE_WANDB:\n",
    "\n",
    "    wandb.login(key=\"207d89e5e9cdfc415370f05503c72a11d5065073\") #TODO\n",
    "\n",
    "    if RESUME_LOGGING:\n",
    "        run = wandb.init(\n",
    "            id     = \"diff-CAE(fc)-EEG\", ### Insert specific run id here if you want to resume a previous run\n",
    "            resume = \"must\", ### You need this to resume previous runs\n",
    "            project = \"project_ssvep-ablations\", ### Project should be created in your wandb\n",
    "            settings = wandb.Settings(_service_wait=300)\n",
    "        )\n",
    "\n",
    "\n",
    "    else:\n",
    "        run = wandb.init(\n",
    "            name    = run_name, ### Wandb creates random run names if you skip this field, we recommend you give useful names\n",
    "            reinit  = True, ### Allows reinitalizing runs when you re-run this cell\n",
    "            project = \"project_ssvep-ablations\", ### Project should be created in your wandb account\n",
    "            config  = config ### Wandb Config for your run\n",
    "        )\n",
    "\n",
    "        ### Save your model architecture as a string with str(model)\n",
    "        model_arch  = str([ddpm, diffe, eegnet_model])\n",
    "        ### Save it in a txt file\n",
    "        arch_file   = open(\"model_arch.txt\", \"w\")\n",
    "        file_write  = arch_file.write(model_arch)\n",
    "        arch_file.close()\n",
    "\n",
    "        ### log it in your wandb run with wandb.save()\n",
    "        wandb.save('model_arch.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4dcc774b-fb53-435e-8ca0-c13ab87559bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CLEAR RAM!!\n",
    "import gc\n",
    "\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "071a1163-15ab-47ff-a3ac-82f8bd51816a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, optimizer, scheduler, metrics, epoch, path):\n",
    "    torch.save(\n",
    "        {'model_state_dict'         : model.state_dict(),\n",
    "         'optimizer_state_dict'     : optimizer.state_dict(),\n",
    "         'scheduler_state_dict'     : scheduler.state_dict(),\n",
    "         'metric'                   : metrics,\n",
    "         'epoch'                    : epoch},\n",
    "         path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "NIjWCm5vUKx-",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "3a0de191bd4b4018a92ada5e6e4607cf",
      "244fb693e8174b82aaa38b9d943ca69b",
      "07acb422b2b34b9791881de2513b32c7",
      "c7524cccdeea422d9ef342b142df608d",
      "6afb371d18044cd3815fd96c9810c3a7",
      "ce5b46cc0a1848f0adcdd11a1e7f95a3",
      "6855930ba8b94aab9f64dbbdcddd51d2",
      "38a3424ad0d5460ba042960d1736b5aa",
      "0a160710838b45c3950a599f2dce7836",
      "713d6a7190db4a54a4cd9ec1e498b1be",
      "9d503d51771842af8d76af60586edd19"
     ]
    },
    "id": "NIjWCm5vUKx-",
    "outputId": "20608bb1-27a0-4dd8-b1e9-6a61ce3e2cba"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nargs = argparse.Namespace()\\nargs.device = device\\nargs.subject = \"ALL\"\\nargs.root_dir = subject_chars_dir\\nsubject = args.subject\\n\\nnum_epochs = config[\\'epochs_AEs\\']\\ntest_period = 1\\nstart_test = test_period\\nalpha = config[\\'alpha\\']\\nbeta = config[\\'beta\\']\\n\\nbest_acc = 0\\nbest_f1 = 0\\nbest_recall = 0\\nbest_precision = 0\\nbest_auc = 0\\n\\nhistory = {\\n    \"train_loss\": [],\\n    \"train_acc\": [],\\n    \"val_loss\": [],\\n    \"val_acc\": []\\n}\\n\\nwith tqdm(total=num_epochs, desc=f\"Method ALL - Joint Training on All Subjects - Autoencoders\") as pbar:\\n    for epoch in range(num_epochs):\\n        ddpm.train()\\n        diffe.train()\\n\\n        epoch_loss = 0\\n        num_batches = 0\\n        epoch_acc = 0\\n        total_samples = 0\\n\\n        ############################## Train ###########################################\\n        for x, y in train_loader:\\n            x, y = x.to(device), y.type(torch.LongTensor).to(device)\\n            y_cat = F.one_hot(y, num_classes=26).type(torch.FloatTensor).to(device)\\n            # Train DDPM\\n            optim1.zero_grad()\\n            x_hat, down, up, noise, t = ddpm(x)\\n\\n            # Align the temporal dimension of x_hat and x\\n            x_hat, x = safe_align_2d(x_hat, x)\\n\\n            # needed if we wish to use x_hat again after first backward call\\n            x_hat_detached = x_hat.detach()\\n\\n            loss_ddpm = F.l1_loss(x_hat, x, reduction=\"none\")\\n            loss_ddpm.mean().backward()\\n            optim1.step()\\n            ddpm_out = x_hat, down, up, t\\n\\n            # Train Diff-E\\n            optim2.zero_grad()\\n            decoder_out, fc_out = diffe(x, ddpm_out)\\n\\n            #loss_gap = criterion(decoder_out, loss_ddpm.detach())\\n            loss_gap = criterion(decoder_out, x_hat_detached)\\n            loss_c = criterion_class(fc_out, y_cat)\\n            loss = beta * loss_gap + alpha * loss_c\\n            loss.backward()\\n            optim2.step()\\n\\n            # Optimizer scheduler step\\n            scheduler1.step()\\n            scheduler2.step()\\n\\n            # EMA update\\n            fc_ema.update()\\n\\n            epoch_loss += loss.item()\\n            num_batches += 1\\n\\n            pred_labels = torch.argmax(fc_out, dim=1)\\n            correct = (pred_labels == y).sum().item()\\n            epoch_acc += correct\\n            total_samples += y.size(0)\\n\\n        history[\"train_loss\"].append(epoch_loss / num_batches)\\n        history[\"train_acc\"].append(epoch_acc / total_samples)\\n\\n        metrics = {\\n            \\'train_loss\\': history[\"train_loss\"][-1],\\n            \\'train_acc\\': history[\"train_acc\"][-1],\\n        }\\n\\n        ############################## validation ###########################################\\n        with torch.no_grad():\\n            if epoch > start_test:\\n                test_period = 1\\n            if epoch % test_period == 0:\\n                ddpm.eval()\\n                diffe.eval()\\n\\n                metrics_val = evaluate(diffe.encoder, fc_ema, val_loader, device)\\n\\n                val_acc = metrics_val[\"accuracy\"]\\n                history[\"val_acc\"].append(val_acc)\\n                f1 = metrics_val[\"f1\"]\\n                recall = metrics_val[\"recall\"]\\n                precision = metrics_val[\"precision\"]\\n                auc = metrics_val[\"auc\"]\\n\\n                val_loss = 0\\n                with torch.no_grad():\\n                    for x, y in val_loader:\\n                        x, y = x.to(device), y.type(torch.LongTensor).to(device)\\n                        y_cat = F.one_hot(y, num_classes=26).float().to(device)\\n\\n                        x_hat, down, up, noise, t = ddpm(x)\\n                        ddpm_out = x_hat, down, up, t\\n\\n                        # Align the temporal dimension of x_hat and x\\n                        x_hat, x = safe_align_2d(x_hat, x)\\n\\n                        loss_ddpm = F.l1_loss(x_hat, x, reduction=\"none\")\\n                        decoder_out, fc_out = diffe(x, ddpm_out)\\n\\n                        #loss_gap = criterion(decoder_out, loss_ddpm)\\n                        loss_gap = criterion(decoder_out, x_hat)\\n                        loss_c = criterion_class(fc_out, y_cat)\\n\\n                        val_loss += (beta * loss_gap + alpha * loss_c).item()\\n                history[\"val_loss\"].append(val_loss / len(val_loader))\\n\\n                metrics.update({\\n                    \\'valid_loss\\': history[\"val_loss\"][-1],\\n                    \\'valid_acc\\': val_acc,\\n                })\\n\\n                best_acc_bool = val_acc > best_acc\\n                best_f1_bool = f1 > best_f1\\n                best_recall_bool = recall > best_recall\\n                best_precision_bool = precision > best_precision\\n                best_auc_bool = auc > best_auc\\n\\n                save_model(ddpm, optim1, scheduler1, metrics, epoch, os.path.join(checkpoint_dir, \\'ddpm_all_subjects.pth\\'))\\n                save_model(diffe, optim2, scheduler2, metrics, epoch, os.path.join(checkpoint_dir, \\'diffe_all_subjects.pth\\'))\\n\\n                if best_acc_bool:\\n                    best_acc = val_acc\\n                    #torch.save(diffe.state_dict(), f\\'/content/drive/MyDrive/project/model/ssvep/diffe_{subject}.pth\\')\\n                    #torch.save(ddpm.state_dict(), os.path.join(checkpoint_dir, \\'ddpm_all_subjects.pth\\'))\\n                    save_model(ddpm, optim1, scheduler1, metrics, epoch, os.path.join(checkpoint_dir, \\'best_ddpm_all_subjects.pth\\'))\\n                    save_model(diffe, optim2, scheduler2, metrics, epoch, os.path.join(checkpoint_dir, \\'best_diffe_all_subjects.pth\\'))\\n                    #torch.save(diffe.state_dict(), r\\'C:\\\\LTI 11785 Introduction to deep learning\\\\project\\\\SSVEP\\\\checkpoints_separated\\\\diffe_all_subjects.pth\\')\\n\\n                    wandb.save(os.path.join(checkpoint_dir, \\'best_ddpm_all_subjects.pth\\'))\\n                    wandb.save(os.path.join(checkpoint_dir, \\'best_diffe_all_subjects.pth\\'))\\n                    \\n                if best_f1_bool:\\n                    best_f1 = f1\\n                if best_recall_bool:\\n                    best_recall = recall\\n                if best_precision_bool:\\n                    best_precision = precision\\n                if best_auc_bool:\\n                    best_auc = auc\\n\\n                description = f\"Val Accuracy: {val_acc*100:.2f}% | Best: {best_acc*100:.2f}%\"\\n                pbar.set_description(f\"Method ALL – Processing subject {subject} – {description}\"\\n                )\\n        #print(f\"[Epoch {epoch+1}/{num_epochs}] Train Loss: {history[\\'train_loss\\'][-1]:.4f} | Val Acc: {val_acc*100:.2f}%\")\\n        print(f\"[Epoch {epoch+1}/{num_epochs}]\")\\n        print(f\"Train Loss: {history[\\'train_loss\\'][-1]:.4f} | Train Acc: {history[\\'train_acc\\'][-1]*100:.2f}%\")\\n        print(f\"Valid Loss: {history[\\'val_loss\\'][-1]:.4f} | Valid Acc: {val_acc*100:.2f}%\")\\n        pbar.update(1)\\n\\n        if run is not None:\\n            run.log(metrics)\\n'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train & Evaluate\n",
    "checkpoint_dir = config['checkpoint_dir']\n",
    "'''\n",
    "args = argparse.Namespace()\n",
    "args.device = device\n",
    "args.subject = \"ALL\"\n",
    "args.root_dir = subject_chars_dir\n",
    "subject = args.subject\n",
    "\n",
    "num_epochs = config['epochs_AEs']\n",
    "test_period = 1\n",
    "start_test = test_period\n",
    "alpha = config['alpha']\n",
    "beta = config['beta']\n",
    "\n",
    "best_acc = 0\n",
    "best_f1 = 0\n",
    "best_recall = 0\n",
    "best_precision = 0\n",
    "best_auc = 0\n",
    "\n",
    "history = {\n",
    "    \"train_loss\": [],\n",
    "    \"train_acc\": [],\n",
    "    \"val_loss\": [],\n",
    "    \"val_acc\": []\n",
    "}\n",
    "\n",
    "with tqdm(total=num_epochs, desc=f\"Method ALL - Joint Training on All Subjects - Autoencoders\") as pbar:\n",
    "    for epoch in range(num_epochs):\n",
    "        ddpm.train()\n",
    "        diffe.train()\n",
    "\n",
    "        epoch_loss = 0\n",
    "        num_batches = 0\n",
    "        epoch_acc = 0\n",
    "        total_samples = 0\n",
    "\n",
    "        ############################## Train ###########################################\n",
    "        for x, y in train_loader:\n",
    "            x, y = x.to(device), y.type(torch.LongTensor).to(device)\n",
    "            y_cat = F.one_hot(y, num_classes=26).type(torch.FloatTensor).to(device)\n",
    "            # Train DDPM\n",
    "            optim1.zero_grad()\n",
    "            x_hat, down, up, noise, t = ddpm(x)\n",
    "\n",
    "            # Align the temporal dimension of x_hat and x\n",
    "            x_hat, x = safe_align_2d(x_hat, x)\n",
    "\n",
    "            # needed if we wish to use x_hat again after first backward call\n",
    "            x_hat_detached = x_hat.detach()\n",
    "\n",
    "            loss_ddpm = F.l1_loss(x_hat, x, reduction=\"none\")\n",
    "            loss_ddpm.mean().backward()\n",
    "            optim1.step()\n",
    "            ddpm_out = x_hat, down, up, t\n",
    "\n",
    "            # Train Diff-E\n",
    "            optim2.zero_grad()\n",
    "            decoder_out, fc_out = diffe(x, ddpm_out)\n",
    "\n",
    "            #loss_gap = criterion(decoder_out, loss_ddpm.detach())\n",
    "            loss_gap = criterion(decoder_out, x_hat_detached)\n",
    "            loss_c = criterion_class(fc_out, y_cat)\n",
    "            loss = beta * loss_gap + alpha * loss_c\n",
    "            loss.backward()\n",
    "            optim2.step()\n",
    "\n",
    "            # Optimizer scheduler step\n",
    "            scheduler1.step()\n",
    "            scheduler2.step()\n",
    "\n",
    "            # EMA update\n",
    "            fc_ema.update()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            num_batches += 1\n",
    "\n",
    "            pred_labels = torch.argmax(fc_out, dim=1)\n",
    "            correct = (pred_labels == y).sum().item()\n",
    "            epoch_acc += correct\n",
    "            total_samples += y.size(0)\n",
    "\n",
    "        history[\"train_loss\"].append(epoch_loss / num_batches)\n",
    "        history[\"train_acc\"].append(epoch_acc / total_samples)\n",
    "\n",
    "        metrics = {\n",
    "            'train_loss': history[\"train_loss\"][-1],\n",
    "            'train_acc': history[\"train_acc\"][-1],\n",
    "        }\n",
    "\n",
    "        ############################## validation ###########################################\n",
    "        with torch.no_grad():\n",
    "            if epoch > start_test:\n",
    "                test_period = 1\n",
    "            if epoch % test_period == 0:\n",
    "                ddpm.eval()\n",
    "                diffe.eval()\n",
    "\n",
    "                metrics_val = evaluate(diffe.encoder, fc_ema, val_loader, device)\n",
    "\n",
    "                val_acc = metrics_val[\"accuracy\"]\n",
    "                history[\"val_acc\"].append(val_acc)\n",
    "                f1 = metrics_val[\"f1\"]\n",
    "                recall = metrics_val[\"recall\"]\n",
    "                precision = metrics_val[\"precision\"]\n",
    "                auc = metrics_val[\"auc\"]\n",
    "\n",
    "                val_loss = 0\n",
    "                with torch.no_grad():\n",
    "                    for x, y in val_loader:\n",
    "                        x, y = x.to(device), y.type(torch.LongTensor).to(device)\n",
    "                        y_cat = F.one_hot(y, num_classes=26).float().to(device)\n",
    "\n",
    "                        x_hat, down, up, noise, t = ddpm(x)\n",
    "                        ddpm_out = x_hat, down, up, t\n",
    "\n",
    "                        # Align the temporal dimension of x_hat and x\n",
    "                        x_hat, x = safe_align_2d(x_hat, x)\n",
    "\n",
    "                        loss_ddpm = F.l1_loss(x_hat, x, reduction=\"none\")\n",
    "                        decoder_out, fc_out = diffe(x, ddpm_out)\n",
    "\n",
    "                        #loss_gap = criterion(decoder_out, loss_ddpm)\n",
    "                        loss_gap = criterion(decoder_out, x_hat)\n",
    "                        loss_c = criterion_class(fc_out, y_cat)\n",
    "\n",
    "                        val_loss += (beta * loss_gap + alpha * loss_c).item()\n",
    "                history[\"val_loss\"].append(val_loss / len(val_loader))\n",
    "\n",
    "                metrics.update({\n",
    "                    'valid_loss': history[\"val_loss\"][-1],\n",
    "                    'valid_acc': val_acc,\n",
    "                })\n",
    "\n",
    "                best_acc_bool = val_acc > best_acc\n",
    "                best_f1_bool = f1 > best_f1\n",
    "                best_recall_bool = recall > best_recall\n",
    "                best_precision_bool = precision > best_precision\n",
    "                best_auc_bool = auc > best_auc\n",
    "\n",
    "                save_model(ddpm, optim1, scheduler1, metrics, epoch, os.path.join(checkpoint_dir, 'ddpm_all_subjects.pth'))\n",
    "                save_model(diffe, optim2, scheduler2, metrics, epoch, os.path.join(checkpoint_dir, 'diffe_all_subjects.pth'))\n",
    "\n",
    "                if best_acc_bool:\n",
    "                    best_acc = val_acc\n",
    "                    #torch.save(diffe.state_dict(), f'/content/drive/MyDrive/project/model/ssvep/diffe_{subject}.pth')\n",
    "                    #torch.save(ddpm.state_dict(), os.path.join(checkpoint_dir, 'ddpm_all_subjects.pth'))\n",
    "                    save_model(ddpm, optim1, scheduler1, metrics, epoch, os.path.join(checkpoint_dir, 'best_ddpm_all_subjects.pth'))\n",
    "                    save_model(diffe, optim2, scheduler2, metrics, epoch, os.path.join(checkpoint_dir, 'best_diffe_all_subjects.pth'))\n",
    "                    #torch.save(diffe.state_dict(), r'C:\\LTI 11785 Introduction to deep learning\\project\\SSVEP\\checkpoints_separated\\diffe_all_subjects.pth')\n",
    "\n",
    "                    wandb.save(os.path.join(checkpoint_dir, 'best_ddpm_all_subjects.pth'))\n",
    "                    wandb.save(os.path.join(checkpoint_dir, 'best_diffe_all_subjects.pth'))\n",
    "                    \n",
    "                if best_f1_bool:\n",
    "                    best_f1 = f1\n",
    "                if best_recall_bool:\n",
    "                    best_recall = recall\n",
    "                if best_precision_bool:\n",
    "                    best_precision = precision\n",
    "                if best_auc_bool:\n",
    "                    best_auc = auc\n",
    "\n",
    "                description = f\"Val Accuracy: {val_acc*100:.2f}% | Best: {best_acc*100:.2f}%\"\n",
    "                pbar.set_description(f\"Method ALL – Processing subject {subject} – {description}\"\n",
    "                )\n",
    "        #print(f\"[Epoch {epoch+1}/{num_epochs}] Train Loss: {history['train_loss'][-1]:.4f} | Val Acc: {val_acc*100:.2f}%\")\n",
    "        print(f\"[Epoch {epoch+1}/{num_epochs}]\")\n",
    "        print(f\"Train Loss: {history['train_loss'][-1]:.4f} | Train Acc: {history['train_acc'][-1]*100:.2f}%\")\n",
    "        print(f\"Valid Loss: {history['val_loss'][-1]:.4f} | Valid Acc: {val_acc*100:.2f}%\")\n",
    "        pbar.update(1)\n",
    "\n",
    "        if run is not None:\n",
    "            run.log(metrics)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "29ead19d-ace6-4d99-a3d8-7c449452343d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#val_losses = []\n",
    "#val_accuracies = []\n",
    "#initializer = RandomNormal(mean=0.0, stddev=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cb13fd4d-ae68-4d9c-9b30-42e3fc082b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate2(diffe, ddpm, EEGNet, generator, device):\n",
    "    labels = np.arange(0, 26)\n",
    "    Y = []\n",
    "    Y_hat = []\n",
    "    for x, y in generator:\n",
    "        x, y = x.to(device), y.type(torch.LongTensor).to(device)\n",
    "        y_cat = F.one_hot(y, num_classes=26).type(torch.FloatTensor).to(device)\n",
    "\n",
    "        #x_hat, down, up, noise, t = ddpm(x)\n",
    "\n",
    "        # Align the temporal dimension of x_hat and x\n",
    "        #x_hat, x = safe_align_2d(x_hat, x)\n",
    "\n",
    "        #ddpm_out = x_hat, down, up, t\n",
    "        #ddpm_out = x, down, up, t\n",
    "\n",
    "        #decoder_out, fc_out = diffe(x, ddpm_out)\n",
    "        \n",
    "        #X_eegnet = decoder_out.view(decoder_out.size(0), 64, -1)  # (samples, 64, 396)\n",
    "        X_eegnet = x.view(x.size(0), 64, -1)\n",
    "        y_hat = EEGNet(X_eegnet)\n",
    "        \n",
    "        y_hat = F.softmax(y_hat, dim=1)\n",
    "\n",
    "        Y.append(y.detach().cpu())\n",
    "        Y_hat.append(y_hat.detach().cpu())\n",
    "\n",
    "    # List of tensors to tensor to numpy\n",
    "    Y = torch.cat(Y, dim=0).numpy()  # (N, )\n",
    "    Y_hat = torch.cat(Y_hat, dim=0).numpy()  # (N, 13): has to sum to 1 for each row\n",
    "\n",
    "    # Accuracy and Confusion Matrix\n",
    "    accuracy = top_k_accuracy_score(Y, Y_hat, k=1, labels=labels)\n",
    "    f1 = f1_score(Y, Y_hat.argmax(axis=1), average=\"macro\", labels=labels)\n",
    "    recall = recall_score(Y, Y_hat.argmax(axis=1), average=\"macro\", labels=labels)\n",
    "    precision = precision_score(Y, Y_hat.argmax(axis=1), average=\"macro\", labels=labels, zero_division=0)\n",
    "    auc = roc_auc_score(Y, Y_hat, average=\"macro\", multi_class=\"ovo\", labels=labels)\n",
    "\n",
    "    metrics = {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"f1\": f1,\n",
    "        \"recall\": recall,\n",
    "        \"precision\": precision,\n",
    "        \"auc\": auc,\n",
    "    }\n",
    "    # df_cm = pd.DataFrame(confusion_matrix(Y, Y_hat.argmax(axis=1)))\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "49dc8713-5e49-4662-bb9f-3920b1ad343d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 20s/step - accuracy: 0.3918 - loss: 2.12 ━━━━━━━━━━━━━━━━━━━━ 21s 21s/step - accuracy: 0.3918 - loss: 2.1260 - val_accuracy: 0.3538 - val_loss: 2.6732\n",
      "Epoch 2/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.3870 - loss: 2.12 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.3870 - loss: 2.1256 - val_accuracy: 0.3538 - val_loss: 2.6709\n",
      "Epoch 3/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 16s/step - accuracy: 0.3889 - loss: 2.12 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.3889 - loss: 2.1277 - val_accuracy: 0.3558 - val_loss: 2.6680\n",
      "Epoch 4/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 16s/step - accuracy: 0.3966 - loss: 2.12 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.3966 - loss: 2.1227 - val_accuracy: 0.3538 - val_loss: 2.6644\n",
      "Epoch 5/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.3976 - loss: 2.11 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.3976 - loss: 2.1140 - val_accuracy: 0.3558 - val_loss: 2.6604\n",
      "Epoch 6/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 16s/step - accuracy: 0.4043 - loss: 2.11 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.4043 - loss: 2.1139 - val_accuracy: 0.3577 - val_loss: 2.6560\n",
      "Epoch 7/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 16s/step - accuracy: 0.4111 - loss: 2.11 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.4111 - loss: 2.1159 - val_accuracy: 0.3538 - val_loss: 2.6512\n",
      "Epoch 8/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 16s/step - accuracy: 0.4226 - loss: 2.10 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.4226 - loss: 2.1088 - val_accuracy: 0.3462 - val_loss: 2.6461\n",
      "Epoch 9/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 16s/step - accuracy: 0.4159 - loss: 2.10 ━━━━━━━━━━━━━━━━━━━━ 17s 17s/step - accuracy: 0.4159 - loss: 2.1032 - val_accuracy: 0.3481 - val_loss: 2.6409\n",
      "Epoch 10/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.4202 - loss: 2.09 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.4202 - loss: 2.0993 - val_accuracy: 0.3423 - val_loss: 2.6357\n",
      "Epoch 11/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.4269 - loss: 2.09 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.4269 - loss: 2.0971 - val_accuracy: 0.3404 - val_loss: 2.6307\n",
      "Epoch 12/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.4269 - loss: 2.09 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.4269 - loss: 2.0919 - val_accuracy: 0.3365 - val_loss: 2.6258\n",
      "Epoch 13/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 16s/step - accuracy: 0.4346 - loss: 2.09 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.4346 - loss: 2.0916 - val_accuracy: 0.3481 - val_loss: 2.6211\n",
      "Epoch 14/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 16s/step - accuracy: 0.4433 - loss: 2.08 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.4433 - loss: 2.0819 - val_accuracy: 0.3577 - val_loss: 2.6166\n",
      "Epoch 15/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 16s/step - accuracy: 0.4370 - loss: 2.08 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.4370 - loss: 2.0846 - val_accuracy: 0.3577 - val_loss: 2.6122\n",
      "Epoch 16/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.4413 - loss: 2.07 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.4413 - loss: 2.0799 - val_accuracy: 0.3577 - val_loss: 2.6081\n",
      "Epoch 17/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.4433 - loss: 2.07 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.4433 - loss: 2.0799 - val_accuracy: 0.3596 - val_loss: 2.6039\n",
      "Epoch 18/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.4543 - loss: 2.07 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.4543 - loss: 2.0706 - val_accuracy: 0.3558 - val_loss: 2.5999\n",
      "Epoch 19/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.4534 - loss: 2.06 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.4534 - loss: 2.0661 - val_accuracy: 0.3615 - val_loss: 2.5958\n",
      "Epoch 20/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 16s/step - accuracy: 0.4505 - loss: 2.06 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.4505 - loss: 2.0669 - val_accuracy: 0.3615 - val_loss: 2.5919\n",
      "Epoch 21/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.4649 - loss: 2.05 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.4649 - loss: 2.0598 - val_accuracy: 0.3654 - val_loss: 2.5879\n",
      "Epoch 22/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 16s/step - accuracy: 0.4625 - loss: 2.06 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.4625 - loss: 2.0624 - val_accuracy: 0.3558 - val_loss: 2.5839\n",
      "Epoch 23/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.4716 - loss: 2.05 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.4716 - loss: 2.0597 - val_accuracy: 0.3558 - val_loss: 2.5799\n",
      "Epoch 24/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.4688 - loss: 2.04 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.4688 - loss: 2.0489 - val_accuracy: 0.3500 - val_loss: 2.5759\n",
      "Epoch 25/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.4716 - loss: 2.05 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.4716 - loss: 2.0500 - val_accuracy: 0.3500 - val_loss: 2.5721\n",
      "Epoch 26/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.4812 - loss: 2.04 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.4812 - loss: 2.0489 - val_accuracy: 0.3442 - val_loss: 2.5684\n",
      "Epoch 27/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.4726 - loss: 2.04 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.4726 - loss: 2.0421 - val_accuracy: 0.3404 - val_loss: 2.5647\n",
      "Epoch 28/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.4798 - loss: 2.04 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.4798 - loss: 2.0411 - val_accuracy: 0.3385 - val_loss: 2.5611\n",
      "Epoch 29/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.4784 - loss: 2.03 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.4784 - loss: 2.0356 - val_accuracy: 0.3423 - val_loss: 2.5575\n",
      "Epoch 30/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.4865 - loss: 2.03 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.4865 - loss: 2.0307 - val_accuracy: 0.3519 - val_loss: 2.5538\n",
      "Epoch 31/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.4837 - loss: 2.03 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.4837 - loss: 2.0304 - val_accuracy: 0.3538 - val_loss: 2.5502\n",
      "Epoch 32/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.4750 - loss: 2.03 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.4750 - loss: 2.0330 - val_accuracy: 0.3538 - val_loss: 2.5466\n",
      "Epoch 33/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.4832 - loss: 2.02 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.4832 - loss: 2.0255 - val_accuracy: 0.3558 - val_loss: 2.5428\n",
      "Epoch 34/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.4764 - loss: 2.02 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.4764 - loss: 2.0218 - val_accuracy: 0.3635 - val_loss: 2.5390\n",
      "Epoch 35/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.4870 - loss: 2.01 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.4870 - loss: 2.0196 - val_accuracy: 0.3654 - val_loss: 2.5352\n",
      "Epoch 36/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.4918 - loss: 2.01 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.4918 - loss: 2.0176 - val_accuracy: 0.3750 - val_loss: 2.5315\n",
      "Epoch 37/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.4875 - loss: 2.01 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.4875 - loss: 2.0145 - val_accuracy: 0.3769 - val_loss: 2.5278\n",
      "Epoch 38/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.4952 - loss: 2.01 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.4952 - loss: 2.0129 - val_accuracy: 0.3788 - val_loss: 2.5241\n",
      "Epoch 39/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.4928 - loss: 2.01 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.4928 - loss: 2.0117 - val_accuracy: 0.3827 - val_loss: 2.5205\n",
      "Epoch 40/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.4865 - loss: 2.00 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.4865 - loss: 2.0087 - val_accuracy: 0.3865 - val_loss: 2.5168\n",
      "Epoch 41/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.4966 - loss: 2.00 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.4966 - loss: 2.0097 - val_accuracy: 0.3865 - val_loss: 2.5132\n",
      "Epoch 42/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5048 - loss: 2.00 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.5048 - loss: 2.0008 - val_accuracy: 0.3942 - val_loss: 2.5095\n",
      "Epoch 43/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.4990 - loss: 2.00 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.4990 - loss: 2.0033 - val_accuracy: 0.3981 - val_loss: 2.5059\n",
      "Epoch 44/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.4938 - loss: 1.99 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.4938 - loss: 1.9987 - val_accuracy: 0.3981 - val_loss: 2.5022\n",
      "Epoch 45/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5053 - loss: 1.99 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.5053 - loss: 1.9974 - val_accuracy: 0.3942 - val_loss: 2.4985\n",
      "Epoch 46/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 16s/step - accuracy: 0.5010 - loss: 1.99 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.5010 - loss: 1.9929 - val_accuracy: 0.3942 - val_loss: 2.4949\n",
      "Epoch 47/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.4995 - loss: 1.99 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.4995 - loss: 1.9933 - val_accuracy: 0.3923 - val_loss: 2.4912\n",
      "Epoch 48/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 16s/step - accuracy: 0.4966 - loss: 1.99 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.4966 - loss: 1.9903 - val_accuracy: 0.3904 - val_loss: 2.4876\n",
      "Epoch 49/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5038 - loss: 1.98 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.5038 - loss: 1.9882 - val_accuracy: 0.3904 - val_loss: 2.4841\n",
      "Epoch 50/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5125 - loss: 1.98 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.5125 - loss: 1.9844 - val_accuracy: 0.3942 - val_loss: 2.4805\n",
      "Epoch 51/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.4966 - loss: 1.98 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.4966 - loss: 1.9858 - val_accuracy: 0.3962 - val_loss: 2.4770\n",
      "Epoch 52/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5019 - loss: 1.98 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.5019 - loss: 1.9807 - val_accuracy: 0.3942 - val_loss: 2.4734\n",
      "Epoch 53/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5149 - loss: 1.98 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.5149 - loss: 1.9816 - val_accuracy: 0.3923 - val_loss: 2.4698\n",
      "Epoch 54/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.5058 - loss: 1.97 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.5058 - loss: 1.9791 - val_accuracy: 0.3942 - val_loss: 2.4661\n",
      "Epoch 55/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5135 - loss: 1.97 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.5135 - loss: 1.9721 - val_accuracy: 0.3981 - val_loss: 2.4624\n",
      "Epoch 56/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5149 - loss: 1.97 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.5149 - loss: 1.9708 - val_accuracy: 0.3981 - val_loss: 2.4587\n",
      "Epoch 57/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5139 - loss: 1.97 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.5139 - loss: 1.9727 - val_accuracy: 0.3981 - val_loss: 2.4551\n",
      "Epoch 58/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5139 - loss: 1.96 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.5139 - loss: 1.9698 - val_accuracy: 0.4077 - val_loss: 2.4516\n",
      "Epoch 59/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5101 - loss: 1.96 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.5101 - loss: 1.9695 - val_accuracy: 0.4096 - val_loss: 2.4484\n",
      "Epoch 60/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5115 - loss: 1.96 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.5115 - loss: 1.9654 - val_accuracy: 0.4115 - val_loss: 2.4450\n",
      "Epoch 61/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5120 - loss: 1.96 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.5120 - loss: 1.9642 - val_accuracy: 0.4135 - val_loss: 2.4418\n",
      "Epoch 62/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5034 - loss: 1.96 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.5034 - loss: 1.9643 - val_accuracy: 0.4115 - val_loss: 2.4385\n",
      "Epoch 63/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5212 - loss: 1.95 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.5212 - loss: 1.9581 - val_accuracy: 0.4115 - val_loss: 2.4352\n",
      "Epoch 64/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5149 - loss: 1.96 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.5149 - loss: 1.9614 - val_accuracy: 0.4115 - val_loss: 2.4318\n",
      "Epoch 65/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5216 - loss: 1.95 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.5216 - loss: 1.9555 - val_accuracy: 0.4115 - val_loss: 2.4283\n",
      "Epoch 66/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5188 - loss: 1.95 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.5188 - loss: 1.9537 - val_accuracy: 0.4115 - val_loss: 2.4247\n",
      "Epoch 67/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5159 - loss: 1.95 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.5159 - loss: 1.9524 - val_accuracy: 0.4096 - val_loss: 2.4211\n",
      "Epoch 68/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5101 - loss: 1.94 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.5101 - loss: 1.9489 - val_accuracy: 0.4077 - val_loss: 2.4175\n",
      "Epoch 69/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5149 - loss: 1.94 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.5149 - loss: 1.9476 - val_accuracy: 0.4096 - val_loss: 2.4141\n",
      "Epoch 70/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5144 - loss: 1.94 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.5144 - loss: 1.9483 - val_accuracy: 0.4096 - val_loss: 2.4108\n",
      "Epoch 71/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5293 - loss: 1.94 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.5293 - loss: 1.9417 - val_accuracy: 0.4154 - val_loss: 2.4077\n",
      "Epoch 72/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5212 - loss: 1.94 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.5212 - loss: 1.9426 - val_accuracy: 0.4154 - val_loss: 2.4046\n",
      "Epoch 73/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 16s/step - accuracy: 0.5240 - loss: 1.94 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.5240 - loss: 1.9404 - val_accuracy: 0.4115 - val_loss: 2.4015\n",
      "Epoch 74/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5250 - loss: 1.93 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.5250 - loss: 1.9366 - val_accuracy: 0.4115 - val_loss: 2.3981\n",
      "Epoch 75/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5207 - loss: 1.93 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.5207 - loss: 1.9394 - val_accuracy: 0.4115 - val_loss: 2.3945\n",
      "Epoch 76/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.5149 - loss: 1.93 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.5149 - loss: 1.9390 - val_accuracy: 0.4135 - val_loss: 2.3908\n",
      "Epoch 77/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5245 - loss: 1.93 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.5245 - loss: 1.9356 - val_accuracy: 0.4173 - val_loss: 2.3870\n",
      "Epoch 78/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5255 - loss: 1.93 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.5255 - loss: 1.9355 - val_accuracy: 0.4154 - val_loss: 2.3833\n",
      "Epoch 79/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5178 - loss: 1.93 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.5178 - loss: 1.9344 - val_accuracy: 0.4154 - val_loss: 2.3795\n",
      "Epoch 80/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5284 - loss: 1.93 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.5284 - loss: 1.9307 - val_accuracy: 0.4154 - val_loss: 2.3758\n",
      "Epoch 81/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5192 - loss: 1.92 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.5192 - loss: 1.9257 - val_accuracy: 0.4135 - val_loss: 2.3724\n",
      "Epoch 82/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5418 - loss: 1.92 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.5418 - loss: 1.9236 - val_accuracy: 0.4135 - val_loss: 2.3690\n",
      "Epoch 83/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5202 - loss: 1.92 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.5202 - loss: 1.9275 - val_accuracy: 0.4096 - val_loss: 2.3657\n",
      "Epoch 84/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5216 - loss: 1.92 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.5216 - loss: 1.9242 - val_accuracy: 0.4096 - val_loss: 2.3623\n",
      "Epoch 85/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5226 - loss: 1.92 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.5226 - loss: 1.9227 - val_accuracy: 0.4115 - val_loss: 2.3589\n",
      "Epoch 86/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5264 - loss: 1.92 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.5264 - loss: 1.9202 - val_accuracy: 0.4154 - val_loss: 2.3553\n",
      "Epoch 87/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5231 - loss: 1.92 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.5231 - loss: 1.9200 - val_accuracy: 0.4192 - val_loss: 2.3517\n",
      "Epoch 88/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5312 - loss: 1.91 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.5312 - loss: 1.9166 - val_accuracy: 0.4192 - val_loss: 2.3480\n",
      "Epoch 89/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5202 - loss: 1.91 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.5202 - loss: 1.9178 - val_accuracy: 0.4192 - val_loss: 2.3444\n",
      "Epoch 90/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5327 - loss: 1.91 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.5327 - loss: 1.9121 - val_accuracy: 0.4192 - val_loss: 2.3410\n",
      "Epoch 91/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5356 - loss: 1.90 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.5356 - loss: 1.9082 - val_accuracy: 0.4135 - val_loss: 2.3376\n",
      "Epoch 92/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5279 - loss: 1.91 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.5279 - loss: 1.9105 - val_accuracy: 0.4135 - val_loss: 2.3342\n",
      "Epoch 93/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5274 - loss: 1.90 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.5274 - loss: 1.9062 - val_accuracy: 0.4173 - val_loss: 2.3307\n",
      "Epoch 94/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5389 - loss: 1.90 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.5389 - loss: 1.9066 - val_accuracy: 0.4192 - val_loss: 2.3273\n",
      "Epoch 95/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5250 - loss: 1.91 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.5250 - loss: 1.9105 - val_accuracy: 0.4173 - val_loss: 2.3239\n",
      "Epoch 96/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5437 - loss: 1.89 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.5437 - loss: 1.8991 - val_accuracy: 0.4135 - val_loss: 2.3204\n",
      "Epoch 97/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5308 - loss: 1.90 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.5308 - loss: 1.9045 - val_accuracy: 0.4173 - val_loss: 2.3168\n",
      "Epoch 98/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5341 - loss: 1.90 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.5341 - loss: 1.9008 - val_accuracy: 0.4173 - val_loss: 2.3133\n",
      "Epoch 99/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 20s/step - accuracy: 0.5341 - loss: 1.89 ━━━━━━━━━━━━━━━━━━━━ 21s 21s/step - accuracy: 0.5341 - loss: 1.8973 - val_accuracy: 0.4192 - val_loss: 2.3099\n",
      "Epoch 100/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5370 - loss: 1.89 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.5370 - loss: 1.8984 - val_accuracy: 0.4212 - val_loss: 2.3064\n",
      "Epoch 101/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5457 - loss: 1.89 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.5457 - loss: 1.8957 - val_accuracy: 0.4192 - val_loss: 2.3030\n",
      "Epoch 102/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5505 - loss: 1.89 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.5505 - loss: 1.8918 - val_accuracy: 0.4231 - val_loss: 2.2996\n",
      "Epoch 103/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5332 - loss: 1.88 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.5332 - loss: 1.8893 - val_accuracy: 0.4231 - val_loss: 2.2965\n",
      "Epoch 104/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5303 - loss: 1.89 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.5303 - loss: 1.8924 - val_accuracy: 0.4231 - val_loss: 2.2931\n",
      "Epoch 105/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5399 - loss: 1.88 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.5399 - loss: 1.8899 - val_accuracy: 0.4212 - val_loss: 2.2900\n",
      "Epoch 106/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5452 - loss: 1.88 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.5452 - loss: 1.8845 - val_accuracy: 0.4192 - val_loss: 2.2865\n",
      "Epoch 107/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5332 - loss: 1.88 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.5332 - loss: 1.8864 - val_accuracy: 0.4192 - val_loss: 2.2828\n",
      "Epoch 108/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5370 - loss: 1.88 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.5370 - loss: 1.8851 - val_accuracy: 0.4212 - val_loss: 2.2790\n",
      "Epoch 109/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5351 - loss: 1.88 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.5351 - loss: 1.8824 - val_accuracy: 0.4250 - val_loss: 2.2752\n",
      "Epoch 110/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5337 - loss: 1.88 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.5337 - loss: 1.8812 - val_accuracy: 0.4288 - val_loss: 2.2714\n",
      "Epoch 111/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5553 - loss: 1.87 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.5553 - loss: 1.8762 - val_accuracy: 0.4269 - val_loss: 2.2678\n",
      "Epoch 112/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5452 - loss: 1.87 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.5452 - loss: 1.8777 - val_accuracy: 0.4346 - val_loss: 2.2644\n",
      "Epoch 113/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5409 - loss: 1.87 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.5409 - loss: 1.8777 - val_accuracy: 0.4346 - val_loss: 2.2612\n",
      "Epoch 114/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5404 - loss: 1.87 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.5404 - loss: 1.8743 - val_accuracy: 0.4346 - val_loss: 2.2580\n",
      "Epoch 115/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5394 - loss: 1.87 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.5394 - loss: 1.8742 - val_accuracy: 0.4365 - val_loss: 2.2550\n",
      "Epoch 116/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5457 - loss: 1.87 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.5457 - loss: 1.8724 - val_accuracy: 0.4365 - val_loss: 2.2517\n",
      "Epoch 117/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5462 - loss: 1.87 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.5462 - loss: 1.8724 - val_accuracy: 0.4404 - val_loss: 2.2482\n",
      "Epoch 118/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5476 - loss: 1.86 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.5476 - loss: 1.8679 - val_accuracy: 0.4423 - val_loss: 2.2447\n",
      "Epoch 119/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19s/step - accuracy: 0.5433 - loss: 1.87 ━━━━━━━━━━━━━━━━━━━━ 20s 20s/step - accuracy: 0.5433 - loss: 1.8713 - val_accuracy: 0.4423 - val_loss: 2.2409\n",
      "Epoch 120/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 927s/step - accuracy: 0.5505 - loss: 1.868 ━━━━━━━━━━━━━━━━━━━━ 928s 928s/step - accuracy: 0.5505 - loss: 1.8684 - val_accuracy: 0.4442 - val_loss: 2.2369\n",
      "Epoch 121/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 16s/step - accuracy: 0.5490 - loss: 1.86 ━━━━━━━━━━━━━━━━━━━━ 17s 17s/step - accuracy: 0.5490 - loss: 1.8623 - val_accuracy: 0.4462 - val_loss: 2.2330\n",
      "Epoch 122/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 12s/step - accuracy: 0.5471 - loss: 1.86 ━━━━━━━━━━━━━━━━━━━━ 13s 13s/step - accuracy: 0.5471 - loss: 1.8637 - val_accuracy: 0.4481 - val_loss: 2.2295\n",
      "Epoch 123/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 11s/step - accuracy: 0.5361 - loss: 1.86 ━━━━━━━━━━━━━━━━━━━━ 12s 12s/step - accuracy: 0.5361 - loss: 1.8628 - val_accuracy: 0.4462 - val_loss: 2.2259\n",
      "Epoch 124/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 12s/step - accuracy: 0.5462 - loss: 1.86 ━━━━━━━━━━━━━━━━━━━━ 13s 13s/step - accuracy: 0.5462 - loss: 1.8630 - val_accuracy: 0.4481 - val_loss: 2.2225\n",
      "Epoch 125/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 12s/step - accuracy: 0.5495 - loss: 1.86 ━━━━━━━━━━━━━━━━━━━━ 13s 13s/step - accuracy: 0.5495 - loss: 1.8619 - val_accuracy: 0.4442 - val_loss: 2.2191\n",
      "Epoch 126/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 12s/step - accuracy: 0.5447 - loss: 1.86 ━━━━━━━━━━━━━━━━━━━━ 13s 13s/step - accuracy: 0.5447 - loss: 1.8623 - val_accuracy: 0.4519 - val_loss: 2.2159\n",
      "Epoch 127/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 21s/step - accuracy: 0.5471 - loss: 1.85 ━━━━━━━━━━━━━━━━━━━━ 23s 23s/step - accuracy: 0.5471 - loss: 1.8593 - val_accuracy: 0.4538 - val_loss: 2.2126\n",
      "Epoch 128/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 16s/step - accuracy: 0.5442 - loss: 1.85 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.5442 - loss: 1.8574 - val_accuracy: 0.4519 - val_loss: 2.2095\n",
      "Epoch 129/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5519 - loss: 1.85 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.5519 - loss: 1.8525 - val_accuracy: 0.4500 - val_loss: 2.2063\n",
      "Epoch 130/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5481 - loss: 1.85 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.5481 - loss: 1.8535 - val_accuracy: 0.4500 - val_loss: 2.2032\n",
      "Epoch 131/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 16s/step - accuracy: 0.5486 - loss: 1.85 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.5486 - loss: 1.8521 - val_accuracy: 0.4519 - val_loss: 2.1996\n",
      "Epoch 132/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5481 - loss: 1.85 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.5481 - loss: 1.8527 - val_accuracy: 0.4500 - val_loss: 2.1961\n",
      "Epoch 133/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5471 - loss: 1.84 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.5471 - loss: 1.8447 - val_accuracy: 0.4500 - val_loss: 2.1925\n",
      "Epoch 134/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5505 - loss: 1.84 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.5505 - loss: 1.8485 - val_accuracy: 0.4500 - val_loss: 2.1891\n",
      "Epoch 135/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5385 - loss: 1.84 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.5385 - loss: 1.8483 - val_accuracy: 0.4519 - val_loss: 2.1858\n",
      "Epoch 136/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5558 - loss: 1.84 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.5558 - loss: 1.8477 - val_accuracy: 0.4500 - val_loss: 2.1827\n",
      "Epoch 137/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5505 - loss: 1.84 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.5505 - loss: 1.8420 - val_accuracy: 0.4423 - val_loss: 2.1798\n",
      "Epoch 138/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5644 - loss: 1.83 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.5644 - loss: 1.8376 - val_accuracy: 0.4462 - val_loss: 2.1770\n",
      "Epoch 139/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5558 - loss: 1.83 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.5558 - loss: 1.8390 - val_accuracy: 0.4481 - val_loss: 2.1739\n",
      "Epoch 140/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5567 - loss: 1.83 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.5567 - loss: 1.8370 - val_accuracy: 0.4481 - val_loss: 2.1703\n",
      "Epoch 141/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5490 - loss: 1.83 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.5490 - loss: 1.8370 - val_accuracy: 0.4481 - val_loss: 2.1664\n",
      "Epoch 142/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5500 - loss: 1.83 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.5500 - loss: 1.8341 - val_accuracy: 0.4481 - val_loss: 2.1627\n",
      "Epoch 143/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5486 - loss: 1.83 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.5486 - loss: 1.8341 - val_accuracy: 0.4481 - val_loss: 2.1595\n",
      "Epoch 144/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5481 - loss: 1.83 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.5481 - loss: 1.8356 - val_accuracy: 0.4462 - val_loss: 2.1566\n",
      "Epoch 145/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5635 - loss: 1.82 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.5635 - loss: 1.8288 - val_accuracy: 0.4500 - val_loss: 2.1540\n",
      "Epoch 146/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5562 - loss: 1.82 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.5562 - loss: 1.8291 - val_accuracy: 0.4481 - val_loss: 2.1510\n",
      "Epoch 147/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5529 - loss: 1.83 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.5529 - loss: 1.8320 - val_accuracy: 0.4462 - val_loss: 2.1477\n",
      "Epoch 148/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5510 - loss: 1.82 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.5510 - loss: 1.8294 - val_accuracy: 0.4462 - val_loss: 2.1442\n",
      "Epoch 149/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5538 - loss: 1.82 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.5538 - loss: 1.8273 - val_accuracy: 0.4481 - val_loss: 2.1409\n",
      "Epoch 150/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5620 - loss: 1.82 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.5620 - loss: 1.8266 - val_accuracy: 0.4481 - val_loss: 2.1375\n",
      "Epoch 151/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5553 - loss: 1.82 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.5553 - loss: 1.8224 - val_accuracy: 0.4462 - val_loss: 2.1341\n",
      "Epoch 152/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5659 - loss: 1.81 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.5659 - loss: 1.8180 - val_accuracy: 0.4462 - val_loss: 2.1309\n",
      "Epoch 153/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5562 - loss: 1.81 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.5562 - loss: 1.8181 - val_accuracy: 0.4462 - val_loss: 2.1278\n",
      "Epoch 154/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5678 - loss: 1.81 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.5678 - loss: 1.8174 - val_accuracy: 0.4442 - val_loss: 2.1248\n",
      "Epoch 155/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5548 - loss: 1.82 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.5548 - loss: 1.8206 - val_accuracy: 0.4442 - val_loss: 2.1216\n",
      "Epoch 156/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5577 - loss: 1.81 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.5577 - loss: 1.8145 - val_accuracy: 0.4423 - val_loss: 2.1184\n",
      "Epoch 157/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5639 - loss: 1.81 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.5639 - loss: 1.8164 - val_accuracy: 0.4442 - val_loss: 2.1153\n",
      "Epoch 158/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5553 - loss: 1.82 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.5553 - loss: 1.8218 - val_accuracy: 0.4442 - val_loss: 2.1124\n",
      "Epoch 159/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5558 - loss: 1.81 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.5558 - loss: 1.8103 - val_accuracy: 0.4442 - val_loss: 2.1094\n",
      "Epoch 160/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5639 - loss: 1.81 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.5639 - loss: 1.8130 - val_accuracy: 0.4442 - val_loss: 2.1065\n",
      "Epoch 161/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5620 - loss: 1.80 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.5620 - loss: 1.8094 - val_accuracy: 0.4462 - val_loss: 2.1036\n",
      "Epoch 162/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5587 - loss: 1.80 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.5587 - loss: 1.8087 - val_accuracy: 0.4500 - val_loss: 2.1005\n",
      "Epoch 163/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5558 - loss: 1.80 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.5558 - loss: 1.8072 - val_accuracy: 0.4500 - val_loss: 2.0975\n",
      "Epoch 164/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5630 - loss: 1.80 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.5630 - loss: 1.8088 - val_accuracy: 0.4519 - val_loss: 2.0945\n",
      "Epoch 165/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5731 - loss: 1.80 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.5731 - loss: 1.8043 - val_accuracy: 0.4462 - val_loss: 2.0914\n",
      "Epoch 166/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5625 - loss: 1.80 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.5625 - loss: 1.8035 - val_accuracy: 0.4481 - val_loss: 2.0883\n",
      "Epoch 167/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5625 - loss: 1.80 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.5625 - loss: 1.8032 - val_accuracy: 0.4500 - val_loss: 2.0850\n",
      "Epoch 168/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5611 - loss: 1.79 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.5611 - loss: 1.7991 - val_accuracy: 0.4519 - val_loss: 2.0820\n",
      "Epoch 169/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19s/step - accuracy: 0.5673 - loss: 1.79 ━━━━━━━━━━━━━━━━━━━━ 21s 21s/step - accuracy: 0.5673 - loss: 1.7973 - val_accuracy: 0.4519 - val_loss: 2.0790\n",
      "Epoch 170/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5620 - loss: 1.80 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.5620 - loss: 1.8002 - val_accuracy: 0.4481 - val_loss: 2.0764\n",
      "Epoch 171/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5731 - loss: 1.79 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.5731 - loss: 1.7987 - val_accuracy: 0.4442 - val_loss: 2.0738\n",
      "Epoch 172/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 16s/step - accuracy: 0.5712 - loss: 1.79 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.5712 - loss: 1.7949 - val_accuracy: 0.4442 - val_loss: 2.0712\n",
      "Epoch 173/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5567 - loss: 1.79 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.5567 - loss: 1.7968 - val_accuracy: 0.4442 - val_loss: 2.0685\n",
      "Epoch 174/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5644 - loss: 1.79 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.5644 - loss: 1.7936 - val_accuracy: 0.4442 - val_loss: 2.0657\n",
      "Epoch 175/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5712 - loss: 1.79 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.5712 - loss: 1.7920 - val_accuracy: 0.4442 - val_loss: 2.0632\n",
      "Epoch 176/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5678 - loss: 1.78 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.5678 - loss: 1.7882 - val_accuracy: 0.4404 - val_loss: 2.0612\n",
      "Epoch 177/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5663 - loss: 1.79 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.5663 - loss: 1.7919 - val_accuracy: 0.4404 - val_loss: 2.0590\n",
      "Epoch 178/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5601 - loss: 1.79 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.5601 - loss: 1.7905 - val_accuracy: 0.4404 - val_loss: 2.0564\n",
      "Epoch 179/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 16s/step - accuracy: 0.5673 - loss: 1.78 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.5673 - loss: 1.7895 - val_accuracy: 0.4404 - val_loss: 2.0536\n",
      "Epoch 180/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5524 - loss: 1.79 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.5524 - loss: 1.7932 - val_accuracy: 0.4404 - val_loss: 2.0509\n",
      "Epoch 181/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5678 - loss: 1.78 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.5678 - loss: 1.7855 - val_accuracy: 0.4365 - val_loss: 2.0484\n",
      "Epoch 182/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5620 - loss: 1.78 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.5620 - loss: 1.7832 - val_accuracy: 0.4385 - val_loss: 2.0463\n",
      "Epoch 183/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5649 - loss: 1.78 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.5649 - loss: 1.7835 - val_accuracy: 0.4385 - val_loss: 2.0441\n",
      "Epoch 184/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5731 - loss: 1.78 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.5731 - loss: 1.7866 - val_accuracy: 0.4385 - val_loss: 2.0416\n",
      "Epoch 185/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5846 - loss: 1.78 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.5846 - loss: 1.7846 - val_accuracy: 0.4404 - val_loss: 2.0390\n",
      "Epoch 186/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5644 - loss: 1.78 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.5644 - loss: 1.7823 - val_accuracy: 0.4423 - val_loss: 2.0361\n",
      "Epoch 187/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5731 - loss: 1.77 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.5731 - loss: 1.7776 - val_accuracy: 0.4423 - val_loss: 2.0332\n",
      "Epoch 188/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5731 - loss: 1.77 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.5731 - loss: 1.7754 - val_accuracy: 0.4404 - val_loss: 2.0304\n",
      "Epoch 189/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.5707 - loss: 1.78 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.5707 - loss: 1.7820 - val_accuracy: 0.4365 - val_loss: 2.0279\n",
      "Epoch 190/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 16s/step - accuracy: 0.5788 - loss: 1.77 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.5788 - loss: 1.7722 - val_accuracy: 0.4346 - val_loss: 2.0255\n",
      "Epoch 191/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5688 - loss: 1.77 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.5688 - loss: 1.7718 - val_accuracy: 0.4385 - val_loss: 2.0232\n",
      "Epoch 192/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5808 - loss: 1.77 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.5808 - loss: 1.7721 - val_accuracy: 0.4404 - val_loss: 2.0209\n",
      "Epoch 193/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5798 - loss: 1.76 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.5798 - loss: 1.7697 - val_accuracy: 0.4442 - val_loss: 2.0187\n",
      "Epoch 194/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5615 - loss: 1.77 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.5615 - loss: 1.7718 - val_accuracy: 0.4442 - val_loss: 2.0167\n",
      "Epoch 195/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5683 - loss: 1.77 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.5683 - loss: 1.7729 - val_accuracy: 0.4442 - val_loss: 2.0148\n",
      "Epoch 196/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5649 - loss: 1.76 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.5649 - loss: 1.7698 - val_accuracy: 0.4423 - val_loss: 2.0128\n",
      "Epoch 197/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5659 - loss: 1.76 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.5659 - loss: 1.7648 - val_accuracy: 0.4385 - val_loss: 2.0108\n",
      "Epoch 198/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5716 - loss: 1.76 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.5716 - loss: 1.7677 - val_accuracy: 0.4404 - val_loss: 2.0086\n",
      "Epoch 199/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5673 - loss: 1.76 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.5673 - loss: 1.7642 - val_accuracy: 0.4404 - val_loss: 2.0059\n",
      "Epoch 200/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 16s/step - accuracy: 0.5760 - loss: 1.76 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.5760 - loss: 1.7604 - val_accuracy: 0.4385 - val_loss: 2.0030\n",
      "Epoch 201/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5707 - loss: 1.75 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.5707 - loss: 1.7591 - val_accuracy: 0.4385 - val_loss: 2.0004\n",
      "Epoch 202/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5668 - loss: 1.75 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.5668 - loss: 1.7578 - val_accuracy: 0.4423 - val_loss: 1.9979\n",
      "Epoch 203/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5760 - loss: 1.76 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.5760 - loss: 1.7619 - val_accuracy: 0.4462 - val_loss: 1.9958\n",
      "Epoch 204/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5750 - loss: 1.75 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.5750 - loss: 1.7563 - val_accuracy: 0.4481 - val_loss: 1.9940\n",
      "Epoch 205/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5784 - loss: 1.75 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.5784 - loss: 1.7547 - val_accuracy: 0.4481 - val_loss: 1.9922\n",
      "Epoch 206/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5803 - loss: 1.75 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.5803 - loss: 1.7523 - val_accuracy: 0.4481 - val_loss: 1.9902\n",
      "Epoch 207/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5707 - loss: 1.75 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.5707 - loss: 1.7582 - val_accuracy: 0.4442 - val_loss: 1.9880\n",
      "Epoch 208/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5755 - loss: 1.75 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.5755 - loss: 1.7588 - val_accuracy: 0.4423 - val_loss: 1.9855\n",
      "Epoch 209/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5731 - loss: 1.75 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.5731 - loss: 1.7518 - val_accuracy: 0.4442 - val_loss: 1.9834\n",
      "Epoch 210/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5784 - loss: 1.75 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.5784 - loss: 1.7518 - val_accuracy: 0.4442 - val_loss: 1.9816\n",
      "Epoch 211/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5745 - loss: 1.75 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.5745 - loss: 1.7521 - val_accuracy: 0.4462 - val_loss: 1.9795\n",
      "Epoch 212/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5822 - loss: 1.75 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.5822 - loss: 1.7543 - val_accuracy: 0.4481 - val_loss: 1.9772\n",
      "Epoch 213/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5822 - loss: 1.74 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.5822 - loss: 1.7498 - val_accuracy: 0.4462 - val_loss: 1.9746\n",
      "Epoch 214/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5721 - loss: 1.74 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.5721 - loss: 1.7485 - val_accuracy: 0.4442 - val_loss: 1.9721\n",
      "Epoch 215/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5798 - loss: 1.74 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.5798 - loss: 1.7411 - val_accuracy: 0.4442 - val_loss: 1.9699\n",
      "Epoch 216/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5798 - loss: 1.74 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.5798 - loss: 1.7407 - val_accuracy: 0.4442 - val_loss: 1.9683\n",
      "Epoch 217/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5774 - loss: 1.74 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.5774 - loss: 1.7454 - val_accuracy: 0.4500 - val_loss: 1.9668\n",
      "Epoch 218/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 16s/step - accuracy: 0.5750 - loss: 1.74 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.5750 - loss: 1.7409 - val_accuracy: 0.4500 - val_loss: 1.9654\n",
      "Epoch 219/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5822 - loss: 1.74 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.5822 - loss: 1.7446 - val_accuracy: 0.4481 - val_loss: 1.9636\n",
      "Epoch 220/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5760 - loss: 1.74 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.5760 - loss: 1.7411 - val_accuracy: 0.4442 - val_loss: 1.9619\n",
      "Epoch 221/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5779 - loss: 1.74 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.5779 - loss: 1.7431 - val_accuracy: 0.4462 - val_loss: 1.9604\n",
      "Epoch 222/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5726 - loss: 1.74 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.5726 - loss: 1.7433 - val_accuracy: 0.4481 - val_loss: 1.9585\n",
      "Epoch 223/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 16s/step - accuracy: 0.5673 - loss: 1.74 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.5673 - loss: 1.7403 - val_accuracy: 0.4481 - val_loss: 1.9565\n",
      "Epoch 224/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5841 - loss: 1.73 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.5841 - loss: 1.7318 - val_accuracy: 0.4462 - val_loss: 1.9547\n",
      "Epoch 225/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5899 - loss: 1.73 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.5899 - loss: 1.7338 - val_accuracy: 0.4481 - val_loss: 1.9528\n",
      "Epoch 226/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5928 - loss: 1.73 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.5928 - loss: 1.7328 - val_accuracy: 0.4500 - val_loss: 1.9509\n",
      "Epoch 227/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5808 - loss: 1.73 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.5808 - loss: 1.7357 - val_accuracy: 0.4500 - val_loss: 1.9489\n",
      "Epoch 228/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5793 - loss: 1.73 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.5793 - loss: 1.7308 - val_accuracy: 0.4481 - val_loss: 1.9468\n",
      "Epoch 229/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5827 - loss: 1.73 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.5827 - loss: 1.7300 - val_accuracy: 0.4500 - val_loss: 1.9448\n",
      "Epoch 230/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5841 - loss: 1.73 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.5841 - loss: 1.7339 - val_accuracy: 0.4519 - val_loss: 1.9429\n",
      "Epoch 231/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5740 - loss: 1.73 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.5740 - loss: 1.7313 - val_accuracy: 0.4481 - val_loss: 1.9405\n",
      "Epoch 232/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5822 - loss: 1.72 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.5822 - loss: 1.7273 - val_accuracy: 0.4442 - val_loss: 1.9384\n",
      "Epoch 233/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5851 - loss: 1.72 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.5851 - loss: 1.7234 - val_accuracy: 0.4423 - val_loss: 1.9367\n",
      "Epoch 234/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5712 - loss: 1.72 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.5712 - loss: 1.7236 - val_accuracy: 0.4442 - val_loss: 1.9354\n",
      "Epoch 235/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5750 - loss: 1.72 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.5750 - loss: 1.7279 - val_accuracy: 0.4442 - val_loss: 1.9341\n",
      "Epoch 236/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5817 - loss: 1.72 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.5817 - loss: 1.7249 - val_accuracy: 0.4365 - val_loss: 1.9328\n",
      "Epoch 237/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5837 - loss: 1.71 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.5837 - loss: 1.7196 - val_accuracy: 0.4365 - val_loss: 1.9317\n",
      "Epoch 238/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5813 - loss: 1.72 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.5813 - loss: 1.7290 - val_accuracy: 0.4442 - val_loss: 1.9311\n",
      "Epoch 239/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5870 - loss: 1.71 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.5870 - loss: 1.7171 - val_accuracy: 0.4462 - val_loss: 1.9296\n",
      "Epoch 240/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5813 - loss: 1.71 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.5813 - loss: 1.7189 - val_accuracy: 0.4462 - val_loss: 1.9272\n",
      "Epoch 241/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5851 - loss: 1.71 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.5851 - loss: 1.7192 - val_accuracy: 0.4423 - val_loss: 1.9249\n",
      "Epoch 242/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 16s/step - accuracy: 0.5779 - loss: 1.71 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.5779 - loss: 1.7193 - val_accuracy: 0.4442 - val_loss: 1.9233\n",
      "Epoch 243/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5764 - loss: 1.72 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.5764 - loss: 1.7212 - val_accuracy: 0.4423 - val_loss: 1.9218\n",
      "Epoch 244/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5909 - loss: 1.71 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.5909 - loss: 1.7146 - val_accuracy: 0.4442 - val_loss: 1.9205\n",
      "Epoch 245/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5755 - loss: 1.71 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.5755 - loss: 1.7107 - val_accuracy: 0.4462 - val_loss: 1.9198\n",
      "Epoch 246/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5760 - loss: 1.71 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.5760 - loss: 1.7138 - val_accuracy: 0.4385 - val_loss: 1.9189\n",
      "Epoch 247/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5861 - loss: 1.71 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.5861 - loss: 1.7173 - val_accuracy: 0.4365 - val_loss: 1.9176\n",
      "Epoch 248/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5894 - loss: 1.71 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.5894 - loss: 1.7198 - val_accuracy: 0.4327 - val_loss: 1.9152\n",
      "Epoch 249/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5813 - loss: 1.70 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.5813 - loss: 1.7058 - val_accuracy: 0.4365 - val_loss: 1.9131\n",
      "Epoch 250/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5832 - loss: 1.70 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.5832 - loss: 1.7078 - val_accuracy: 0.4385 - val_loss: 1.9115\n",
      "Epoch 251/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5788 - loss: 1.70 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.5788 - loss: 1.7083 - val_accuracy: 0.4308 - val_loss: 1.9102\n",
      "Epoch 252/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5938 - loss: 1.70 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.5938 - loss: 1.7039 - val_accuracy: 0.4327 - val_loss: 1.9086\n",
      "Epoch 253/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5933 - loss: 1.70 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.5933 - loss: 1.7060 - val_accuracy: 0.4327 - val_loss: 1.9075\n",
      "Epoch 254/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5981 - loss: 1.70 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.5981 - loss: 1.7004 - val_accuracy: 0.4308 - val_loss: 1.9061\n",
      "Epoch 255/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5817 - loss: 1.70 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.5817 - loss: 1.7008 - val_accuracy: 0.4365 - val_loss: 1.9044\n",
      "Epoch 256/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5851 - loss: 1.70 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.5851 - loss: 1.7068 - val_accuracy: 0.4327 - val_loss: 1.9020\n",
      "Epoch 257/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5865 - loss: 1.70 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.5865 - loss: 1.7008 - val_accuracy: 0.4327 - val_loss: 1.8998\n",
      "Epoch 258/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5889 - loss: 1.70 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.5889 - loss: 1.7039 - val_accuracy: 0.4288 - val_loss: 1.8984\n",
      "Epoch 259/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5846 - loss: 1.70 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.5846 - loss: 1.7061 - val_accuracy: 0.4288 - val_loss: 1.8975\n",
      "Epoch 260/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5784 - loss: 1.70 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.5784 - loss: 1.7021 - val_accuracy: 0.4346 - val_loss: 1.8962\n",
      "Epoch 261/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.6058 - loss: 1.69 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.6058 - loss: 1.6930 - val_accuracy: 0.4250 - val_loss: 1.8954\n",
      "Epoch 262/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5870 - loss: 1.70 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.5870 - loss: 1.7002 - val_accuracy: 0.4250 - val_loss: 1.8953\n",
      "Epoch 263/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.6014 - loss: 1.69 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.6014 - loss: 1.6901 - val_accuracy: 0.4269 - val_loss: 1.8940\n",
      "Epoch 264/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5817 - loss: 1.69 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.5817 - loss: 1.6954 - val_accuracy: 0.4365 - val_loss: 1.8927\n",
      "Epoch 265/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5880 - loss: 1.70 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.5880 - loss: 1.7011 - val_accuracy: 0.4346 - val_loss: 1.8914\n",
      "Epoch 266/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5837 - loss: 1.69 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.5837 - loss: 1.6974 - val_accuracy: 0.4288 - val_loss: 1.8899\n",
      "Epoch 267/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5942 - loss: 1.69 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.5942 - loss: 1.6901 - val_accuracy: 0.4346 - val_loss: 1.8883\n",
      "Epoch 268/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5918 - loss: 1.69 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.5918 - loss: 1.6906 - val_accuracy: 0.4404 - val_loss: 1.8870\n",
      "Epoch 269/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5904 - loss: 1.69 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.5904 - loss: 1.6946 - val_accuracy: 0.4346 - val_loss: 1.8859\n",
      "Epoch 270/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5894 - loss: 1.68 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.5894 - loss: 1.6897 - val_accuracy: 0.4346 - val_loss: 1.8839\n",
      "Epoch 271/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5880 - loss: 1.68 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.5880 - loss: 1.6893 - val_accuracy: 0.4385 - val_loss: 1.8823\n",
      "Epoch 272/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5971 - loss: 1.69 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.5971 - loss: 1.6909 - val_accuracy: 0.4346 - val_loss: 1.8818\n",
      "Epoch 273/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5947 - loss: 1.68 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.5947 - loss: 1.6872 - val_accuracy: 0.4346 - val_loss: 1.8813\n",
      "Epoch 274/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5913 - loss: 1.68 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.5913 - loss: 1.6859 - val_accuracy: 0.4365 - val_loss: 1.8796\n",
      "Epoch 275/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5870 - loss: 1.68 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.5870 - loss: 1.6855 - val_accuracy: 0.4365 - val_loss: 1.8779\n",
      "Epoch 276/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5885 - loss: 1.68 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.5885 - loss: 1.6882 - val_accuracy: 0.4308 - val_loss: 1.8762\n",
      "Epoch 277/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5832 - loss: 1.68 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.5832 - loss: 1.6829 - val_accuracy: 0.4250 - val_loss: 1.8747\n",
      "Epoch 278/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5899 - loss: 1.68 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.5899 - loss: 1.6819 - val_accuracy: 0.4269 - val_loss: 1.8733\n",
      "Epoch 279/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5947 - loss: 1.68 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.5947 - loss: 1.6807 - val_accuracy: 0.4212 - val_loss: 1.8721\n",
      "Epoch 280/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5889 - loss: 1.68 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.5889 - loss: 1.6830 - val_accuracy: 0.4250 - val_loss: 1.8706\n",
      "Epoch 281/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5990 - loss: 1.68 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.5990 - loss: 1.6818 - val_accuracy: 0.4269 - val_loss: 1.8695\n",
      "Epoch 282/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5976 - loss: 1.67 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.5976 - loss: 1.6747 - val_accuracy: 0.4346 - val_loss: 1.8685\n",
      "Epoch 283/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 16s/step - accuracy: 0.6010 - loss: 1.67 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.6010 - loss: 1.6778 - val_accuracy: 0.4365 - val_loss: 1.8671\n",
      "Epoch 284/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.6043 - loss: 1.67 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.6043 - loss: 1.6717 - val_accuracy: 0.4346 - val_loss: 1.8659\n",
      "Epoch 285/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.6062 - loss: 1.67 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.6062 - loss: 1.6716 - val_accuracy: 0.4288 - val_loss: 1.8652\n",
      "Epoch 286/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5885 - loss: 1.67 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.5885 - loss: 1.6731 - val_accuracy: 0.4365 - val_loss: 1.8642\n",
      "Epoch 287/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.6072 - loss: 1.67 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.6072 - loss: 1.6769 - val_accuracy: 0.4385 - val_loss: 1.8634\n",
      "Epoch 288/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.5875 - loss: 1.67 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.5875 - loss: 1.6737 - val_accuracy: 0.4327 - val_loss: 1.8620\n",
      "Epoch 289/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.5990 - loss: 1.66 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.5990 - loss: 1.6690 - val_accuracy: 0.4346 - val_loss: 1.8603\n",
      "Epoch 290/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5894 - loss: 1.67 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.5894 - loss: 1.6723 - val_accuracy: 0.4442 - val_loss: 1.8598\n",
      "Epoch 291/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5904 - loss: 1.67 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.5904 - loss: 1.6704 - val_accuracy: 0.4481 - val_loss: 1.8587\n",
      "Epoch 292/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.6053 - loss: 1.66 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.6053 - loss: 1.6680 - val_accuracy: 0.4385 - val_loss: 1.8578\n",
      "Epoch 293/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.6072 - loss: 1.66 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.6072 - loss: 1.6659 - val_accuracy: 0.4404 - val_loss: 1.8574\n",
      "Epoch 294/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5962 - loss: 1.67 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.5962 - loss: 1.6707 - val_accuracy: 0.4462 - val_loss: 1.8568\n",
      "Epoch 295/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.6024 - loss: 1.66 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.6024 - loss: 1.6673 - val_accuracy: 0.4442 - val_loss: 1.8567\n",
      "Epoch 296/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.6072 - loss: 1.66 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.6072 - loss: 1.6632 - val_accuracy: 0.4365 - val_loss: 1.8553\n",
      "Epoch 297/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.6014 - loss: 1.66 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.6014 - loss: 1.6685 - val_accuracy: 0.4423 - val_loss: 1.8538\n",
      "Epoch 298/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.6029 - loss: 1.65 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.6029 - loss: 1.6587 - val_accuracy: 0.4442 - val_loss: 1.8527\n",
      "Epoch 299/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5928 - loss: 1.66 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.5928 - loss: 1.6688 - val_accuracy: 0.4423 - val_loss: 1.8518\n",
      "Epoch 300/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5880 - loss: 1.66 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.5880 - loss: 1.6639 - val_accuracy: 0.4558 - val_loss: 1.8502\n",
      "Epoch 301/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5957 - loss: 1.66 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.5957 - loss: 1.6637 - val_accuracy: 0.4385 - val_loss: 1.8484\n",
      "Epoch 302/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.6048 - loss: 1.65 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.6048 - loss: 1.6593 - val_accuracy: 0.4385 - val_loss: 1.8472\n",
      "Epoch 303/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.6014 - loss: 1.65 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.6014 - loss: 1.6580 - val_accuracy: 0.4423 - val_loss: 1.8463\n",
      "Epoch 304/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5909 - loss: 1.66 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.5909 - loss: 1.6629 - val_accuracy: 0.4423 - val_loss: 1.8452\n",
      "Epoch 305/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.6043 - loss: 1.66 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.6043 - loss: 1.6601 - val_accuracy: 0.4385 - val_loss: 1.8436\n",
      "Epoch 306/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.6072 - loss: 1.65 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.6072 - loss: 1.6551 - val_accuracy: 0.4404 - val_loss: 1.8419\n",
      "Epoch 307/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.6019 - loss: 1.65 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.6019 - loss: 1.6556 - val_accuracy: 0.4385 - val_loss: 1.8410\n",
      "Epoch 308/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5947 - loss: 1.65 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.5947 - loss: 1.6516 - val_accuracy: 0.4500 - val_loss: 1.8403\n",
      "Epoch 309/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5995 - loss: 1.65 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.5995 - loss: 1.6527 - val_accuracy: 0.4288 - val_loss: 1.8403\n",
      "Epoch 310/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5875 - loss: 1.65 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.5875 - loss: 1.6561 - val_accuracy: 0.4481 - val_loss: 1.8398\n",
      "Epoch 311/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5918 - loss: 1.65 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.5918 - loss: 1.6536 - val_accuracy: 0.4462 - val_loss: 1.8402\n",
      "Epoch 312/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5923 - loss: 1.65 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.5923 - loss: 1.6526 - val_accuracy: 0.4519 - val_loss: 1.8399\n",
      "Epoch 313/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5986 - loss: 1.64 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.5986 - loss: 1.6496 - val_accuracy: 0.4481 - val_loss: 1.8390\n",
      "Epoch 314/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.6154 - loss: 1.64 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.6154 - loss: 1.6445 - val_accuracy: 0.4365 - val_loss: 1.8375\n",
      "Epoch 315/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.6120 - loss: 1.65 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.6120 - loss: 1.6504 - val_accuracy: 0.4365 - val_loss: 1.8367\n",
      "Epoch 316/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.6096 - loss: 1.64 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.6096 - loss: 1.6462 - val_accuracy: 0.4423 - val_loss: 1.8362\n",
      "Epoch 317/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.6034 - loss: 1.64 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.6034 - loss: 1.6447 - val_accuracy: 0.4346 - val_loss: 1.8356\n",
      "Epoch 318/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5966 - loss: 1.64 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.5966 - loss: 1.6481 - val_accuracy: 0.4442 - val_loss: 1.8334\n",
      "Epoch 319/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.6048 - loss: 1.64 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.6048 - loss: 1.6453 - val_accuracy: 0.4481 - val_loss: 1.8319\n",
      "Epoch 320/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.6000 - loss: 1.64 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.6000 - loss: 1.6449 - val_accuracy: 0.4423 - val_loss: 1.8314\n",
      "Epoch 321/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.6024 - loss: 1.64 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.6024 - loss: 1.6469 - val_accuracy: 0.4558 - val_loss: 1.8314\n",
      "Epoch 322/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.6082 - loss: 1.64 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.6082 - loss: 1.6418 - val_accuracy: 0.4538 - val_loss: 1.8290\n",
      "Epoch 323/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.6067 - loss: 1.64 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.6067 - loss: 1.6444 - val_accuracy: 0.4442 - val_loss: 1.8274\n",
      "Epoch 324/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.6139 - loss: 1.64 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.6139 - loss: 1.6407 - val_accuracy: 0.4558 - val_loss: 1.8264\n",
      "Epoch 325/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.6010 - loss: 1.63 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.6010 - loss: 1.6388 - val_accuracy: 0.4442 - val_loss: 1.8258\n",
      "Epoch 326/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.6101 - loss: 1.63 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.6101 - loss: 1.6390 - val_accuracy: 0.4519 - val_loss: 1.8244\n",
      "Epoch 327/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.6010 - loss: 1.63 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.6010 - loss: 1.6364 - val_accuracy: 0.4558 - val_loss: 1.8233\n",
      "Epoch 328/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 16s/step - accuracy: 0.6115 - loss: 1.63 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.6115 - loss: 1.6350 - val_accuracy: 0.4327 - val_loss: 1.8228\n",
      "Epoch 329/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.5990 - loss: 1.63 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.5990 - loss: 1.6393 - val_accuracy: 0.4500 - val_loss: 1.8213\n",
      "Epoch 330/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.6091 - loss: 1.63 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.6091 - loss: 1.6373 - val_accuracy: 0.4596 - val_loss: 1.8214\n",
      "Epoch 331/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5990 - loss: 1.63 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.5990 - loss: 1.6376 - val_accuracy: 0.4500 - val_loss: 1.8203\n",
      "Epoch 332/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.6087 - loss: 1.63 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.6087 - loss: 1.6345 - val_accuracy: 0.4481 - val_loss: 1.8195\n",
      "Epoch 333/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5981 - loss: 1.64 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.5981 - loss: 1.6413 - val_accuracy: 0.4462 - val_loss: 1.8190\n",
      "Epoch 334/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.6005 - loss: 1.63 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.6005 - loss: 1.6323 - val_accuracy: 0.4462 - val_loss: 1.8190\n",
      "Epoch 335/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.6053 - loss: 1.63 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.6053 - loss: 1.6331 - val_accuracy: 0.4558 - val_loss: 1.8178\n",
      "Epoch 336/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.6034 - loss: 1.63 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.6034 - loss: 1.6319 - val_accuracy: 0.4519 - val_loss: 1.8167\n",
      "Epoch 337/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.6115 - loss: 1.63 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.6115 - loss: 1.6311 - val_accuracy: 0.4423 - val_loss: 1.8166\n",
      "Epoch 338/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.6024 - loss: 1.62 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.6024 - loss: 1.6277 - val_accuracy: 0.4577 - val_loss: 1.8167\n",
      "Epoch 339/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5990 - loss: 1.63 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.5990 - loss: 1.6312 - val_accuracy: 0.4538 - val_loss: 1.8162\n",
      "Epoch 340/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.6058 - loss: 1.62 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.6058 - loss: 1.6272 - val_accuracy: 0.4423 - val_loss: 1.8155\n",
      "Epoch 341/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.6125 - loss: 1.62 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.6125 - loss: 1.6225 - val_accuracy: 0.4519 - val_loss: 1.8153\n",
      "Epoch 342/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.6072 - loss: 1.62 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.6072 - loss: 1.6227 - val_accuracy: 0.4481 - val_loss: 1.8156\n",
      "Epoch 343/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.6058 - loss: 1.62 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.6058 - loss: 1.6227 - val_accuracy: 0.4481 - val_loss: 1.8143\n",
      "Epoch 344/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.6096 - loss: 1.62 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.6096 - loss: 1.6206 - val_accuracy: 0.4404 - val_loss: 1.8125\n",
      "Epoch 345/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.6115 - loss: 1.62 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.6115 - loss: 1.6211 - val_accuracy: 0.4365 - val_loss: 1.8108\n",
      "Epoch 346/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.6183 - loss: 1.61 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.6183 - loss: 1.6178 - val_accuracy: 0.4442 - val_loss: 1.8094\n",
      "Epoch 347/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.6173 - loss: 1.61 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.6173 - loss: 1.6189 - val_accuracy: 0.4519 - val_loss: 1.8092\n",
      "Epoch 348/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.6019 - loss: 1.62 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.6019 - loss: 1.6261 - val_accuracy: 0.4481 - val_loss: 1.8085\n",
      "Epoch 349/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.6096 - loss: 1.62 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.6096 - loss: 1.6246 - val_accuracy: 0.4423 - val_loss: 1.8072\n",
      "Epoch 350/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.6082 - loss: 1.61 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.6082 - loss: 1.6181 - val_accuracy: 0.4423 - val_loss: 1.8066\n",
      "Epoch 351/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.6087 - loss: 1.61 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.6087 - loss: 1.6199 - val_accuracy: 0.4462 - val_loss: 1.8068\n",
      "Epoch 352/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.6005 - loss: 1.61 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.6005 - loss: 1.6171 - val_accuracy: 0.4462 - val_loss: 1.8069\n",
      "Epoch 353/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.6101 - loss: 1.62 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.6101 - loss: 1.6206 - val_accuracy: 0.4423 - val_loss: 1.8059\n",
      "Epoch 354/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.6038 - loss: 1.61 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.6038 - loss: 1.6147 - val_accuracy: 0.4462 - val_loss: 1.8052\n",
      "Epoch 355/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.6168 - loss: 1.61 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.6168 - loss: 1.6189 - val_accuracy: 0.4462 - val_loss: 1.8041\n",
      "Epoch 356/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.6168 - loss: 1.61 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.6168 - loss: 1.6116 - val_accuracy: 0.4500 - val_loss: 1.8038\n",
      "Epoch 357/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.6000 - loss: 1.61 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.6000 - loss: 1.6158 - val_accuracy: 0.4500 - val_loss: 1.8033\n",
      "Epoch 358/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.6096 - loss: 1.61 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.6096 - loss: 1.6119 - val_accuracy: 0.4423 - val_loss: 1.8027\n",
      "Epoch 359/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.6077 - loss: 1.60 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.6077 - loss: 1.6099 - val_accuracy: 0.4481 - val_loss: 1.8024\n",
      "Epoch 360/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.6029 - loss: 1.61 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.6029 - loss: 1.6122 - val_accuracy: 0.4519 - val_loss: 1.8020\n",
      "Epoch 361/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.5981 - loss: 1.61 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.5981 - loss: 1.6133 - val_accuracy: 0.4558 - val_loss: 1.8011\n",
      "Epoch 362/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 16s/step - accuracy: 0.6111 - loss: 1.60 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.6111 - loss: 1.6046 - val_accuracy: 0.4442 - val_loss: 1.8004\n",
      "Epoch 363/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.6255 - loss: 1.60 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.6255 - loss: 1.6052 - val_accuracy: 0.4596 - val_loss: 1.8010\n",
      "Epoch 364/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.6106 - loss: 1.60 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.6106 - loss: 1.6073 - val_accuracy: 0.4442 - val_loss: 1.8000\n",
      "Epoch 365/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.6144 - loss: 1.60 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.6144 - loss: 1.6058 - val_accuracy: 0.4538 - val_loss: 1.7992\n",
      "Epoch 366/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.6125 - loss: 1.61 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.6125 - loss: 1.6113 - val_accuracy: 0.4519 - val_loss: 1.7977\n",
      "Epoch 367/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.6058 - loss: 1.60 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.6058 - loss: 1.6096 - val_accuracy: 0.4500 - val_loss: 1.7965\n",
      "Epoch 368/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.6082 - loss: 1.60 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.6082 - loss: 1.6072 - val_accuracy: 0.4596 - val_loss: 1.7963\n",
      "Epoch 369/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.6072 - loss: 1.60 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.6072 - loss: 1.6051 - val_accuracy: 0.4442 - val_loss: 1.7945\n",
      "Epoch 370/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.6144 - loss: 1.60 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.6144 - loss: 1.6072 - val_accuracy: 0.4519 - val_loss: 1.7935\n",
      "Epoch 371/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.6226 - loss: 1.59 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.6226 - loss: 1.5978 - val_accuracy: 0.4462 - val_loss: 1.7915\n",
      "Epoch 372/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.6096 - loss: 1.60 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.6096 - loss: 1.6029 - val_accuracy: 0.4462 - val_loss: 1.7914\n",
      "Epoch 373/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.6168 - loss: 1.59 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.6168 - loss: 1.5987 - val_accuracy: 0.4500 - val_loss: 1.7916\n",
      "Epoch 374/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.6087 - loss: 1.60 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.6087 - loss: 1.6035 - val_accuracy: 0.4538 - val_loss: 1.7904\n",
      "Epoch 375/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.6163 - loss: 1.60 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.6163 - loss: 1.6006 - val_accuracy: 0.4442 - val_loss: 1.7902\n",
      "Epoch 376/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.6087 - loss: 1.59 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.6087 - loss: 1.5951 - val_accuracy: 0.4462 - val_loss: 1.7897\n",
      "Epoch 377/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.6202 - loss: 1.59 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.6202 - loss: 1.5966 - val_accuracy: 0.4596 - val_loss: 1.7894\n",
      "Epoch 378/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.6125 - loss: 1.59 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.6125 - loss: 1.5970 - val_accuracy: 0.4577 - val_loss: 1.7900\n",
      "Epoch 379/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.6144 - loss: 1.59 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.6144 - loss: 1.5947 - val_accuracy: 0.4577 - val_loss: 1.7893\n",
      "Epoch 380/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.6154 - loss: 1.59 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.6154 - loss: 1.5976 - val_accuracy: 0.4500 - val_loss: 1.7900\n",
      "Epoch 381/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.6082 - loss: 1.59 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.6082 - loss: 1.5976 - val_accuracy: 0.4538 - val_loss: 1.7890\n",
      "Epoch 382/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.6106 - loss: 1.59 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.6106 - loss: 1.5920 - val_accuracy: 0.4538 - val_loss: 1.7887\n",
      "Epoch 383/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.6173 - loss: 1.58 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.6173 - loss: 1.5884 - val_accuracy: 0.4538 - val_loss: 1.7875\n",
      "Epoch 384/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.6250 - loss: 1.58 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.6250 - loss: 1.5889 - val_accuracy: 0.4615 - val_loss: 1.7864\n",
      "Epoch 385/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.6216 - loss: 1.59 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.6216 - loss: 1.5900 - val_accuracy: 0.4481 - val_loss: 1.7862\n",
      "Epoch 386/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.6111 - loss: 1.59 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.6111 - loss: 1.5911 - val_accuracy: 0.4538 - val_loss: 1.7852\n",
      "Epoch 387/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.6250 - loss: 1.59 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.6250 - loss: 1.5907 - val_accuracy: 0.4500 - val_loss: 1.7857\n",
      "Epoch 388/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.6236 - loss: 1.58 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.6236 - loss: 1.5842 - val_accuracy: 0.4577 - val_loss: 1.7849\n",
      "Epoch 389/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.6207 - loss: 1.58 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.6207 - loss: 1.5848 - val_accuracy: 0.4615 - val_loss: 1.7855\n",
      "Epoch 390/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.6192 - loss: 1.58 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.6192 - loss: 1.5860 - val_accuracy: 0.4596 - val_loss: 1.7840\n",
      "Epoch 391/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.6231 - loss: 1.58 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.6231 - loss: 1.5869 - val_accuracy: 0.4596 - val_loss: 1.7827\n",
      "Epoch 392/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.6240 - loss: 1.58 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.6240 - loss: 1.5865 - val_accuracy: 0.4615 - val_loss: 1.7841\n",
      "Epoch 393/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.6192 - loss: 1.58 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.6192 - loss: 1.5877 - val_accuracy: 0.4404 - val_loss: 1.7852\n",
      "Epoch 394/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.6231 - loss: 1.58 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.6231 - loss: 1.5864 - val_accuracy: 0.4635 - val_loss: 1.7865\n",
      "Epoch 395/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.6029 - loss: 1.59 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.6029 - loss: 1.5931 - val_accuracy: 0.4538 - val_loss: 1.7816\n",
      "Epoch 396/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.6159 - loss: 1.58 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.6159 - loss: 1.5870 - val_accuracy: 0.4500 - val_loss: 1.7819\n",
      "Epoch 397/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.6192 - loss: 1.58 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.6192 - loss: 1.5836 - val_accuracy: 0.4654 - val_loss: 1.7849\n",
      "Epoch 398/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.6072 - loss: 1.58 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.6072 - loss: 1.5873 - val_accuracy: 0.4577 - val_loss: 1.7792\n",
      "Epoch 399/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.6130 - loss: 1.58 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.6130 - loss: 1.5822 - val_accuracy: 0.4500 - val_loss: 1.7783\n",
      "Epoch 400/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.6298 - loss: 1.57 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.6298 - loss: 1.5787 - val_accuracy: 0.4500 - val_loss: 1.7799\n",
      "Epoch 401/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.6115 - loss: 1.58 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.6115 - loss: 1.5825 - val_accuracy: 0.4596 - val_loss: 1.7768\n",
      "Epoch 402/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19s/step - accuracy: 0.6139 - loss: 1.57 ━━━━━━━━━━━━━━━━━━━━ 21s 21s/step - accuracy: 0.6139 - loss: 1.5749 - val_accuracy: 0.4615 - val_loss: 1.7777\n",
      "Epoch 403/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.6212 - loss: 1.57 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.6212 - loss: 1.5790 - val_accuracy: 0.4596 - val_loss: 1.7815\n",
      "Epoch 404/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.6101 - loss: 1.57 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.6101 - loss: 1.5792 - val_accuracy: 0.4481 - val_loss: 1.7789\n",
      "Epoch 405/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.6120 - loss: 1.57 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.6120 - loss: 1.5744 - val_accuracy: 0.4558 - val_loss: 1.7774\n",
      "Epoch 406/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.6192 - loss: 1.57 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.6192 - loss: 1.5717 - val_accuracy: 0.4635 - val_loss: 1.7787\n",
      "Epoch 407/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.6221 - loss: 1.56 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.6221 - loss: 1.5681 - val_accuracy: 0.4538 - val_loss: 1.7744\n",
      "Epoch 408/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.6322 - loss: 1.56 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.6322 - loss: 1.5685 - val_accuracy: 0.4577 - val_loss: 1.7731\n",
      "Epoch 409/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.6250 - loss: 1.56 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.6250 - loss: 1.5694 - val_accuracy: 0.4635 - val_loss: 1.7770\n",
      "Epoch 410/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 16s/step - accuracy: 0.6202 - loss: 1.56 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.6202 - loss: 1.5698 - val_accuracy: 0.4481 - val_loss: 1.7727\n",
      "Epoch 411/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.6216 - loss: 1.57 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.6216 - loss: 1.5750 - val_accuracy: 0.4481 - val_loss: 1.7729\n",
      "Epoch 412/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 16s/step - accuracy: 0.6250 - loss: 1.57 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.6250 - loss: 1.5709 - val_accuracy: 0.4538 - val_loss: 1.7772\n",
      "Epoch 413/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.6298 - loss: 1.57 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.6298 - loss: 1.5731 - val_accuracy: 0.4500 - val_loss: 1.7736\n",
      "Epoch 414/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.6288 - loss: 1.56 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.6288 - loss: 1.5668 - val_accuracy: 0.4538 - val_loss: 1.7744\n",
      "Epoch 415/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.6269 - loss: 1.56 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.6269 - loss: 1.5631 - val_accuracy: 0.4596 - val_loss: 1.7752\n",
      "Epoch 416/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.6216 - loss: 1.56 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.6216 - loss: 1.5664 - val_accuracy: 0.4635 - val_loss: 1.7724\n",
      "Epoch 417/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.6269 - loss: 1.56 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.6269 - loss: 1.5642 - val_accuracy: 0.4519 - val_loss: 1.7724\n",
      "Epoch 418/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.6168 - loss: 1.56 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.6168 - loss: 1.5634 - val_accuracy: 0.4635 - val_loss: 1.7732\n",
      "Epoch 419/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.6216 - loss: 1.55 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.6216 - loss: 1.5578 - val_accuracy: 0.4577 - val_loss: 1.7728\n",
      "Epoch 420/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.6178 - loss: 1.56 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.6178 - loss: 1.5668 - val_accuracy: 0.4558 - val_loss: 1.7726\n",
      "Epoch 421/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.6346 - loss: 1.56 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.6346 - loss: 1.5604 - val_accuracy: 0.4558 - val_loss: 1.7726\n",
      "Epoch 422/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.6260 - loss: 1.55 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.6260 - loss: 1.5595 - val_accuracy: 0.4577 - val_loss: 1.7717\n",
      "Epoch 423/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.6240 - loss: 1.55 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.6240 - loss: 1.5534 - val_accuracy: 0.4519 - val_loss: 1.7703\n",
      "Epoch 424/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 16s/step - accuracy: 0.6337 - loss: 1.55 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.6337 - loss: 1.5562 - val_accuracy: 0.4596 - val_loss: 1.7699\n",
      "Epoch 425/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.6226 - loss: 1.56 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.6226 - loss: 1.5600 - val_accuracy: 0.4558 - val_loss: 1.7709\n",
      "Epoch 426/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.6231 - loss: 1.55 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.6231 - loss: 1.5580 - val_accuracy: 0.4558 - val_loss: 1.7703\n",
      "Epoch 427/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.6269 - loss: 1.55 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.6269 - loss: 1.5559 - val_accuracy: 0.4615 - val_loss: 1.7698\n",
      "Epoch 428/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 16s/step - accuracy: 0.6255 - loss: 1.55 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.6255 - loss: 1.5534 - val_accuracy: 0.4692 - val_loss: 1.7710\n",
      "Epoch 429/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.6356 - loss: 1.55 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.6356 - loss: 1.5523 - val_accuracy: 0.4462 - val_loss: 1.7691\n",
      "Epoch 430/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.6293 - loss: 1.55 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.6293 - loss: 1.5555 - val_accuracy: 0.4519 - val_loss: 1.7687\n",
      "Epoch 431/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.6337 - loss: 1.55 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.6337 - loss: 1.5511 - val_accuracy: 0.4635 - val_loss: 1.7705\n",
      "Epoch 432/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.6212 - loss: 1.55 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.6212 - loss: 1.5536 - val_accuracy: 0.4404 - val_loss: 1.7675\n",
      "Epoch 433/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 16s/step - accuracy: 0.6274 - loss: 1.55 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.6274 - loss: 1.5505 - val_accuracy: 0.4519 - val_loss: 1.7655\n",
      "Epoch 434/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 24s/step - accuracy: 0.6288 - loss: 1.55 ━━━━━━━━━━━━━━━━━━━━ 25s 25s/step - accuracy: 0.6288 - loss: 1.5529 - val_accuracy: 0.4577 - val_loss: 1.7654\n",
      "Epoch 435/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 28s/step - accuracy: 0.6322 - loss: 1.54 ━━━━━━━━━━━━━━━━━━━━ 30s 30s/step - accuracy: 0.6322 - loss: 1.5476 - val_accuracy: 0.4519 - val_loss: 1.7642\n",
      "Epoch 436/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 15s/step - accuracy: 0.6428 - loss: 1.54 ━━━━━━━━━━━━━━━━━━━━ 16s 16s/step - accuracy: 0.6428 - loss: 1.5461 - val_accuracy: 0.4577 - val_loss: 1.7647\n",
      "Epoch 437/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 1934s/step - accuracy: 0.6269 - loss: 1.54 ━━━━━━━━━━━━━━━━━━━━ 1935s 1935s/step - accuracy: 0.6269 - loss: 1.5459 - val_accuracy: 0.4538 - val_loss: 1.7661\n",
      "Epoch 438/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 9s/step - accuracy: 0.6332 - loss: 1.548 ━━━━━━━━━━━━━━━━━━━━ 10s 10s/step - accuracy: 0.6332 - loss: 1.5488 - val_accuracy: 0.4481 - val_loss: 1.7669\n",
      "Epoch 439/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 9s/step - accuracy: 0.6341 - loss: 1.546 ━━━━━━━━━━━━━━━━━━━━ 9s 9s/step - accuracy: 0.6341 - loss: 1.5467 - val_accuracy: 0.4538 - val_loss: 1.7669\n",
      "Epoch 440/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 9s/step - accuracy: 0.6279 - loss: 1.544 ━━━━━━━━━━━━━━━━━━━━ 9s 9s/step - accuracy: 0.6279 - loss: 1.5446 - val_accuracy: 0.4654 - val_loss: 1.7670\n",
      "Epoch 441/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 8s/step - accuracy: 0.6346 - loss: 1.544 ━━━━━━━━━━━━━━━━━━━━ 9s 9s/step - accuracy: 0.6346 - loss: 1.5443 - val_accuracy: 0.4500 - val_loss: 1.7657\n",
      "Epoch 442/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 9s/step - accuracy: 0.6413 - loss: 1.546 ━━━━━━━━━━━━━━━━━━━━ 9s 9s/step - accuracy: 0.6413 - loss: 1.5465 - val_accuracy: 0.4635 - val_loss: 1.7659\n",
      "Epoch 443/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 8s/step - accuracy: 0.6303 - loss: 1.547 ━━━━━━━━━━━━━━━━━━━━ 9s 9s/step - accuracy: 0.6303 - loss: 1.5479 - val_accuracy: 0.4577 - val_loss: 1.7654\n",
      "Epoch 444/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 8s/step - accuracy: 0.6337 - loss: 1.546 ━━━━━━━━━━━━━━━━━━━━ 9s 9s/step - accuracy: 0.6337 - loss: 1.5463 - val_accuracy: 0.4519 - val_loss: 1.7643\n",
      "Epoch 445/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 8s/step - accuracy: 0.6216 - loss: 1.545 ━━━━━━━━━━━━━━━━━━━━ 9s 9s/step - accuracy: 0.6216 - loss: 1.5451 - val_accuracy: 0.4596 - val_loss: 1.7647\n",
      "Epoch 446/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 14s/step - accuracy: 0.6173 - loss: 1.54 ━━━━━━━━━━━━━━━━━━━━ 15s 15s/step - accuracy: 0.6173 - loss: 1.5422 - val_accuracy: 0.4538 - val_loss: 1.7628\n",
      "Epoch 447/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.6288 - loss: 1.54 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.6288 - loss: 1.5415 - val_accuracy: 0.4577 - val_loss: 1.7628\n",
      "Epoch 448/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.6365 - loss: 1.54 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.6365 - loss: 1.5440 - val_accuracy: 0.4615 - val_loss: 1.7633\n",
      "Epoch 449/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 16s/step - accuracy: 0.6322 - loss: 1.54 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.6322 - loss: 1.5429 - val_accuracy: 0.4404 - val_loss: 1.7629\n",
      "Epoch 450/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 16s/step - accuracy: 0.6303 - loss: 1.54 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.6303 - loss: 1.5402 - val_accuracy: 0.4596 - val_loss: 1.7646\n",
      "Epoch 451/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.6274 - loss: 1.53 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.6274 - loss: 1.5385 - val_accuracy: 0.4462 - val_loss: 1.7640\n",
      "Epoch 452/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 16s/step - accuracy: 0.6308 - loss: 1.54 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.6308 - loss: 1.5436 - val_accuracy: 0.4577 - val_loss: 1.7653\n",
      "Epoch 453/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 16s/step - accuracy: 0.6361 - loss: 1.53 ━━━━━━━━━━━━━━━━━━━━ 17s 17s/step - accuracy: 0.6361 - loss: 1.5372 - val_accuracy: 0.4654 - val_loss: 1.7657\n",
      "Epoch 454/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 16s/step - accuracy: 0.6260 - loss: 1.53 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.6260 - loss: 1.5373 - val_accuracy: 0.4635 - val_loss: 1.7645\n",
      "Epoch 455/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.6255 - loss: 1.53 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.6255 - loss: 1.5359 - val_accuracy: 0.4654 - val_loss: 1.7637\n",
      "Epoch 456/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.6409 - loss: 1.53 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.6409 - loss: 1.5333 - val_accuracy: 0.4596 - val_loss: 1.7626\n",
      "Epoch 457/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.6327 - loss: 1.53 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.6327 - loss: 1.5316 - val_accuracy: 0.4558 - val_loss: 1.7610\n",
      "Epoch 458/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 16s/step - accuracy: 0.6462 - loss: 1.52 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.6462 - loss: 1.5287 - val_accuracy: 0.4654 - val_loss: 1.7628\n",
      "Epoch 459/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 16s/step - accuracy: 0.6370 - loss: 1.53 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.6370 - loss: 1.5317 - val_accuracy: 0.4519 - val_loss: 1.7612\n",
      "Epoch 460/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 15s/step - accuracy: 0.6380 - loss: 1.53 ━━━━━━━━━━━━━━━━━━━━ 17s 17s/step - accuracy: 0.6380 - loss: 1.5338 - val_accuracy: 0.4673 - val_loss: 1.7660\n",
      "Epoch 461/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 16s/step - accuracy: 0.6236 - loss: 1.53 ━━━━━━━━━━━━━━━━━━━━ 17s 17s/step - accuracy: 0.6236 - loss: 1.5319 - val_accuracy: 0.4423 - val_loss: 1.7616\n",
      "Epoch 462/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.6216 - loss: 1.53 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.6216 - loss: 1.5337 - val_accuracy: 0.4654 - val_loss: 1.7622\n",
      "Epoch 463/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 14s/step - accuracy: 0.6428 - loss: 1.53 ━━━━━━━━━━━━━━━━━━━━ 15s 15s/step - accuracy: 0.6428 - loss: 1.5314 - val_accuracy: 0.4596 - val_loss: 1.7601\n",
      "Epoch 464/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 16s/step - accuracy: 0.6394 - loss: 1.52 ━━━━━━━━━━━━━━━━━━━━ 17s 17s/step - accuracy: 0.6394 - loss: 1.5222 - val_accuracy: 0.4577 - val_loss: 1.7586\n",
      "Epoch 465/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 16s/step - accuracy: 0.6413 - loss: 1.51 ━━━━━━━━━━━━━━━━━━━━ 17s 17s/step - accuracy: 0.6413 - loss: 1.5194 - val_accuracy: 0.4596 - val_loss: 1.7621\n",
      "Epoch 466/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 16s/step - accuracy: 0.6322 - loss: 1.53 ━━━━━━━━━━━━━━━━━━━━ 17s 17s/step - accuracy: 0.6322 - loss: 1.5304 - val_accuracy: 0.4385 - val_loss: 1.7581\n",
      "Epoch 467/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 16s/step - accuracy: 0.6332 - loss: 1.52 ━━━━━━━━━━━━━━━━━━━━ 17s 17s/step - accuracy: 0.6332 - loss: 1.5278 - val_accuracy: 0.4615 - val_loss: 1.7584\n",
      "Epoch 468/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.6284 - loss: 1.52 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.6284 - loss: 1.5276 - val_accuracy: 0.4654 - val_loss: 1.7575\n",
      "Epoch 469/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 16s/step - accuracy: 0.6404 - loss: 1.52 ━━━━━━━━━━━━━━━━━━━━ 17s 17s/step - accuracy: 0.6404 - loss: 1.5233 - val_accuracy: 0.4519 - val_loss: 1.7569\n",
      "Epoch 470/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 15s/step - accuracy: 0.6428 - loss: 1.51 ━━━━━━━━━━━━━━━━━━━━ 16s 16s/step - accuracy: 0.6428 - loss: 1.5189 - val_accuracy: 0.4673 - val_loss: 1.7598\n",
      "Epoch 471/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 15s/step - accuracy: 0.6380 - loss: 1.51 ━━━━━━━━━━━━━━━━━━━━ 17s 17s/step - accuracy: 0.6380 - loss: 1.5193 - val_accuracy: 0.4500 - val_loss: 1.7569\n",
      "Epoch 472/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 16s/step - accuracy: 0.6288 - loss: 1.52 ━━━━━━━━━━━━━━━━━━━━ 17s 17s/step - accuracy: 0.6288 - loss: 1.5233 - val_accuracy: 0.4654 - val_loss: 1.7566\n",
      "Epoch 473/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 15s/step - accuracy: 0.6337 - loss: 1.52 ━━━━━━━━━━━━━━━━━━━━ 17s 17s/step - accuracy: 0.6337 - loss: 1.5228 - val_accuracy: 0.4635 - val_loss: 1.7585\n",
      "Epoch 474/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 15s/step - accuracy: 0.6288 - loss: 1.51 ━━━━━━━━━━━━━━━━━━━━ 16s 16s/step - accuracy: 0.6288 - loss: 1.5173 - val_accuracy: 0.4404 - val_loss: 1.7568\n",
      "Epoch 475/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 15s/step - accuracy: 0.6365 - loss: 1.51 ━━━━━━━━━━━━━━━━━━━━ 17s 17s/step - accuracy: 0.6365 - loss: 1.5155 - val_accuracy: 0.4596 - val_loss: 1.7601\n",
      "Epoch 476/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 15s/step - accuracy: 0.6433 - loss: 1.51 ━━━━━━━━━━━━━━━━━━━━ 16s 16s/step - accuracy: 0.6433 - loss: 1.5132 - val_accuracy: 0.4615 - val_loss: 1.7559\n",
      "Epoch 477/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 14s/step - accuracy: 0.6341 - loss: 1.51 ━━━━━━━━━━━━━━━━━━━━ 15s 15s/step - accuracy: 0.6341 - loss: 1.5185 - val_accuracy: 0.4615 - val_loss: 1.7557\n",
      "Epoch 478/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 14s/step - accuracy: 0.6438 - loss: 1.51 ━━━━━━━━━━━━━━━━━━━━ 15s 15s/step - accuracy: 0.6438 - loss: 1.5124 - val_accuracy: 0.4596 - val_loss: 1.7567\n",
      "Epoch 479/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 14s/step - accuracy: 0.6389 - loss: 1.51 ━━━━━━━━━━━━━━━━━━━━ 15s 15s/step - accuracy: 0.6389 - loss: 1.5114 - val_accuracy: 0.4558 - val_loss: 1.7553\n",
      "Epoch 480/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 14s/step - accuracy: 0.6293 - loss: 1.51 ━━━━━━━━━━━━━━━━━━━━ 15s 15s/step - accuracy: 0.6293 - loss: 1.5111 - val_accuracy: 0.4615 - val_loss: 1.7571\n",
      "Epoch 481/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 14s/step - accuracy: 0.6279 - loss: 1.51 ━━━━━━━━━━━━━━━━━━━━ 15s 15s/step - accuracy: 0.6279 - loss: 1.5124 - val_accuracy: 0.4577 - val_loss: 1.7565\n",
      "Epoch 482/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 14s/step - accuracy: 0.6462 - loss: 1.51 ━━━━━━━━━━━━━━━━━━━━ 15s 15s/step - accuracy: 0.6462 - loss: 1.5148 - val_accuracy: 0.4635 - val_loss: 1.7577\n",
      "Epoch 483/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 14s/step - accuracy: 0.6375 - loss: 1.51 ━━━━━━━━━━━━━━━━━━━━ 15s 15s/step - accuracy: 0.6375 - loss: 1.5163 - val_accuracy: 0.4596 - val_loss: 1.7576\n",
      "Epoch 484/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 14s/step - accuracy: 0.6433 - loss: 1.51 ━━━━━━━━━━━━━━━━━━━━ 15s 15s/step - accuracy: 0.6433 - loss: 1.5102 - val_accuracy: 0.4635 - val_loss: 1.7558\n",
      "Epoch 485/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 13s/step - accuracy: 0.6375 - loss: 1.51 ━━━━━━━━━━━━━━━━━━━━ 15s 15s/step - accuracy: 0.6375 - loss: 1.5129 - val_accuracy: 0.4596 - val_loss: 1.7546\n",
      "Epoch 486/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 14s/step - accuracy: 0.6389 - loss: 1.51 ━━━━━━━━━━━━━━━━━━━━ 15s 15s/step - accuracy: 0.6389 - loss: 1.5122 - val_accuracy: 0.4577 - val_loss: 1.7511\n",
      "Epoch 487/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 14s/step - accuracy: 0.6500 - loss: 1.50 ━━━━━━━━━━━━━━━━━━━━ 15s 15s/step - accuracy: 0.6500 - loss: 1.5039 - val_accuracy: 0.4635 - val_loss: 1.7510\n",
      "Epoch 488/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 14s/step - accuracy: 0.6375 - loss: 1.50 ━━━━━━━━━━━━━━━━━━━━ 15s 15s/step - accuracy: 0.6375 - loss: 1.5063 - val_accuracy: 0.4558 - val_loss: 1.7506\n",
      "Epoch 489/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 14s/step - accuracy: 0.6529 - loss: 1.50 ━━━━━━━━━━━━━━━━━━━━ 15s 15s/step - accuracy: 0.6529 - loss: 1.5063 - val_accuracy: 0.4558 - val_loss: 1.7529\n",
      "Epoch 490/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 14s/step - accuracy: 0.6404 - loss: 1.50 ━━━━━━━━━━━━━━━━━━━━ 15s 15s/step - accuracy: 0.6404 - loss: 1.5078 - val_accuracy: 0.4442 - val_loss: 1.7527\n",
      "Epoch 491/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 14s/step - accuracy: 0.6428 - loss: 1.50 ━━━━━━━━━━━━━━━━━━━━ 15s 15s/step - accuracy: 0.6428 - loss: 1.5015 - val_accuracy: 0.4635 - val_loss: 1.7567\n",
      "Epoch 492/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 14s/step - accuracy: 0.6510 - loss: 1.49 ━━━━━━━━━━━━━━━━━━━━ 15s 15s/step - accuracy: 0.6510 - loss: 1.4992 - val_accuracy: 0.4519 - val_loss: 1.7541\n",
      "Epoch 493/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 14s/step - accuracy: 0.6361 - loss: 1.50 ━━━━━━━━━━━━━━━━━━━━ 15s 15s/step - accuracy: 0.6361 - loss: 1.5013 - val_accuracy: 0.4654 - val_loss: 1.7563\n",
      "Epoch 494/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 14s/step - accuracy: 0.6418 - loss: 1.50 ━━━━━━━━━━━━━━━━━━━━ 15s 15s/step - accuracy: 0.6418 - loss: 1.5005 - val_accuracy: 0.4692 - val_loss: 1.7548\n",
      "Epoch 495/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 14s/step - accuracy: 0.6413 - loss: 1.49 ━━━━━━━━━━━━━━━━━━━━ 15s 15s/step - accuracy: 0.6413 - loss: 1.4996 - val_accuracy: 0.4615 - val_loss: 1.7563\n",
      "Epoch 496/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 14s/step - accuracy: 0.6389 - loss: 1.50 ━━━━━━━━━━━━━━━━━━━━ 15s 15s/step - accuracy: 0.6389 - loss: 1.5078 - val_accuracy: 0.4615 - val_loss: 1.7547\n",
      "Epoch 497/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 14s/step - accuracy: 0.6346 - loss: 1.50 ━━━━━━━━━━━━━━━━━━━━ 15s 15s/step - accuracy: 0.6346 - loss: 1.5004 - val_accuracy: 0.4635 - val_loss: 1.7567\n",
      "Epoch 498/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 14s/step - accuracy: 0.6438 - loss: 1.49 ━━━━━━━━━━━━━━━━━━━━ 16s 16s/step - accuracy: 0.6438 - loss: 1.4982 - val_accuracy: 0.4596 - val_loss: 1.7548\n",
      "Epoch 499/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 14s/step - accuracy: 0.6519 - loss: 1.49 ━━━━━━━━━━━━━━━━━━━━ 15s 15s/step - accuracy: 0.6519 - loss: 1.4949 - val_accuracy: 0.4577 - val_loss: 1.7549\n",
      "Epoch 500/500\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 14s/step - accuracy: 0.6351 - loss: 1.49 ━━━━━━━━━━━━━━━━━━━━ 15s 15s/step - accuracy: 0.6351 - loss: 1.4988 - val_accuracy: 0.4577 - val_loss: 1.7537\n",
      "\n",
      "Final Epoch - Validation Loss: 1.7537, Validation Accuracy: 0.4577\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['C:\\\\LTI 11785 Introduction to deep learning\\\\project\\\\SSVEP\\\\wandb\\\\run-20250418_165422-u0fwht0f\\\\files\\\\eegnet_model.keras']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from wandb.integration.keras import WandbCallback\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "\n",
    "\n",
    "wandb_cb = WandbCallback(save_graph=False, save_model=False)\n",
    "\n",
    "\n",
    "class TQDMProgressBar(Callback):\n",
    "    def on_train_begin(self, logs=None):\n",
    "        self.epochs = self.params['epochs']\n",
    "        self.pbar = tqdm(total=self.epochs, desc=\"Epochs\", ncols=100)\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        val_loss = logs.get('val_loss', None)\n",
    "        val_acc = logs.get('val_accuracy', None)\n",
    "        msg = f\" val_loss: {val_loss:.4f} - val_accuracy: {val_acc:.4f}\"\n",
    "        self.pbar.set_postfix_str(msg)\n",
    "        self.pbar.update(1)\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        self.pbar.close()\n",
    "\n",
    "\n",
    "num_epochs = config['epochs_EEG']\n",
    "test_period = 1\n",
    "start_test = test_period\n",
    "alpha = 1\n",
    "beta = 0\n",
    "\n",
    "\n",
    "# Train the model and store the training history\n",
    "history = eegnet_model.fit(\n",
    "    X_train_np, y_train_np,\n",
    "    batch_size=X_train_np.shape[0],\n",
    "    epochs=num_epochs,\n",
    "    validation_data=(X_val_np, y_val_np),\n",
    "    verbose=1,  # \n",
    "    callbacks=[wandb_cb]\n",
    "    #callbacks=[TQDMProgressBar(), wandb_cb]  # ✅ use custom progress\n",
    ")\n",
    "\n",
    "# Extract training and validation history for plotting later\n",
    "train_loss = history.history['loss']\n",
    "val_loss   = history.history['val_loss']\n",
    "train_acc  = history.history.get('accuracy') or history.history.get('acc')  # depends on TF version\n",
    "val_acc    = history.history.get('val_accuracy') or history.history.get('val_acc')\n",
    "\n",
    "# Print final results\n",
    "print(f\"\\nFinal Epoch - Validation Loss: {val_loss[-1]:.4f}, Validation Accuracy: {val_acc[-1]:.4f}\")\n",
    "\n",
    "\n",
    "plot_model(eegnet_model, show_shapes=True)\n",
    "wandb.log({\"model_architecture\": wandb.Image(\"model.png\")})\n",
    "\n",
    "eegnet_model.save(\"eegnet_model.keras\")\n",
    "wandb.save(\"eegnet_model.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4117c82a-d16c-415e-9005-2edd65456e20",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 354
    },
    "id": "BzCmk5Zy_96e",
    "outputId": "efdbb9f5-c9ca-4973-9161-61b30bb6f909"
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'History' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[41], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Plot training & validation loss\u001b[39;00m\n\u001b[0;32m      7\u001b[0m plt\u001b[38;5;241m.\u001b[39msubplot(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m----> 8\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(history[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m], label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining Loss\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      9\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(history[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m], label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidation Loss\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     10\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: 'History' object is not subscriptable"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.style.use('tableau-colorblind10')\n",
    "\n",
    "# Plot training & validation loss\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history[\"train_loss\"], label=\"Training Loss\")\n",
    "plt.plot(history[\"val_loss\"], label=\"Validation Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "\n",
    "# Plot training & validation accuracy\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot([v * 100 for v in history[\"train_acc\"]], label=\"Training Accuracy\")\n",
    "plt.plot([v * 100 for v in history[\"val_acc\"]], label=\"Validation Accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy (%)\")\n",
    "plt.legend()\n",
    "\n",
    "plt.suptitle(\"Training Progress (Loss & Accuracy)\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "LGmMW-mZVOrM",
   "metadata": {
    "id": "LGmMW-mZVOrM"
   },
   "outputs": [],
   "source": [
    "def evaluate_on_loader(test_loader, name=\"test\"):\n",
    "    diffe.eval()\n",
    "    labels = np.arange(0, 26)\n",
    "    Y, Y_hat = [], []\n",
    "    with torch.no_grad():\n",
    "        for x, y in test_loader:\n",
    "            x, y = x.to(device), y.type(torch.LongTensor).to(device)\n",
    "            y_cat = F.one_hot(y, num_classes=26).type(torch.FloatTensor).to(device)\n",
    "\n",
    "            x_hat, down, up, noise, t = ddpm(x)\n",
    "\n",
    "            # Align the temporal dimension of x_hat and x\n",
    "            #x_hat, x = safe_align_2d(x_hat, x)\n",
    "\n",
    "            #ddpm_out = x_hat, down, up, t\n",
    "            #ddpm_out = x, down, up, t\n",
    "            #decoder_out, fc_out = diffe(x, ddpm_out)\n",
    "\n",
    "            # EMA update\n",
    "            #fc_ema.update()\n",
    "            #print(decoder_out.shape)\n",
    "            #X_eegnet = decoder_out.view(decoder_out.size(0), 64, -1)  # (samples, 64, 396)\n",
    "            X_eegnet = x.view(x.size(0), 64, -1)\n",
    "            #print(X_eegnet.shape)\n",
    "            outputs = eegnet_model(X_eegnet)\n",
    "\n",
    "            #encoder_out = diffe.encoder(x)\n",
    "            #y_hat = diffe.fc(encoder_out[1])\n",
    "            y_hat = F.softmax(outputs, dim=1)\n",
    "\n",
    "            Y.append(y.detach().cpu())\n",
    "            Y_hat.append(y_hat.detach().cpu())\n",
    "\n",
    "    Y = torch.cat(Y, dim=0).numpy()\n",
    "    Y_hat = torch.cat(Y_hat, dim=0).numpy()\n",
    "\n",
    "    accuracy = top_k_accuracy_score(Y, Y_hat, k=1, labels=labels)\n",
    "    print(f\" {name} Accuracy: {accuracy:.2%}\")\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xs-0xKrFvCnJ",
   "metadata": {
    "id": "xs-0xKrFvCnJ"
   },
   "source": [
    "#Evaluate on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "KVr8fRRkVRf1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KVr8fRRkVRf1",
    "outputId": "72215c66-53fc-42f0-cbe6-64751cc5e33f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   0%|     | 2/500 [3:38:40<907:29:20, 6560.16s/it,  val_loss: 2.8423 - val_accuracy: 0.3308]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Exception encountered when calling Functional.call().\n\n\u001b[1mcan't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.\u001b[0m\n\nArguments received by Functional.call():\n  • inputs=tensor([[[1.1532e+01, 9.4915e-01, 5.8162e-02,  ..., 1.2838e-07,\n          2.1597e-07, 1.2596e-03],\n         [6.1933e+00, 6.9370e-01, 9.2214e-03,  ..., 1.4117e-05,\n          2.4021e-04, 1.6509e-02],\n         [4.3549e-01, 1.7802e-01, 1.3728e-02,  ..., 6.3406e-05,\n          8.3403e-04, 4.6617e-02],\n         ...,\n         [1.5231e+01, 8.5079e-01, 3.8146e-01,  ..., 1.0547e-04,\n          1.3368e-04, 1.9567e-02],\n         [1.6407e+01, 8.0682e-01, 5.1869e-01,  ..., 1.1097e-04,\n          1.6912e-04, 2.2491e-02],\n         [1.7998e+01, 9.0363e-01, 5.7324e-01,  ..., 1.1668e-04,\n          1.4410e-04, 2.0255e-02]],\n\n        [[1.0439e+01, 4.1854e-01, 6.0779e-02,  ..., 7.3464e-05,\n          5.6858e-04, 1.2531e-02],\n         [1.4212e+01, 7.5987e-01, 1.3274e-01,  ..., 5.8905e-05,\n          2.9880e-04, 6.4408e-03],\n         [2.1794e+00, 7.4150e-03, 3.7476e-03,  ..., 3.4641e-05,\n          2.5337e-04, 2.6344e-03],\n         ...,\n         [3.0022e+00, 1.9217e-03, 1.9497e-04,  ..., 1.1828e-05,\n          7.1437e-05, 2.0873e-02],\n         [3.8157e+00, 9.8976e-03, 4.3034e-06,  ..., 1.0549e-04,\n          4.1429e-06, 1.4311e-02],\n         [3.4643e+00, 3.4215e-03, 1.3830e-04,  ..., 1.0107e-04,\n          6.4806e-06, 1.5243e-02]],\n\n        [[7.2232e+01, 1.0502e+01, 3.0649e+00,  ..., 3.6113e-05,\n          1.0655e-04, 4.1617e-03],\n         [8.4707e+00, 1.6690e+00, 8.7405e-01,  ..., 2.3222e-04,\n          6.2585e-04, 2.0861e-02],\n         [1.4670e+00, 7.1786e-03, 1.0359e-01,  ..., 7.1947e-04,\n          8.4523e-04, 1.0490e-02],\n         ...,\n         [2.5639e+01, 3.3388e+00, 1.1592e+00,  ..., 4.9905e-05,\n          8.5496e-04, 2.3978e-02],\n         [3.4143e+01, 4.2753e+00, 1.3738e+00,  ..., 1.6035e-04,\n          4.6707e-04, 8.9398e-03],\n         [3.5561e+01, 4.3844e+00, 1.4322e+00,  ..., 1.4201e-04,\n          4.5586e-04, 9.0977e-03]],\n\n        ...,\n\n        [[2.8971e+00, 2.4640e-01, 7.7646e-03,  ..., 8.1685e-05,\n          2.6406e-03, 1.1152e-01],\n         [6.6191e+00, 2.3034e-01, 1.1627e-03,  ..., 4.4256e-05,\n          1.4679e-04, 3.7414e-03],\n         [1.8233e+01, 3.5203e-02, 6.9810e-02,  ..., 8.9060e-07,\n          1.7584e-04, 9.3973e-03],\n         ...,\n         [2.3230e+01, 1.4515e+00, 4.5558e-01,  ..., 4.6019e-05,\n          7.8364e-06, 8.2453e-03],\n         [2.1140e+01, 1.3672e+00, 3.4159e-01,  ..., 4.3801e-05,\n          4.8043e-05, 1.1283e-02],\n         [2.0765e+01, 1.3411e+00, 3.3569e-01,  ..., 2.7887e-05,\n          5.0002e-05, 1.1101e-02]],\n\n        [[8.9151e-03, 4.6230e-01, 1.1833e-01,  ..., 2.3897e-03,\n          2.0624e-03, 5.0579e-02],\n         [4.6731e-01, 1.2249e-01, 2.2351e-01,  ..., 3.3091e-04,\n          2.7394e-04, 1.7149e-02],\n         [7.7562e+00, 2.0935e-01, 5.3325e-01,  ..., 7.4748e-06,\n          5.8070e-05, 5.0222e-03],\n         ...,\n         [4.8599e+00, 4.1820e-01, 5.7522e-02,  ..., 2.1119e-05,\n          4.1422e-05, 1.6768e-03],\n         [8.0720e+00, 5.7027e-01, 7.9799e-02,  ..., 1.6546e-05,\n          1.9231e-05, 6.5788e-04],\n         [9.1502e+00, 7.0226e-01, 9.2069e-02,  ..., 2.6650e-05,\n          2.9932e-05, 7.3041e-04]],\n\n        [[5.0533e+01, 2.5271e+00, 2.8343e+00,  ..., 1.3777e-04,\n          5.2424e-04, 7.7775e-03],\n         [6.6788e+01, 4.0263e+00, 3.6113e+00,  ..., 2.6028e-06,\n          9.4107e-05, 2.2673e-03],\n         [3.9992e+01, 1.9780e+00, 2.7637e+00,  ..., 1.5076e-05,\n          2.2621e-04, 2.7754e-03],\n         ...,\n         [1.4968e+00, 4.0504e-02, 2.5941e-08,  ..., 3.2871e-05,\n          1.0860e-03, 1.1799e-02],\n         [8.3069e-01, 1.2154e-02, 2.3213e-09,  ..., 5.3332e-05,\n          1.0270e-03, 1.2370e-02],\n         [6.8865e-01, 6.5360e-03, 2.3365e-04,  ..., 5.9020e-05,\n          1.0207e-03, 1.2646e-02]]], device='cuda:0')\n  • training=None\n  • mask=None",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[43], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m acc1 \u001b[38;5;241m=\u001b[39m evaluate_on_loader(test1_loader, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest1 (Seen Subject)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      2\u001b[0m acc2 \u001b[38;5;241m=\u001b[39m evaluate_on_loader(test2_loader, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest2 (Unseen Subject)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[42], line 25\u001b[0m, in \u001b[0;36mevaluate_on_loader\u001b[1;34m(test_loader, name)\u001b[0m\n\u001b[0;32m     23\u001b[0m X_eegnet \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m64\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m#print(X_eegnet.shape)\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m outputs \u001b[38;5;241m=\u001b[39m eegnet_model(X_eegnet)\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m#encoder_out = diffe.encoder(x)\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m#y_hat = diffe.fc(encoder_out[1])\u001b[39;00m\n\u001b[0;32m     29\u001b[0m y_hat \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msoftmax(outputs, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch_env_p11\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch_env_p11\\Lib\\site-packages\\torch\\_tensor.py:1149\u001b[0m, in \u001b[0;36mTensor.__array__\u001b[1;34m(self, dtype)\u001b[0m\n\u001b[0;32m   1147\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(Tensor\u001b[38;5;241m.\u001b[39m__array__, (\u001b[38;5;28mself\u001b[39m,), \u001b[38;5;28mself\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m   1148\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1149\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m   1150\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mastype(dtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[1;31mTypeError\u001b[0m: Exception encountered when calling Functional.call().\n\n\u001b[1mcan't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.\u001b[0m\n\nArguments received by Functional.call():\n  • inputs=tensor([[[1.1532e+01, 9.4915e-01, 5.8162e-02,  ..., 1.2838e-07,\n          2.1597e-07, 1.2596e-03],\n         [6.1933e+00, 6.9370e-01, 9.2214e-03,  ..., 1.4117e-05,\n          2.4021e-04, 1.6509e-02],\n         [4.3549e-01, 1.7802e-01, 1.3728e-02,  ..., 6.3406e-05,\n          8.3403e-04, 4.6617e-02],\n         ...,\n         [1.5231e+01, 8.5079e-01, 3.8146e-01,  ..., 1.0547e-04,\n          1.3368e-04, 1.9567e-02],\n         [1.6407e+01, 8.0682e-01, 5.1869e-01,  ..., 1.1097e-04,\n          1.6912e-04, 2.2491e-02],\n         [1.7998e+01, 9.0363e-01, 5.7324e-01,  ..., 1.1668e-04,\n          1.4410e-04, 2.0255e-02]],\n\n        [[1.0439e+01, 4.1854e-01, 6.0779e-02,  ..., 7.3464e-05,\n          5.6858e-04, 1.2531e-02],\n         [1.4212e+01, 7.5987e-01, 1.3274e-01,  ..., 5.8905e-05,\n          2.9880e-04, 6.4408e-03],\n         [2.1794e+00, 7.4150e-03, 3.7476e-03,  ..., 3.4641e-05,\n          2.5337e-04, 2.6344e-03],\n         ...,\n         [3.0022e+00, 1.9217e-03, 1.9497e-04,  ..., 1.1828e-05,\n          7.1437e-05, 2.0873e-02],\n         [3.8157e+00, 9.8976e-03, 4.3034e-06,  ..., 1.0549e-04,\n          4.1429e-06, 1.4311e-02],\n         [3.4643e+00, 3.4215e-03, 1.3830e-04,  ..., 1.0107e-04,\n          6.4806e-06, 1.5243e-02]],\n\n        [[7.2232e+01, 1.0502e+01, 3.0649e+00,  ..., 3.6113e-05,\n          1.0655e-04, 4.1617e-03],\n         [8.4707e+00, 1.6690e+00, 8.7405e-01,  ..., 2.3222e-04,\n          6.2585e-04, 2.0861e-02],\n         [1.4670e+00, 7.1786e-03, 1.0359e-01,  ..., 7.1947e-04,\n          8.4523e-04, 1.0490e-02],\n         ...,\n         [2.5639e+01, 3.3388e+00, 1.1592e+00,  ..., 4.9905e-05,\n          8.5496e-04, 2.3978e-02],\n         [3.4143e+01, 4.2753e+00, 1.3738e+00,  ..., 1.6035e-04,\n          4.6707e-04, 8.9398e-03],\n         [3.5561e+01, 4.3844e+00, 1.4322e+00,  ..., 1.4201e-04,\n          4.5586e-04, 9.0977e-03]],\n\n        ...,\n\n        [[2.8971e+00, 2.4640e-01, 7.7646e-03,  ..., 8.1685e-05,\n          2.6406e-03, 1.1152e-01],\n         [6.6191e+00, 2.3034e-01, 1.1627e-03,  ..., 4.4256e-05,\n          1.4679e-04, 3.7414e-03],\n         [1.8233e+01, 3.5203e-02, 6.9810e-02,  ..., 8.9060e-07,\n          1.7584e-04, 9.3973e-03],\n         ...,\n         [2.3230e+01, 1.4515e+00, 4.5558e-01,  ..., 4.6019e-05,\n          7.8364e-06, 8.2453e-03],\n         [2.1140e+01, 1.3672e+00, 3.4159e-01,  ..., 4.3801e-05,\n          4.8043e-05, 1.1283e-02],\n         [2.0765e+01, 1.3411e+00, 3.3569e-01,  ..., 2.7887e-05,\n          5.0002e-05, 1.1101e-02]],\n\n        [[8.9151e-03, 4.6230e-01, 1.1833e-01,  ..., 2.3897e-03,\n          2.0624e-03, 5.0579e-02],\n         [4.6731e-01, 1.2249e-01, 2.2351e-01,  ..., 3.3091e-04,\n          2.7394e-04, 1.7149e-02],\n         [7.7562e+00, 2.0935e-01, 5.3325e-01,  ..., 7.4748e-06,\n          5.8070e-05, 5.0222e-03],\n         ...,\n         [4.8599e+00, 4.1820e-01, 5.7522e-02,  ..., 2.1119e-05,\n          4.1422e-05, 1.6768e-03],\n         [8.0720e+00, 5.7027e-01, 7.9799e-02,  ..., 1.6546e-05,\n          1.9231e-05, 6.5788e-04],\n         [9.1502e+00, 7.0226e-01, 9.2069e-02,  ..., 2.6650e-05,\n          2.9932e-05, 7.3041e-04]],\n\n        [[5.0533e+01, 2.5271e+00, 2.8343e+00,  ..., 1.3777e-04,\n          5.2424e-04, 7.7775e-03],\n         [6.6788e+01, 4.0263e+00, 3.6113e+00,  ..., 2.6028e-06,\n          9.4107e-05, 2.2673e-03],\n         [3.9992e+01, 1.9780e+00, 2.7637e+00,  ..., 1.5076e-05,\n          2.2621e-04, 2.7754e-03],\n         ...,\n         [1.4968e+00, 4.0504e-02, 2.5941e-08,  ..., 3.2871e-05,\n          1.0860e-03, 1.1799e-02],\n         [8.3069e-01, 1.2154e-02, 2.3213e-09,  ..., 5.3332e-05,\n          1.0270e-03, 1.2370e-02],\n         [6.8865e-01, 6.5360e-03, 2.3365e-04,  ..., 5.9020e-05,\n          1.0207e-03, 1.2646e-02]]], device='cuda:0')\n  • training=None\n  • mask=None"
     ]
    }
   ],
   "source": [
    "acc1 = evaluate_on_loader(test1_loader, name=\"Test1 (Seen Subject)\")\n",
    "acc2 = evaluate_on_loader(test2_loader, name=\"Test2 (Unseen Subject)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "db6e70df-d1ef-4aa4-8606-85e120a00914",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>▁▄▄▄▅▅▅▅▅▅▆▅▆▅▆▆▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇█████</td></tr><tr><td>epoch</td><td>▁▁▁▁▁▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▄▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇▇▇██</td></tr><tr><td>loss</td><td>█▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▁▁▃▃▄▅▅▅▇▇▇▇▆▇▇▆▆▆▆▇▆▆▇▇▇▇███▇█▇▇█▇█▇▇█</td></tr><tr><td>val_loss</td><td>█▆▆▅▅▅▅▅▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>0.6351</td></tr><tr><td>best_epoch</td><td>487</td></tr><tr><td>best_val_loss</td><td>1.7506</td></tr><tr><td>epoch</td><td>499</td></tr><tr><td>loss</td><td>1.49876</td></tr><tr><td>val_accuracy</td><td>0.45769</td></tr><tr><td>val_loss</td><td>1.75369</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Ben_project_base_line_EEGNet_with_spectrogram</strong> at: <a href='https://wandb.ai/yucheng_shao-carnegie-mellon-university/project_ssvep-ablations/runs/u0fwht0f' target=\"_blank\">https://wandb.ai/yucheng_shao-carnegie-mellon-university/project_ssvep-ablations/runs/u0fwht0f</a><br> View project at: <a href='https://wandb.ai/yucheng_shao-carnegie-mellon-university/project_ssvep-ablations' target=\"_blank\">https://wandb.ai/yucheng_shao-carnegie-mellon-university/project_ssvep-ablations</a><br>Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 2 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250418_165422-u0fwht0f\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if USE_WANDB:\n",
    "    run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20230b2-6f7c-491a-8481-f4e8b6b01325",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (pytorch_env_p11)",
   "language": "python",
   "name": "pytorch_env_p11"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "07acb422b2b34b9791881de2513b32c7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_38a3424ad0d5460ba042960d1736b5aa",
      "max": 500,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_0a160710838b45c3950a599f2dce7836",
      "value": 500
     }
    },
    "0a160710838b45c3950a599f2dce7836": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "244fb693e8174b82aaa38b9d943ca69b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ce5b46cc0a1848f0adcdd11a1e7f95a3",
      "placeholder": "​",
      "style": "IPY_MODEL_6855930ba8b94aab9f64dbbdcddd51d2",
      "value": "Method ALL – Processing subject ALL – Val Accuracy: 10.00% | Best: 12.50%: 100%"
     }
    },
    "38a3424ad0d5460ba042960d1736b5aa": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3a0de191bd4b4018a92ada5e6e4607cf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_244fb693e8174b82aaa38b9d943ca69b",
       "IPY_MODEL_07acb422b2b34b9791881de2513b32c7",
       "IPY_MODEL_c7524cccdeea422d9ef342b142df608d"
      ],
      "layout": "IPY_MODEL_6afb371d18044cd3815fd96c9810c3a7"
     }
    },
    "6855930ba8b94aab9f64dbbdcddd51d2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6afb371d18044cd3815fd96c9810c3a7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "713d6a7190db4a54a4cd9ec1e498b1be": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9d503d51771842af8d76af60586edd19": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c7524cccdeea422d9ef342b142df608d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_713d6a7190db4a54a4cd9ec1e498b1be",
      "placeholder": "​",
      "style": "IPY_MODEL_9d503d51771842af8d76af60586edd19",
      "value": " 500/500 [15:47&lt;00:00,  1.77s/it]"
     }
    },
    "ce5b46cc0a1848f0adcdd11a1e7f95a3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
