{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "0017f312",
      "metadata": {
        "id": "0017f312"
      },
      "source": [
        "# SSVEP Character Classification with DiffE"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ema-pytorch"
      ],
      "metadata": {
        "id": "WnZeypG6gsYU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea8ce474-1f7d-420e-cc23-6384682b81a7"
      },
      "id": "WnZeypG6gsYU",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ema-pytorch\n",
            "  Downloading ema_pytorch-0.7.7-py3-none-any.whl.metadata (689 bytes)\n",
            "Requirement already satisfied: torch>=2.0 in /usr/local/lib/python3.11/dist-packages (from ema-pytorch) (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->ema-pytorch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->ema-pytorch) (4.13.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->ema-pytorch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->ema-pytorch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->ema-pytorch) (2025.3.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=2.0->ema-pytorch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=2.0->ema-pytorch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=2.0->ema-pytorch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2.0->ema-pytorch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=2.0->ema-pytorch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=2.0->ema-pytorch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=2.0->ema-pytorch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=2.0->ema-pytorch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=2.0->ema-pytorch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->ema-pytorch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->ema-pytorch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->ema-pytorch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=2.0->ema-pytorch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->ema-pytorch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->ema-pytorch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0->ema-pytorch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0->ema-pytorch) (3.0.2)\n",
            "Downloading ema_pytorch-0.7.7-py3-none-any.whl (9.8 kB)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m79.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m64.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m47.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m78.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, ema-pytorch\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed ema-pytorch-0.7.7 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "!git clone https://github.com/diffe2023/Diff-E.git\n",
        "sys.path.append('/content/Diff-E')\n"
      ],
      "metadata": {
        "id": "s8yKwaRCQxyM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6834ff78-72c4-4ba6-835c-6286cbab9ed2"
      },
      "id": "s8yKwaRCQxyM",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Diff-E'...\n",
            "remote: Enumerating objects: 160, done.\u001b[K\n",
            "remote: Counting objects: 100% (55/55), done.\u001b[K\n",
            "remote: Compressing objects: 100% (29/29), done.\u001b[K\n",
            "remote: Total 160 (delta 43), reused 27 (delta 26), pack-reused 105 (from 1)\u001b[K\n",
            "Receiving objects: 100% (160/160), 53.41 KiB | 10.68 MiB/s, done.\n",
            "Resolving deltas: 100% (60/60), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "from einops import reduce\n",
        "from functools import partial\n",
        "import random\n",
        "import argparse\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from ema_pytorch import EMA\n",
        "from tqdm.notebook import tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import (\n",
        "    f1_score,\n",
        "    roc_auc_score,\n",
        "    precision_score,\n",
        "    recall_score,\n",
        "    top_k_accuracy_score,\n",
        ")"
      ],
      "metadata": {
        "id": "4MhV2qw9hu-m"
      },
      "id": "4MhV2qw9hu-m",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Uncomment this if you want to use Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "WjdWMaswQjkV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6dd75392-2fd4-478f-ab28-075983c65228"
      },
      "id": "WjdWMaswQjkV",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#load and preprocess data\n"
      ],
      "metadata": {
        "id": "Do2JYBiZZp1z"
      },
      "id": "Do2JYBiZZp1z"
    },
    {
      "cell_type": "markdown",
      "source": [
        "We retain the original SSVEP data preprocessing procedures, including:\n",
        "\n",
        "    Chebyshev bandpass filtering (6–90 Hz),\n",
        "    Noise removal and segmentation using a sliding window of 250 samples, followed by manual channel-wise standardization (subtracting the mean and dividing by the standard deviation),\n",
        "    Downsampling from 1000 Hz to 250 Hz.\n",
        "\n",
        "In this work, we directly load the preprocessed character-level files \"S#_chars.npy\" for subsequent training.\n",
        "\n",
        "Unlike the original setup where models are trained separately for each subject, we train a single model jointly across all subjects by concatenating all subjects' data."
      ],
      "metadata": {
        "id": "dPlrZ7Lez1XQ"
      },
      "id": "dPlrZ7Lez1XQ"
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function to perform z-score normalization on the data\n",
        "def zscore_norm(data):\n",
        "    # Calculate the mean and standard deviation for each channel in each batch\n",
        "    mean = torch.mean(data, dim=(1, 2))\n",
        "    std = torch.std(data, dim=(1, 2))\n",
        "\n",
        "    # Subtract the mean from each channel in each batch and divide by the standard deviation\n",
        "    norm_data = (data - mean[:, None, None]) / std[:, None, None]\n",
        "\n",
        "    return norm_data\n",
        "\n",
        "\n",
        "# Define a function to perform min-max normalization on the data\n",
        "def minmax_norm(data):\n",
        "    # Calculate the minimum and maximum values for each channel and sequence in the batch\n",
        "    min_vals = torch.min(data, dim=-1)[0]\n",
        "    max_vals = torch.max(data, dim=-1)[0]\n",
        "\n",
        "    # Scale the data to the range [0, 1]\n",
        "    norm_data = (data - min_vals.unsqueeze(-1)) / (\n",
        "        max_vals.unsqueeze(-1) - min_vals.unsqueeze(-1)\n",
        "    )\n",
        "\n",
        "    return norm_data"
      ],
      "metadata": {
        "id": "FD4sHQTI0fua"
      },
      "id": "FD4sHQTI0fua",
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EEGDataset(Dataset):\n",
        "    \"Characterizes a dataset for PyTorch\"\n",
        "\n",
        "    def __init__(self, X, Y, transform=None):\n",
        "        \"Initialization\"\n",
        "        self.X = X\n",
        "        self.Y = Y\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        \"Denotes the total number of samples\"\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"Generates one sample of data\"\n",
        "        # Load data and get label\n",
        "        x = self.X[index]\n",
        "        y = self.Y[index]\n",
        "        if self.transform:\n",
        "            x = self.transform(x)\n",
        "        return x.squeeze(), y"
      ],
      "metadata": {
        "id": "kpv9Xa910cDi"
      },
      "id": "kpv9Xa910cDi",
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(root_dir, subject, session=None):\n",
        "    filename = f\"S{subject}_chars.npy\"\n",
        "    file_path = os.path.join(root_dir, filename)\n",
        "    data = np.load(file_path)  # shape: [26, 6, 64, 250]\n",
        "\n",
        "    X = data.reshape(-1, 64, 250)  # [26*6, 64, 250]\n",
        "    Y = np.repeat(np.arange(26), 6)  # label：0~25，each character repeats 6 times\n",
        "\n",
        "    X = torch.from_numpy(X).float()\n",
        "    Y = torch.tensor(Y, dtype=torch.long)\n",
        "    return X, Y\n",
        "\n",
        "def get_dataloader(X, Y, batch_size, batch_size2, seed, shuffle=True):\n",
        "    X_train, X_test, Y_train, Y_test = train_test_split(\n",
        "        X, Y, test_size=0.2, shuffle=shuffle, stratify=Y, random_state=seed\n",
        "    )\n",
        "\n",
        "    training_set = EEGDataset(X_train, Y_train)\n",
        "    training_loader = DataLoader(training_set, batch_size=batch_size, shuffle=shuffle)\n",
        "\n",
        "    test_set = EEGDataset(X_test, Y_test)\n",
        "    test_loader = DataLoader(test_set, batch_size=batch_size2, shuffle=False)\n",
        "\n",
        "    return training_loader, test_loader"
      ],
      "metadata": {
        "id": "ZA5ajGmtryq3"
      },
      "id": "ZA5ajGmtryq3",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_all_subjects(root_dir, subject_ids, session=None):\n",
        "    all_X, all_Y = [], []\n",
        "    for sid in subject_ids:\n",
        "        X, Y = load_data(root_dir=root_dir, subject=sid, session=session)\n",
        "        # print(f\"[Subject {sid}] X: {X.shape}, Y: {Y.shape}\")\n",
        "        # if X.shape[0] != Y.shape[0]:\n",
        "        #     print(f\"Mismatch at subject {sid}!\")\n",
        "        all_X.append(X)\n",
        "        all_Y.append(Y)\n",
        "    X = torch.cat(all_X, dim=0)\n",
        "    Y = torch.cat(all_Y, dim=0)\n",
        "    return X, Y"
      ],
      "metadata": {
        "id": "E7OtlGcDr8ZZ"
      },
      "id": "E7OtlGcDr8ZZ",
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seed = 42\n",
        "random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "print(\"Random Seed: \", seed)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R1jZy_mPsxNI",
        "outputId": "36892423-e298-4150-9f70-d6a51e3b5065"
      },
      "id": "R1jZy_mPsxNI",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Seed:  42\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "subject_chars_dir = '/content/drive/MyDrive/project/dataset/ssvep/chars' # Where to store all character data\n",
        "subject_ids = list(range(1, 36)) #35 samples + 1\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "batch_size = 32\n",
        "batch_size2 = 260\n",
        "seed = 42\n",
        "shuffle = True\n",
        "\n",
        "# load all subjects' data\n",
        "X, Y = load_all_subjects(subject_chars_dir, subject_ids)\n",
        "train_loader, test_loader = get_dataloader(X, Y, batch_size, batch_size2, seed, shuffle=shuffle)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "seWeK5dArjjM",
        "outputId": "bb8782ef-bada-4dd6-d041-39535b867eb1"
      },
      "id": "seWeK5dArjjM",
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Subject 1] X: torch.Size([156, 64, 250]), Y: torch.Size([156])\n",
            "[Subject 2] X: torch.Size([156, 64, 250]), Y: torch.Size([156])\n",
            "[Subject 3] X: torch.Size([156, 64, 250]), Y: torch.Size([156])\n",
            "[Subject 4] X: torch.Size([156, 64, 250]), Y: torch.Size([156])\n",
            "[Subject 5] X: torch.Size([156, 64, 250]), Y: torch.Size([156])\n",
            "[Subject 6] X: torch.Size([156, 64, 250]), Y: torch.Size([156])\n",
            "[Subject 7] X: torch.Size([156, 64, 250]), Y: torch.Size([156])\n",
            "[Subject 8] X: torch.Size([156, 64, 250]), Y: torch.Size([156])\n",
            "[Subject 9] X: torch.Size([156, 64, 250]), Y: torch.Size([156])\n",
            "[Subject 10] X: torch.Size([156, 64, 250]), Y: torch.Size([156])\n",
            "[Subject 11] X: torch.Size([156, 64, 250]), Y: torch.Size([156])\n",
            "[Subject 12] X: torch.Size([156, 64, 250]), Y: torch.Size([156])\n",
            "[Subject 13] X: torch.Size([156, 64, 250]), Y: torch.Size([156])\n",
            "[Subject 14] X: torch.Size([156, 64, 250]), Y: torch.Size([156])\n",
            "[Subject 15] X: torch.Size([156, 64, 250]), Y: torch.Size([156])\n",
            "[Subject 16] X: torch.Size([156, 64, 250]), Y: torch.Size([156])\n",
            "[Subject 17] X: torch.Size([156, 64, 250]), Y: torch.Size([156])\n",
            "[Subject 18] X: torch.Size([156, 64, 250]), Y: torch.Size([156])\n",
            "[Subject 19] X: torch.Size([156, 64, 250]), Y: torch.Size([156])\n",
            "[Subject 20] X: torch.Size([156, 64, 250]), Y: torch.Size([156])\n",
            "[Subject 21] X: torch.Size([156, 64, 250]), Y: torch.Size([156])\n",
            "[Subject 22] X: torch.Size([156, 64, 250]), Y: torch.Size([156])\n",
            "[Subject 23] X: torch.Size([156, 64, 250]), Y: torch.Size([156])\n",
            "[Subject 24] X: torch.Size([156, 64, 250]), Y: torch.Size([156])\n",
            "[Subject 25] X: torch.Size([156, 64, 250]), Y: torch.Size([156])\n",
            "[Subject 26] X: torch.Size([156, 64, 250]), Y: torch.Size([156])\n",
            "[Subject 27] X: torch.Size([156, 64, 250]), Y: torch.Size([156])\n",
            "[Subject 28] X: torch.Size([156, 64, 250]), Y: torch.Size([156])\n",
            "[Subject 29] X: torch.Size([156, 64, 250]), Y: torch.Size([156])\n",
            "[Subject 30] X: torch.Size([156, 64, 250]), Y: torch.Size([156])\n",
            "[Subject 31] X: torch.Size([156, 64, 250]), Y: torch.Size([156])\n",
            "[Subject 32] X: torch.Size([156, 64, 250]), Y: torch.Size([156])\n",
            "[Subject 33] X: torch.Size([156, 64, 250]), Y: torch.Size([156])\n",
            "[Subject 34] X: torch.Size([156, 64, 250]), Y: torch.Size([156])\n",
            "[Subject 35] X: torch.Size([156, 64, 250]), Y: torch.Size([156])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"X shape:\", X.shape)\n",
        "print(\"Y shape:\", Y.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f0JmYf8_7foW",
        "outputId": "a7848b5a-1dd9-4fdd-cd8f-06096f09eae2"
      },
      "id": "f0JmYf8_7foW",
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X shape: torch.Size([5460, 64, 250])\n",
            "Y shape: torch.Size([5460])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#model"
      ],
      "metadata": {
        "id": "D-8XtejnbSv2"
      },
      "id": "D-8XtejnbSv2"
    },
    {
      "cell_type": "markdown",
      "source": [
        "The overall model follows the original GitHub implementation, with a few necessary adjustments to align tensor time dimensions. Specifically, we aligned the temporal dimensions between up2 + temb and down1, up3 and x, and x_hat and x. These changes are clearly marked with comments in the code."
      ],
      "metadata": {
        "id": "mV-wGFCs2hpy"
      },
      "id": "mV-wGFCs2hpy"
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "ceb479ce",
      "metadata": {
        "id": "ceb479ce"
      },
      "outputs": [],
      "source": [
        "#define diffE model\n",
        "def get_padding(kernel_size, dilation=1):\n",
        "    return int((kernel_size * dilation - dilation) / 2)\n",
        "\n",
        "\n",
        "# Swish activation function\n",
        "class Swish(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x * self.sigmoid(x)\n",
        "\n",
        "\n",
        "class SinusoidalPosEmb(nn.Module):\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "\n",
        "    def forward(self, x):\n",
        "        device = x.device\n",
        "        half_dim = self.dim // 2\n",
        "        emb = math.log(10000) / (half_dim - 1)\n",
        "        emb = torch.exp(torch.arange(half_dim, device=device) * -emb)\n",
        "        emb = x[:, None] * emb[None, :]\n",
        "        emb = torch.cat((emb.sin(), emb.cos()), dim=-1)\n",
        "        return emb\n",
        "\n",
        "\n",
        "class WeightStandardizedConv1d(nn.Conv1d):\n",
        "    \"\"\"\n",
        "    https://arxiv.org/abs/1903.10520\n",
        "    weight standardization purportedly works synergistically with group normalization\n",
        "    \"\"\"\n",
        "\n",
        "    def forward(self, x):\n",
        "        eps = 1e-5 if x.dtype == torch.float32 else 1e-3\n",
        "\n",
        "        weight = self.weight\n",
        "        mean = reduce(weight, \"o ... -> o 1 1\", \"mean\")\n",
        "        var = reduce(weight, \"o ... -> o 1 1\", partial(torch.var, unbiased=False))\n",
        "        normalized_weight = (weight - mean) * (var + eps).rsqrt()\n",
        "\n",
        "        return F.conv1d(\n",
        "            x,\n",
        "            normalized_weight,\n",
        "            self.bias,\n",
        "            self.stride,\n",
        "            self.padding,\n",
        "            self.dilation,\n",
        "            self.groups,\n",
        "        )\n",
        "\n",
        "\n",
        "class ResidualConvBlock(nn.Module):\n",
        "    def __init__(self, inc: int, outc: int, kernel_size: int, stride=1, gn=8):\n",
        "        super().__init__()\n",
        "        \"\"\"\n",
        "        standard ResNet style convolutional block\n",
        "        \"\"\"\n",
        "        self.same_channels = inc == outc\n",
        "        self.ks = kernel_size\n",
        "        self.conv = nn.Sequential(\n",
        "            WeightStandardizedConv1d(inc, outc, self.ks, stride, get_padding(self.ks)),\n",
        "            nn.GroupNorm(gn, outc),\n",
        "            nn.PReLU(),\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x1 = self.conv(x)\n",
        "        if self.same_channels:\n",
        "            out = (x + x1) / 2\n",
        "        else:\n",
        "            out = x1\n",
        "        return out\n",
        "\n",
        "\n",
        "class UnetDown(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, gn=8, factor=2):\n",
        "        super(UnetDown, self).__init__()\n",
        "        self.pool = nn.MaxPool1d(factor)\n",
        "        self.layer = ResidualConvBlock(in_channels, out_channels, kernel_size, gn=gn)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.layer(x)\n",
        "        x = self.pool(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class UnetUp(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, gn=8, factor=2):\n",
        "        super(UnetUp, self).__init__()\n",
        "        self.pool = nn.Upsample(scale_factor=factor, mode=\"nearest\")\n",
        "        self.layer = ResidualConvBlock(in_channels, out_channels, kernel_size, gn=gn)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(x)\n",
        "        x = self.layer(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class EmbedFC(nn.Module):\n",
        "    def __init__(self, input_dim, emb_dim):\n",
        "        super(EmbedFC, self).__init__()\n",
        "        \"\"\"\n",
        "        generic one layer FC NN for embedding things\n",
        "        \"\"\"\n",
        "        self.input_dim = input_dim\n",
        "        layers = [\n",
        "            nn.Linear(input_dim, emb_dim),\n",
        "            nn.PReLU(),\n",
        "            nn.Linear(emb_dim, emb_dim),\n",
        "        ]\n",
        "        self.model = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, self.input_dim)\n",
        "        return self.model(x)\n",
        "\n",
        "\n",
        "class ConditionalUNet(nn.Module):\n",
        "    def __init__(self, in_channels, n_feat=256):\n",
        "        super(ConditionalUNet, self).__init__()\n",
        "\n",
        "        self.in_channels = in_channels\n",
        "        self.n_feat = n_feat\n",
        "\n",
        "        self.d1_out = n_feat * 1\n",
        "        self.d2_out = n_feat * 2\n",
        "        self.d3_out = n_feat * 3\n",
        "        self.d4_out = n_feat * 4\n",
        "\n",
        "        self.u1_out = n_feat\n",
        "        self.u2_out = n_feat\n",
        "        self.u3_out = n_feat\n",
        "        self.u4_out = in_channels\n",
        "\n",
        "        self.sin_emb = SinusoidalPosEmb(n_feat)\n",
        "        # self.timeembed1 = EmbedFC(n_feat, self.u1_out)\n",
        "        # self.timeembed2 = EmbedFC(n_feat, self.u2_out)\n",
        "        # self.timeembed3 = EmbedFC(n_feat, self.u3_out)\n",
        "\n",
        "        self.down1 = UnetDown(in_channels, self.d1_out, 1, gn=8, factor=2)\n",
        "        self.down2 = UnetDown(self.d1_out, self.d2_out, 1, gn=8, factor=2)\n",
        "        self.down3 = UnetDown(self.d2_out, self.d3_out, 1, gn=8, factor=2)\n",
        "\n",
        "        self.up2 = UnetUp(self.d3_out, self.u2_out, 1, gn=8, factor=2)\n",
        "        self.up3 = UnetUp(self.u2_out + self.d2_out, self.u3_out, 1, gn=8, factor=2)\n",
        "        self.up4 = UnetUp(self.u3_out + self.d1_out, self.u4_out, 1, gn=8, factor=2)\n",
        "        self.out = nn.Conv1d(self.u4_out + in_channels, in_channels, 1)\n",
        "\n",
        "    def forward(self, x, t):\n",
        "        down1 = self.down1(x)  # 2000 -> 1000\n",
        "        down2 = self.down2(down1)  # 1000 -> 500\n",
        "        down3 = self.down3(down2)  # 500 -> 250\n",
        "\n",
        "        temb = self.sin_emb(t).view(-1, self.n_feat, 1)  # [b, n_feat, 1]\n",
        "\n",
        "        up1 = self.up2(down3)  # 250 -> 500\n",
        "        up2 = self.up3(torch.cat([up1 + temb, down2], 1))  # 500 -> 1000\n",
        "\n",
        "        # Align the temporal dimension of up2 + temb and down1\n",
        "        if (up2 + temb).shape[-1] != down1.shape[-1]:\n",
        "            target_len = min((up2 + temb).shape[-1], down1.shape[-1])\n",
        "            up2 = F.interpolate(up2, size=target_len)\n",
        "            down1 = F.interpolate(down1, size=target_len)\n",
        "\n",
        "        up3 = self.up4(torch.cat([up2 + temb, down1], 1))  # 1000 -> 2000\n",
        "\n",
        "        # Align the temporal dimension of up3 and x\n",
        "        if up3.shape[-1] != x.shape[-1]:\n",
        "            target_len = min(up3.shape[-1], x.shape[-1])\n",
        "            up3 = F.interpolate(up3, size=target_len)\n",
        "            x = F.interpolate(x, size=target_len)\n",
        "\n",
        "        out = self.out(torch.cat([up3, x], 1))  # 2000 -> 2000\n",
        "\n",
        "        down = (down1, down2, down3)\n",
        "        up = (up1, up2, up3)\n",
        "        return out, down, up\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, in_channels, dim=512):\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        self.in_channels = in_channels\n",
        "        self.e1_out = dim\n",
        "        self.e2_out = dim\n",
        "        self.e3_out = dim\n",
        "\n",
        "        self.down1 = UnetDown(in_channels, self.e1_out, 1, gn=8, factor=2)\n",
        "        self.down2 = UnetDown(self.e1_out, self.e2_out, 1, gn=8, factor=2)\n",
        "        self.down3 = UnetDown(self.e2_out, self.e3_out, 1, gn=8, factor=2)\n",
        "\n",
        "        self.avg_pooling = nn.AdaptiveAvgPool1d(output_size=1)\n",
        "        self.max_pooling = nn.AdaptiveMaxPool1d(output_size=1)\n",
        "        self.act = nn.Tanh()\n",
        "\n",
        "    def forward(self, x0):\n",
        "        # Down sampling\n",
        "        dn1 = self.down1(x0)  # 2048 -> 1024\n",
        "        dn2 = self.down2(dn1)  # 1024 -> 512\n",
        "        dn3 = self.down3(dn2)  # 512 -> 256\n",
        "        z = self.avg_pooling(dn3).view(-1, self.e3_out)  # [b, features]\n",
        "        down = (dn1, dn2, dn3)\n",
        "        out = (down, z)\n",
        "        return out\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, in_channels, n_feat=256, encoder_dim=512, n_classes=13):\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        self.in_channels = in_channels\n",
        "        self.n_feat = n_feat\n",
        "        self.n_classes = n_classes\n",
        "        self.e1_out = encoder_dim\n",
        "        self.e2_out = encoder_dim\n",
        "        self.e3_out = encoder_dim\n",
        "        self.d1_out = n_feat\n",
        "        self.d2_out = n_feat * 2\n",
        "        self.d3_out = n_feat * 3\n",
        "        self.u1_out = n_feat\n",
        "        self.u2_out = n_feat\n",
        "        self.u3_out = n_feat\n",
        "        self.u4_out = in_channels\n",
        "\n",
        "        # self.sin_emb = SinusoidalPosEmb(n_feat)\n",
        "        # self.timeembed1 = EmbedFC(n_feat, self.e3_out)\n",
        "        # self.timeembed2 = EmbedFC(n_feat, self.u2_out)\n",
        "        # self.timeembed3 = EmbedFC(n_feat, self.u3_out)\n",
        "        # self.contextembed1 = EmbedFC(self.e3_out, self.e3_out)\n",
        "        # self.contextembed2 = EmbedFC(self.e3_out, self.u2_out)\n",
        "        # self.contextembed3 = EmbedFC(self.e3_out, self.u3_out)\n",
        "\n",
        "        # Unet up sampling\n",
        "        self.up1 = UnetUp(self.d3_out + self.e3_out, self.u2_out, 1, gn=8, factor=2)\n",
        "        self.up2 = UnetUp(self.d2_out + self.u2_out, self.u3_out, 1, gn=8, factor=2)\n",
        "        self.up3 = nn.Sequential(\n",
        "            nn.Upsample(scale_factor=2, mode=\"nearest\"),\n",
        "            nn.Conv1d(\n",
        "                self.d1_out + self.u3_out + in_channels * 2, in_channels, 1, 1, 0\n",
        "            ),\n",
        "        )\n",
        "\n",
        "        # self.out = nn.Conv1d(self.u4_out+in_channels, in_channels, 1)\n",
        "        self.pool = nn.AvgPool1d(2)\n",
        "\n",
        "    def forward(self, x0, encoder_out, diffusion_out):\n",
        "        # Encoder output\n",
        "        down, z = encoder_out\n",
        "        dn1, dn2, dn3 = down\n",
        "\n",
        "        # DDPM output\n",
        "        x_hat, down_ddpm, up, t = diffusion_out\n",
        "        dn11, dn22, dn33 = down_ddpm\n",
        "\n",
        "        # embed context, time step\n",
        "        # temb = self.sin_emb(t).view(-1, self.n_feat, 1) # [b, n_feat, 1]\n",
        "        # temb1 = self.timeembed1(temb).view(-1, self.e3_out, 1) # [b, features]\n",
        "        # temb2 = self.timeembed2(temb).view(-1, self.u2_out, 1) # [b, features]\n",
        "        # temb3 = self.timeembed3(temb).view(-1, self.u3_out, 1) # [b, features]\n",
        "        # ct2 = self.contextembed2(z).view(-1, self.u2_out, 1) # [b, n_feat, 1]\n",
        "        # ct3 = self.contextembed3(z).view(-1, self.u3_out, 1) # [b, n_feat, 1]\n",
        "\n",
        "        # Up sampling\n",
        "        up1 = self.up1(torch.cat([dn3, dn33.detach()], 1))\n",
        "        up2 = self.up2(torch.cat([up1, dn22.detach()], 1))\n",
        "        out = self.up3(\n",
        "            torch.cat([self.pool(x0), self.pool(x_hat.detach()), up2, dn11.detach()], 1)\n",
        "        )\n",
        "        return out\n",
        "\n",
        "\n",
        "class DiffE(nn.Module):\n",
        "    def __init__(self, encoder, decoder, fc):\n",
        "        super(DiffE, self).__init__()\n",
        "\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.fc = fc\n",
        "\n",
        "    def forward(self, x0, ddpm_out):\n",
        "        encoder_out = self.encoder(x0)\n",
        "        decoder_out = self.decoder(x0, encoder_out, ddpm_out)\n",
        "        fc_out = self.fc(encoder_out[1])\n",
        "        return decoder_out, fc_out\n",
        "\n",
        "\n",
        "class DecoderNoDiff(nn.Module):\n",
        "    def __init__(self, in_channels, n_feat=256, encoder_dim=512, n_classes=13):\n",
        "        super(DecoderNoDiff, self).__init__()\n",
        "\n",
        "        self.in_channels = in_channels\n",
        "        self.n_feat = n_feat\n",
        "        self.n_classes = n_classes\n",
        "        self.e1_out = encoder_dim\n",
        "        self.e2_out = encoder_dim\n",
        "        self.e3_out = encoder_dim\n",
        "        self.u1_out = n_feat\n",
        "        self.u2_out = n_feat\n",
        "        self.u3_out = n_feat\n",
        "        self.u4_out = n_feat\n",
        "\n",
        "        self.sin_emb = SinusoidalPosEmb(n_feat)\n",
        "        self.timeembed1 = EmbedFC(n_feat, self.e3_out)\n",
        "        self.timeembed2 = EmbedFC(n_feat, self.u2_out)\n",
        "        self.timeembed3 = EmbedFC(n_feat, self.u3_out)\n",
        "        self.contextembed1 = EmbedFC(self.e3_out, self.e3_out)\n",
        "        self.contextembed2 = EmbedFC(self.e3_out, self.u2_out)\n",
        "        self.contextembed3 = EmbedFC(self.e3_out, self.u3_out)\n",
        "\n",
        "        # Unet up sampling\n",
        "        self.up2 = UnetUp(self.e3_out, self.u2_out, 1, gn=8, factor=2)\n",
        "        self.up3 = UnetUp(self.e2_out + self.u2_out, self.u3_out, 1, gn=8, factor=2)\n",
        "        # self.up4 = UnetUp(self.e1_out+self.u3_out, self.u4_out, 1, 1, gn=in_channels, factor=2, is_res=True)\n",
        "        self.up4 = nn.Sequential(\n",
        "            nn.Upsample(scale_factor=2, mode=\"nearest\"),\n",
        "            nn.Conv1d(self.u3_out + self.e1_out + in_channels, in_channels, 1, 1, 0),\n",
        "        )\n",
        "\n",
        "        self.out = nn.Conv1d(self.u4_out, in_channels, 1)\n",
        "        self.pool = nn.AvgPool1d(2)\n",
        "\n",
        "    def forward(self, x0, x_hat, encoder_out, t):\n",
        "        down, z = encoder_out\n",
        "        dn1, dn2, dn3 = down\n",
        "        tembd = self.sin_emb(t).view(-1, self.n_feat, 1)  # [b, n_feat, 1]\n",
        "        tembd1 = self.timeembed1(self.sin_emb(t)).view(\n",
        "            -1, self.e3_out, 1\n",
        "        )  # [b, n_feat, 1]\n",
        "        tembd2 = self.timeembed2(self.sin_emb(t)).view(\n",
        "            -1, self.u2_out, 1\n",
        "        )  # [b, n_feat, 1]\n",
        "        tembd3 = self.timeembed3(self.sin_emb(t)).view(\n",
        "            -1, self.u3_out, 1\n",
        "        )  # [b, n_feat, 1]\n",
        "\n",
        "        # Up sampling\n",
        "        ddpm_loss = F.l1_loss(x0, x_hat, reduction=\"none\")\n",
        "\n",
        "        up2 = self.up2(dn3)  # 256 -> 512\n",
        "        up3 = self.up3(torch.cat([up2, dn2], 1))  # 512 -> 1024\n",
        "        out = self.up4(\n",
        "            torch.cat([self.pool(x0), self.pool(x_hat), up3, dn1], 1)\n",
        "        )  # 1024 -> 2048\n",
        "        # out = self.out(torch.cat([out, x_hat], 1)) # 2048 -> 2048\n",
        "        # out = self.out(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class LinearClassifier(nn.Module):\n",
        "    def __init__(self, in_dim, latent_dim, emb_dim):\n",
        "        super().__init__()\n",
        "        self.linear_out = nn.Sequential(\n",
        "            nn.Linear(in_features=in_dim, out_features=latent_dim),\n",
        "            nn.GroupNorm(4, latent_dim),\n",
        "            nn.PReLU(),\n",
        "            nn.Linear(in_features=latent_dim, out_features=latent_dim),\n",
        "            nn.GroupNorm(4, latent_dim),\n",
        "            nn.PReLU(),\n",
        "            nn.Linear(in_features=latent_dim, out_features=emb_dim),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.linear_out(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "def cosine_beta_schedule(timesteps, s=0.008):\n",
        "    \"\"\"\n",
        "    cosine schedule\n",
        "    as proposed in https://openreview.net/forum?id=-NEXDKk8gZ\n",
        "    \"\"\"\n",
        "    steps = timesteps + 1\n",
        "    t = torch.linspace(0, timesteps, steps, dtype=torch.float64) / timesteps\n",
        "    alphas_cumprod = torch.cos((t + s) / (1 + s) * math.pi * 0.5) ** 2\n",
        "    alphas_cumprod = alphas_cumprod / alphas_cumprod[0]\n",
        "    betas = 1 - (alphas_cumprod[1:] / alphas_cumprod[:-1])\n",
        "    return torch.clip(betas, 0, 0.999)\n",
        "\n",
        "\n",
        "def sigmoid_beta_schedule(timesteps, start=-3, end=3, tau=1, clamp_min=1e-5):\n",
        "    \"\"\"\n",
        "    sigmoid schedule\n",
        "    proposed in https://arxiv.org/abs/2212.11972 - Figure 8\n",
        "    \"\"\"\n",
        "    steps = timesteps + 1\n",
        "    t = torch.linspace(0, timesteps, steps, dtype=torch.float64) / timesteps\n",
        "    v_start = torch.tensor(start / tau).sigmoid()\n",
        "    v_end = torch.tensor(end / tau).sigmoid()\n",
        "    alphas_cumprod = (-((t * (end - start) + start) / tau).sigmoid() + v_end) / (\n",
        "        v_end - v_start\n",
        "    )\n",
        "    alphas_cumprod = alphas_cumprod / alphas_cumprod[0]\n",
        "    betas = 1 - (alphas_cumprod[1:] / alphas_cumprod[:-1])\n",
        "    return torch.clip(betas, 0, 0.999)\n",
        "\n",
        "\n",
        "def ddpm_schedules(beta1, beta2, T):\n",
        "    # assert beta1 < beta2 < 1.0, \"beta1 and beta2 must be in (0, 1)\"\n",
        "    # beta_t = (beta2 - beta1) * torch.arange(0, T + 1, dtype=torch.float32) / T + beta1\n",
        "    beta_t = cosine_beta_schedule(T, s=0.008).float()\n",
        "    # beta_t = sigmoid_beta_schedule(T).float()\n",
        "\n",
        "    alpha_t = 1 - beta_t\n",
        "\n",
        "    log_alpha_t = torch.log(alpha_t)\n",
        "    alphabar_t = torch.cumsum(log_alpha_t, dim=0).exp()\n",
        "\n",
        "    sqrtab = torch.sqrt(alphabar_t)\n",
        "\n",
        "    sqrtmab = torch.sqrt(1 - alphabar_t)\n",
        "\n",
        "    return {\n",
        "        \"sqrtab\": sqrtab,  # \\sqrt{\\bar{\\alpha_t}}\n",
        "        \"sqrtmab\": sqrtmab,  # \\sqrt{1-\\bar{\\alpha_t}}\n",
        "    }\n",
        "\n",
        "\n",
        "class DDPM(nn.Module):\n",
        "    def __init__(self, nn_model, betas, n_T, device):\n",
        "        super(DDPM, self).__init__()\n",
        "        self.nn_model = nn_model.to(device)\n",
        "\n",
        "        for k, v in ddpm_schedules(betas[0], betas[1], n_T).items():\n",
        "            self.register_buffer(k, v)\n",
        "\n",
        "        self.n_T = n_T\n",
        "        self.device = device\n",
        "\n",
        "    def forward(self, x):\n",
        "        _ts = torch.randint(1, self.n_T, (x.shape[0],)).to(\n",
        "            self.device\n",
        "        )  # t ~ Uniform(0, n_T)\n",
        "        noise = torch.randn_like(x)  # eps ~ N(0, 1)\n",
        "        x_t = self.sqrtab[_ts, None, None] * x + self.sqrtmab[_ts, None, None] * noise\n",
        "        times = _ts / self.n_T\n",
        "        output, down, up = self.nn_model(x_t, times)\n",
        "        return output, down, up, noise, times"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define model\n",
        "num_classes = 26\n",
        "channels = X.shape[1]\n",
        "\n",
        "n_T = 1000\n",
        "ddpm_dim = 128\n",
        "encoder_dim = 256\n",
        "fc_dim = 512\n",
        "\n",
        "ddpm_model = ConditionalUNet(in_channels=channels, n_feat=ddpm_dim).to(device)\n",
        "ddpm = DDPM(nn_model=ddpm_model, betas=(1e-6, 1e-2), n_T=n_T, device=device).to(device)\n",
        "encoder = Encoder(in_channels=channels, dim=encoder_dim).to(device)\n",
        "decoder = Decoder(in_channels=channels, n_feat=ddpm_dim, encoder_dim=encoder_dim).to(device)\n",
        "fc = LinearClassifier(encoder_dim, fc_dim, emb_dim=num_classes).to(device)\n",
        "diffe = DiffE(encoder, decoder, fc).to(device)\n",
        "\n",
        "print(\"ddpm size: \", sum(p.numel() for p in ddpm.parameters()))\n",
        "print(\"encoder size: \", sum(p.numel() for p in encoder.parameters()))\n",
        "print(\"decoder size: \", sum(p.numel() for p in decoder.parameters()))\n",
        "print(\"fc size: \", sum(p.numel() for p in fc.parameters()))"
      ],
      "metadata": {
        "id": "6Rigb_x3lTcZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06b1adf3-5581-465f-f503-e358bf0ab515"
      },
      "id": "6Rigb_x3lTcZ",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ddpm size:  265478\n",
            "encoder size:  149763\n",
            "decoder size:  156482\n",
            "fc size:  409628\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#train"
      ],
      "metadata": {
        "id": "gNFmH1U1cUfr"
      },
      "id": "gNFmH1U1cUfr"
    },
    {
      "cell_type": "code",
      "source": [
        "# Criterion\n",
        "criterion = nn.L1Loss()\n",
        "criterion_class = nn.MSELoss()\n",
        "\n",
        "# Define optimizer\n",
        "base_lr, lr = 9e-5, 1.5e-3\n",
        "optim1 = optim.RMSprop(ddpm.parameters(), lr=base_lr)\n",
        "optim2 = optim.RMSprop(diffe.parameters(), lr=base_lr)\n",
        "\n",
        "# EMAs\n",
        "fc_ema = EMA(diffe.fc, beta=0.95, update_after_step=100, update_every=10,)\n",
        "\n",
        "step_size = 150\n",
        "scheduler1 = optim.lr_scheduler.CyclicLR(\n",
        "    optimizer=optim1,\n",
        "    base_lr=base_lr,\n",
        "    max_lr=lr,\n",
        "    step_size_up=step_size,\n",
        "    mode=\"exp_range\",\n",
        "    cycle_momentum=False,\n",
        "    gamma=0.9998,\n",
        ")\n",
        "scheduler2 = optim.lr_scheduler.CyclicLR(\n",
        "    optimizer=optim2,\n",
        "    base_lr=base_lr,\n",
        "    max_lr=lr,\n",
        "    step_size_up=step_size,\n",
        "    mode=\"exp_range\",\n",
        "    cycle_momentum=False,\n",
        "    gamma=0.9998,\n",
        ")"
      ],
      "metadata": {
        "id": "e6b9kOFsmNm2"
      },
      "id": "e6b9kOFsmNm2",
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate function\n",
        "def evaluate(encoder, fc, generator, device):\n",
        "    labels = np.arange(0, 26)\n",
        "    Y = []\n",
        "    Y_hat = []\n",
        "    for x, y in generator:\n",
        "        x, y = x.to(device), y.type(torch.LongTensor).to(device)\n",
        "        encoder_out = encoder(x)\n",
        "        y_hat = fc(encoder_out[1])\n",
        "        y_hat = F.softmax(y_hat, dim=1)\n",
        "\n",
        "        Y.append(y.detach().cpu())\n",
        "        Y_hat.append(y_hat.detach().cpu())\n",
        "\n",
        "    # List of tensors to tensor to numpy\n",
        "    Y = torch.cat(Y, dim=0).numpy()  # (N, )\n",
        "    Y_hat = torch.cat(Y_hat, dim=0).numpy()  # (N, 13): has to sum to 1 for each row\n",
        "\n",
        "    # Accuracy and Confusion Matrix\n",
        "    accuracy = top_k_accuracy_score(Y, Y_hat, k=1, labels=labels)\n",
        "    f1 = f1_score(Y, Y_hat.argmax(axis=1), average=\"macro\", labels=labels)\n",
        "    recall = recall_score(Y, Y_hat.argmax(axis=1), average=\"macro\", labels=labels)\n",
        "    precision = precision_score(Y, Y_hat.argmax(axis=1), average=\"macro\", labels=labels)\n",
        "    auc = roc_auc_score(Y, Y_hat, average=\"macro\", multi_class=\"ovo\", labels=labels)\n",
        "\n",
        "    metrics = {\n",
        "        \"accuracy\": accuracy,\n",
        "        \"f1\": f1,\n",
        "        \"recall\": recall,\n",
        "        \"precision\": precision,\n",
        "        \"auc\": auc,\n",
        "    }\n",
        "    # df_cm = pd.DataFrame(confusion_matrix(Y, Y_hat.argmax(axis=1)))\n",
        "    return metrics\n"
      ],
      "metadata": {
        "id": "DEi7SkZfvNbe"
      },
      "id": "DEi7SkZfvNbe",
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train & Evaluate\n",
        "args = argparse.Namespace()\n",
        "args.device = device\n",
        "args.subject = \"ALL\"\n",
        "args.root_dir = subject_chars_dir\n",
        "subject = args.subject\n",
        "\n",
        "num_epochs = 500\n",
        "test_period = 1\n",
        "start_test = test_period\n",
        "alpha = 0.1\n",
        "\n",
        "best_acc = 0\n",
        "best_f1 = 0\n",
        "best_recall = 0\n",
        "best_precision = 0\n",
        "best_auc = 0\n",
        "\n",
        "with tqdm(total=num_epochs, desc=f\"Method ALL - Joint Training on All Subjects\") as pbar:\n",
        "    for epoch in range(num_epochs):\n",
        "        ddpm.train()\n",
        "        diffe.train()\n",
        "\n",
        "        ############################## Train ###########################################\n",
        "        for x, y in train_loader:\n",
        "            x, y = x.to(device), y.type(torch.LongTensor).to(device)\n",
        "            y_cat = F.one_hot(y, num_classes=26).type(torch.FloatTensor).to(device)\n",
        "            # Train DDPM\n",
        "            optim1.zero_grad()\n",
        "            x_hat, down, up, noise, t = ddpm(x)\n",
        "\n",
        "            # Align the temporal dimension of x_hat and x\n",
        "            if x_hat.shape[-1] != x.shape[-1]:\n",
        "                target_len = min(x_hat.shape[-1], x.shape[-1])\n",
        "                x_hat = F.interpolate(x_hat, size=target_len)\n",
        "                x = F.interpolate(x, size=target_len)\n",
        "\n",
        "            loss_ddpm = F.l1_loss(x_hat, x, reduction=\"none\")\n",
        "            loss_ddpm.mean().backward()\n",
        "            optim1.step()\n",
        "            ddpm_out = x_hat, down, up, t\n",
        "\n",
        "            # Train Diff-E\n",
        "            optim2.zero_grad()\n",
        "            decoder_out, fc_out = diffe(x, ddpm_out)\n",
        "\n",
        "            loss_gap = criterion(decoder_out, loss_ddpm.detach())\n",
        "            loss_c = criterion_class(fc_out, y_cat)\n",
        "            loss = loss_gap + alpha * loss_c\n",
        "            loss.backward()\n",
        "            optim2.step()\n",
        "\n",
        "            # Optimizer scheduler step\n",
        "            scheduler1.step()\n",
        "            scheduler2.step()\n",
        "\n",
        "            # EMA update\n",
        "            fc_ema.update()\n",
        "\n",
        "        ############################## Test ###########################################\n",
        "        with torch.no_grad():\n",
        "            if epoch > start_test:\n",
        "                test_period = 1\n",
        "            if epoch % test_period == 0:\n",
        "                ddpm.eval()\n",
        "                diffe.eval()\n",
        "\n",
        "                metrics_test = evaluate(diffe.encoder, fc_ema, test_loader, device)\n",
        "\n",
        "                acc = metrics_test[\"accuracy\"]\n",
        "                f1 = metrics_test[\"f1\"]\n",
        "                recall = metrics_test[\"recall\"]\n",
        "                precision = metrics_test[\"precision\"]\n",
        "                auc = metrics_test[\"auc\"]\n",
        "\n",
        "                best_acc_bool = acc > best_acc\n",
        "                best_f1_bool = f1 > best_f1\n",
        "                best_recall_bool = recall > best_recall\n",
        "                best_precision_bool = precision > best_precision\n",
        "                best_auc_bool = auc > best_auc\n",
        "\n",
        "                if best_acc_bool:\n",
        "                    best_acc = acc\n",
        "                    #torch.save(diffe.state_dict(), f'/content/drive/MyDrive/project/model/ssvep/diffe_{subject}.pth')\n",
        "                    torch.save(diffe.state_dict(), '/content/drive/MyDrive/project/model/ssvep/diffe_all_subjects.pth')\n",
        "                if best_f1_bool:\n",
        "                    best_f1 = f1\n",
        "                if best_recall_bool:\n",
        "                    best_recall = recall\n",
        "                if best_precision_bool:\n",
        "                    best_precision = precision\n",
        "                if best_auc_bool:\n",
        "                    best_auc = auc\n",
        "\n",
        "                # print(\"Subject: {0}\".format(subject))\n",
        "                # # print(\"ddpm test loss: {0:.4f}\".format(t_test_loss_ddpm/len(test_generator)))\n",
        "                # # print(\"encoder test loss: {0:.4f}\".format(t_test_loss_ed/len(test_generator)))\n",
        "                # print(\"accuracy:  {0:.2f}%\".format(acc*100), \"best: {0:.2f}%\".format(best_acc*100))\n",
        "                # print(\"f1-score:  {0:.2f}%\".format(f1*100), \"best: {0:.2f}%\".format(best_f1*100))\n",
        "                # print(\"recall:    {0:.2f}%\".format(recall*100), \"best: {0:.2f}%\".format(best_recall*100))\n",
        "                # print(\"precision: {0:.2f}%\".format(precision*100), \"best: {0:.2f}%\".format(best_precision*100))\n",
        "                # print(\"auc:       {0:.2f}%\".format(auc*100), \"best: {0:.2f}%\".format(best_auc*100))\n",
        "                # writer.add_scalar(f\"EEGNet/Accuracy/subject_{subject}\", acc*100, epoch)\n",
        "                # writer.add_scalar(f\"EEGNet/F1-score/subject_{subject}\", f1*100, epoch)\n",
        "                # writer.add_scalar(f\"EEGNet/Recall/subject_{subject}\", recall*100, epoch)\n",
        "                # writer.add_scalar(f\"EEGNet/Precision/subject_{subject}\", precision*100, epoch)\n",
        "                # writer.add_scalar(f\"EEGNet/AUC/subject_{subject}\", auc*100, epoch)\n",
        "\n",
        "                # if best_acc_bool or best_f1_bool or best_recall_bool or best_precision_bool or best_auc_bool:\n",
        "                #     performance = {'subject': subject,\n",
        "                #                 'epoch': epoch,\n",
        "                #                 'accuracy': best_acc*100,\n",
        "                #                 'f1_score': best_f1*100,\n",
        "                #                 'recall': best_recall*100,\n",
        "                #                 'precision': best_precision*100,\n",
        "                #                 'auc': best_auc*100\n",
        "                #                 }\n",
        "                #     with open(output_file, 'a') as f:\n",
        "                #         f.write(f\"{performance['subject']}, {performance['epoch']}, {performance['accuracy']}, {performance['f1_score']}, {performance['recall']}, {performance['precision']}, {performance['auc']}\\n\")\n",
        "                description = f\"Accuracy: {acc*100:.2f}% | Best: {best_acc*100:.2f}%\"\n",
        "                pbar.set_description(f\"Method ALL – Processing subject {subject} – {description}\"\n",
        "                )\n",
        "        pbar.update(1)"
      ],
      "metadata": {
        "id": "NIjWCm5vUKx-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 77,
          "referenced_widgets": [
            "15585251361643ab98c2945d25d0d554",
            "4e781dd1c0aa49cb9469d78b5022c5df",
            "23dac5e52d7541d79e2d4fe31f325f53",
            "7df8b49454e04da6b30f54ef64acd666",
            "5182d4851197406782a1f249f4fbce66",
            "5e5ab200c4124d33bcc6b674b81faad7",
            "c14fb649878c4ee7a0a1e61112560390",
            "3fa6794e4f834fb7bd576db8939f5774",
            "87774bb4a66e42c297b7a828107fc9aa",
            "399d8561dd064d498a2a31dcd116956c",
            "814008c594dd4c75872301d85b00e453"
          ]
        },
        "outputId": "46bffd16-f1d5-47ee-c531-c5e5777a191b"
      },
      "id": "NIjWCm5vUKx-",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Method ALL - Joint Training on All Subjects:   0%|          | 0/500 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "15585251361643ab98c2945d25d0d554"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Evaluate on test data"
      ],
      "metadata": {
        "id": "xs-0xKrFvCnJ"
      },
      "id": "xs-0xKrFvCnJ"
    },
    {
      "cell_type": "code",
      "source": [
        "diffe.eval()\n",
        "\n",
        "# evaluate\n",
        "labels = np.arange(0, 26)  # necessary change: 26 classes\n",
        "Y = []\n",
        "Y_hat = []\n",
        "with torch.no_grad():\n",
        "    for x, y in test_loader:\n",
        "        x, y = x.to(device), y.type(torch.LongTensor).to(device)\n",
        "        encoder_out = diffe.encoder(x)\n",
        "        y_hat = diffe.fc(encoder_out[1])\n",
        "        y_hat = F.softmax(y_hat, dim=1)\n",
        "\n",
        "        Y.append(y.detach().cpu())\n",
        "        Y_hat.append(y_hat.detach().cpu())\n",
        "\n",
        "Y = torch.cat(Y, dim=0).numpy()  # (N, )\n",
        "Y_hat = torch.cat(Y_hat, dim=0).numpy()  # (N, 26)\n",
        "\n",
        "accuracy = top_k_accuracy_score(Y, Y_hat, k=1, labels=labels)\n",
        "print(f'Test accuracy: {accuracy:.2%}')"
      ],
      "metadata": {
        "id": "9Kaz_i-8VJfi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b007e678-557b-4cc1-d870-d4482cf519f6"
      },
      "id": "9Kaz_i-8VJfi",
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy: 13.10%\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "15585251361643ab98c2945d25d0d554": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4e781dd1c0aa49cb9469d78b5022c5df",
              "IPY_MODEL_23dac5e52d7541d79e2d4fe31f325f53",
              "IPY_MODEL_7df8b49454e04da6b30f54ef64acd666"
            ],
            "layout": "IPY_MODEL_5182d4851197406782a1f249f4fbce66"
          }
        },
        "4e781dd1c0aa49cb9469d78b5022c5df": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5e5ab200c4124d33bcc6b674b81faad7",
            "placeholder": "​",
            "style": "IPY_MODEL_c14fb649878c4ee7a0a1e61112560390",
            "value": "Method ALL – Processing subject ALL – Accuracy: 55.68% | Best: 59.89%:  30%"
          }
        },
        "23dac5e52d7541d79e2d4fe31f325f53": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3fa6794e4f834fb7bd576db8939f5774",
            "max": 500,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_87774bb4a66e42c297b7a828107fc9aa",
            "value": 150
          }
        },
        "7df8b49454e04da6b30f54ef64acd666": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_399d8561dd064d498a2a31dcd116956c",
            "placeholder": "​",
            "style": "IPY_MODEL_814008c594dd4c75872301d85b00e453",
            "value": " 150/500 [08:08&lt;18:19,  3.14s/it]"
          }
        },
        "5182d4851197406782a1f249f4fbce66": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5e5ab200c4124d33bcc6b674b81faad7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c14fb649878c4ee7a0a1e61112560390": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3fa6794e4f834fb7bd576db8939f5774": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "87774bb4a66e42c297b7a828107fc9aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "399d8561dd064d498a2a31dcd116956c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "814008c594dd4c75872301d85b00e453": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}