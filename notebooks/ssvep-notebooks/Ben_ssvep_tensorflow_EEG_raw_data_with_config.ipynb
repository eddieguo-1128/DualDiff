{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0017f312",
   "metadata": {
    "id": "0017f312"
   },
   "source": [
    "# SSVEP Character Classification with DiffE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "WnZeypG6gsYU",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "WnZeypG6gsYU",
    "outputId": "97ecbbad-d616-4794-b73d-184bb9e3a8a6"
   },
   "outputs": [],
   "source": [
    "#!pip install ema-pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "s8yKwaRCQxyM",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "s8yKwaRCQxyM",
    "outputId": "6db7da5b-c0d8-4691-9dd9-39dc83ae5a8e"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "#!git clone https://github.com/diffe2023/Diff-E.git\n",
    "#sys.path.append('/content/Diff-E')\n",
    "sys.path.append(r'C:\\LTI 11785 Introduction to deep learning\\project\\SSVEP\\Diff-E')\n",
    "sys.path.append(r'C:\\LTI 11785 Introduction to deep learning\\project\\SSVEP\\arl-eegmodels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4MhV2qw9hu-m",
   "metadata": {
    "id": "4MhV2qw9hu-m"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from einops import reduce\n",
    "from functools import partial\n",
    "import random\n",
    "import argparse\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchaudio.transforms as T\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from ema_pytorch import EMA\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    f1_score,\n",
    "    roc_auc_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    top_k_accuracy_score,\n",
    ")\n",
    "\n",
    "from EEGModels import EEGNet, ShallowConvNet, DeepConvNet\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import KFold\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.initializers import RandomNormal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bc7c99e6-2046-4f2d-8323-813c144db7f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'Name': 'Yucheng Shao',\n",
    "    #'subset': 1,\n",
    "    'batch_size': 384,\n",
    "    'eeg_lr': 0.0012,\n",
    "    'num_classes': 40,\n",
    "    \n",
    "    'epochs_AEs': 500,\n",
    "    'alpha': 0,\n",
    "    'beta': 1,\n",
    "    'ddpm_dim': 128,\n",
    "    'encoder_dim': 256,\n",
    "    'fc_dim': 512,\n",
    "    'n_T': 1000,\n",
    "    'AE_base_lr': 9e-5,\n",
    "    'AE_max_lr': 1.5e-3,\n",
    "\n",
    "    'epochs_EEG': 300,\n",
    "    'eeg_F1': 16,\n",
    "    'eeg_F2': 32,\n",
    "    'eeg_D': 2,\n",
    "    'eeg_droupout': 0.2,\n",
    "    'eeg_channels': 64,\n",
    "    'eeg_samples': 250,\n",
    "    'eeg_kernel_length': 64,\n",
    "    'eeg_pool_1': 4,\n",
    "    'eeg_pool_2': 8,\n",
    "\n",
    "    'checkpoint_dir': r'C:\\LTI 11785 Introduction to deep learning\\project\\SSVEP\\checkpoints_separated',\n",
    "    'augument': False,\n",
    "    #'num_workers': 0\n",
    "    # Include other parameters as needed.\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "WjdWMaswQjkV",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "WjdWMaswQjkV",
    "outputId": "13515977-d235-41b3-aaa4-fef32a38c060"
   },
   "outputs": [],
   "source": [
    "# Uncomment this if you want to use Google Drive\n",
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Do2JYBiZZp1z",
   "metadata": {
    "id": "Do2JYBiZZp1z"
   },
   "source": [
    "#load and preprocess data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dPlrZ7Lez1XQ",
   "metadata": {
    "id": "dPlrZ7Lez1XQ"
   },
   "source": [
    "We retain the original SSVEP data preprocessing procedures, including:\n",
    "\n",
    "    Chebyshev bandpass filtering (6–90 Hz),\n",
    "    Noise removal and segmentation using a sliding window of 250 samples, followed by manual channel-wise standardization (subtracting the mean and dividing by the standard deviation),\n",
    "    Downsampling from 1000 Hz to 250 Hz.\n",
    "\n",
    "In this work, we directly load the preprocessed character-level files \"S#_chars.npy\" for subsequent training.\n",
    "\n",
    "Unlike the original setup where models are trained separately for each subject, we train a single model jointly across all subjects by concatenating all subjects' data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "FD4sHQTI0fua",
   "metadata": {
    "id": "FD4sHQTI0fua"
   },
   "outputs": [],
   "source": [
    "# Define a function to perform z-score normalization on the data\n",
    "def zscore_norm(data):\n",
    "    # Calculate the mean and standard deviation for each channel in each batch\n",
    "    mean = torch.mean(data, dim=(1, 2))\n",
    "    std = torch.std(data, dim=(1, 2))\n",
    "\n",
    "    # Subtract the mean from each channel in each batch and divide by the standard deviation\n",
    "    norm_data = (data - mean[:, None, None]) / std[:, None, None]\n",
    "\n",
    "    return norm_data\n",
    "\n",
    "\n",
    "# Define a function to perform min-max normalization on the data\n",
    "def minmax_norm(data):\n",
    "    # Calculate the minimum and maximum values for each channel and sequence in the batch\n",
    "    min_vals = torch.min(data, dim=-1)[0]\n",
    "    max_vals = torch.max(data, dim=-1)[0]\n",
    "\n",
    "    # Scale the data to the range [0, 1]\n",
    "    norm_data = (data - min_vals.unsqueeze(-1)) / (\n",
    "        max_vals.unsqueeze(-1) - min_vals.unsqueeze(-1)\n",
    "    )\n",
    "\n",
    "    return norm_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "kpv9Xa910cDi",
   "metadata": {
    "id": "kpv9Xa910cDi"
   },
   "outputs": [],
   "source": [
    "'''class EEGDataset(Dataset):\n",
    "    \"Characterizes a dataset for PyTorch\"\n",
    "\n",
    "    def __init__(self, X, Y, transform=None):\n",
    "        \"Initialization\"\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        \"Denotes the total number of samples\"\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"Generates one sample of data\"\n",
    "        # Load data and get label\n",
    "        x = self.X[index]\n",
    "        y = self.Y[index]\n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "        return x.squeeze(), y'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7Z4LtRwCGwmm",
   "metadata": {
    "id": "7Z4LtRwCGwmm"
   },
   "outputs": [],
   "source": [
    "'''#define spectrogram\n",
    "#time_frames = 1 + (signal_length - n_fft) // hop_length\n",
    "#1 + (250 - 64) // 16 = 12 time frame\n",
    "spectrogram_transform = T.Spectrogram(\n",
    "    n_fft=64,       # width of window. original: 128, but then the time_frame will become 0 when downsampling\n",
    "    win_length=None, # default: n_fft\n",
    "    hop_length=16, # default: win_length // 2\n",
    "    normalized=True\n",
    ")'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bpIIK-nqQEXL",
   "metadata": {
    "id": "bpIIK-nqQEXL"
   },
   "outputs": [],
   "source": [
    "'''def load_data_by_session(root_dir, subject_id, session_idx_list):\n",
    "    data = np.load(os.path.join(root_dir, f\"S{subject_id}_chars.npy\"))  # [26, 6, 64, 250]\n",
    "    data = data[:, session_idx_list]  # [26, len(session), 64, 250]\n",
    "    X = data.reshape(-1, 64, 250)  # [samples, 64, 250]\n",
    "    Y = np.repeat(np.arange(26), len(session_idx_list))\n",
    "\n",
    "    # transform to spectrogram\n",
    "    X_spectrogram = []\n",
    "    for sample in X:  # sample: [64, 250]\n",
    "        spec_per_channel = []\n",
    "        for ch in sample:\n",
    "            ch_tensor = torch.tensor(ch, dtype=torch.float32)\n",
    "            spec = spectrogram_transform(ch_tensor)  # [freq, time]\n",
    "            spec_per_channel.append(spec)\n",
    "        spec_tensor = torch.stack(spec_per_channel)  # [64, freq, time]\n",
    "        X_spectrogram.append(spec_tensor)\n",
    "\n",
    "    X_spectrogram = torch.stack(X_spectrogram)  # [samples, 64, freq, time]\n",
    "    Y = torch.tensor(Y, dtype=torch.long)\n",
    "\n",
    "    print(\"X shape after Spectrogram:\", X_spectrogram.shape)  # [B, 64, freq≈33, time≈12]\n",
    "\n",
    "    return X_spectrogram, Y\n",
    "\n",
    "\n",
    "def load_split_dataset(root_dir, num_seen=25, seed=43, cache_dir='processed_by_session'):\n",
    "    os.makedirs(cache_dir, exist_ok=True)\n",
    "    random.seed(seed)\n",
    "\n",
    "    all_subjects = list(range(1, 36))\n",
    "    seen_subjects = random.sample(all_subjects, num_seen)#25 seen\n",
    "    unseen_subjects = [sid for sid in all_subjects if sid not in seen_subjects]#10 unseen\n",
    "\n",
    "    split_cfg = {\n",
    "        \"train\":  [(sid, [0, 1, 2, 3]) for sid in seen_subjects],\n",
    "        \"val\":    [(sid, [4]) for sid in seen_subjects],\n",
    "        \"test1\":  [(sid, [5]) for sid in seen_subjects],\n",
    "        \"test2\":  [(sid, [0, 1, 2, 3, 4, 5]) for sid in unseen_subjects]\n",
    "    }\n",
    "\n",
    "    print(f\"[Split] Seen subjects (train/val/test1): {seen_subjects}\")\n",
    "    print(f\"[Split] Unseen subjects (test2): {unseen_subjects}\")\n",
    "\n",
    "    loaders = {}\n",
    "    for split, sid_sess in split_cfg.items():\n",
    "        X_all, Y_all = [], []\n",
    "        for sid, sess in sid_sess:\n",
    "            sess_str = ''.join(str(s) for s in sess)\n",
    "            \n",
    "            cache_X_path = os.path.join(cache_dir, f\"S{sid:02d}_sess_{sess_str}_X.npy\")\n",
    "            cache_Y_path = os.path.join(cache_dir, f\"S{sid:02d}_sess_{sess_str}_Y.npy\")\n",
    "\n",
    "            if os.path.exists(cache_X_path) and os.path.exists(cache_Y_path):\n",
    "                X = torch.tensor(np.load(cache_X_path))\n",
    "                Y = torch.tensor(np.load(cache_Y_path))\n",
    "                print(f\"[Cache] Loaded S{sid:02d} sessions {sess_str} from cache\")\n",
    "            else:\n",
    "                X, Y = load_data_by_session(root_dir, sid, sess)\n",
    "                np.save(cache_X_path, X.cpu().numpy())\n",
    "                np.save(cache_Y_path, Y.cpu().numpy())\n",
    "                print(f\"[Cache] Saved S{sid:02d} sessions {sess_str} to cache\")\n",
    "                \n",
    "            X_all.append(X)\n",
    "            Y_all.append(Y)\n",
    "        X_all = torch.cat(X_all, dim=0)\n",
    "        Y_all = torch.cat(Y_all, dim=0)\n",
    "        dataset = EEGDataset(X_all, Y_all)\n",
    "        loaders[split] = DataLoader(dataset, batch_size=32, shuffle=(split == \"train\"))\n",
    "    return loaders\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "R1jZy_mPsxNI",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R1jZy_mPsxNI",
    "outputId": "7f6b04f7-34a4-481b-e459-92f8db552739"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Seed:  44\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "seed = 44\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "print(\"Random Seed: \", seed)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "seWeK5dArjjM",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "seWeK5dArjjM",
    "outputId": "cabe5a3f-7cd8-440a-a870-5fa634af8a8d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'subject_chars_dir = r\\'C:\\\\LTI 11785 Introduction to deep learning\\\\project\\\\SSVEP\\\\chars\\' # Where to store all character data\\nsubject_ids = list(range(1, 36)) #35 samples + 1\\n\\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\\nbatch_size = 256\\nbatch_size2 = 256\\nseed = 44\\nshuffle = True\\n\\n# load all subjects\\' data\\nloaders = load_split_dataset(subject_chars_dir, num_seen=20, seed=seed)\\ntrain_loader = loaders[\"train\"]\\nval_loader   = loaders[\"val\"]\\ntest1_loader = loaders[\"test1\"]\\ntest2_loader = loaders[\"test2\"]\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#subject_chars_dir = '/content/drive/MyDrive/project/dataset/ssvep/chars' # Where to store all character data\n",
    "'''subject_chars_dir = r'C:\\LTI 11785 Introduction to deep learning\\project\\SSVEP\\chars' # Where to store all character data\n",
    "subject_ids = list(range(1, 36)) #35 samples + 1\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "batch_size = 256\n",
    "batch_size2 = 256\n",
    "seed = 44\n",
    "shuffle = True\n",
    "\n",
    "# load all subjects' data\n",
    "loaders = load_split_dataset(subject_chars_dir, num_seen=20, seed=seed)\n",
    "train_loader = loaders[\"train\"]\n",
    "val_loader   = loaders[\"val\"]\n",
    "test1_loader = loaders[\"test1\"]\n",
    "test2_loader = loaders[\"test2\"]\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e0397229-d1a4-44c6-99f8-66919abfb2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EEGDataset(Dataset):\n",
    "    def __init__(self, features, labels, augment=False):\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "        self.augment = augment\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.features[idx]\n",
    "        y = self.labels[idx]\n",
    "\n",
    "        if self.augment:\n",
    "            # apply your augmentation logic here\n",
    "            x = self.apply_augmentation(x)\n",
    "\n",
    "        return x, y\n",
    "\n",
    "    def apply_augmentation(self, x):\n",
    "        # define any augmentation wanted\n",
    "        return x\n",
    "\n",
    "\n",
    "def get_dataloader(file_path_X, file_path_y, batch_size=64, shuffle=False, augment=False):\n",
    "    X = np.load(file_path_X)\n",
    "    y = np.load(file_path_y)\n",
    "    X_T = torch.tensor(X, dtype=torch.float32)\n",
    "    y_T = torch.tensor(y, dtype=torch.float32)  # y is already 1-hot, long in case use cross-entropy\n",
    "    \n",
    "    dataset = EEGDataset(X_T, y_T, augment=augment)  # passed the tensors\n",
    "    \n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "\n",
    "train_loader = get_dataloader('processed_data/X_train.npy', \n",
    "                              'processed_data/y_train.npy', \n",
    "                              batch_size=64, shuffle=True)\n",
    "val_loader   = get_dataloader('processed_data/X_val.npy', \n",
    "                              'processed_data/y_val.npy', \n",
    "                              batch_size=128, shuffle=True)\n",
    "test_loader  = get_dataloader('processed_data/X_test1.npy', \n",
    "                              'processed_data/y_test1.npy', \n",
    "                              batch_size=128, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f0JmYf8_7foW",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f0JmYf8_7foW",
    "outputId": "451a29b4-453b-4649-e095-e829c058ff1b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4800 1200 1200\n",
      "X shape: torch.Size([64, 64, 250])\n",
      "Y shape: torch.Size([64, 40])\n"
     ]
    }
   ],
   "source": [
    "#print(len(train_loader.dataset), len(val_loader.dataset), len(test1_loader.dataset), len(test2_loader.dataset))\n",
    "print(len(train_loader.dataset), len(val_loader.dataset), len(test_loader.dataset))\n",
    "for x, y in train_loader:\n",
    "    print(\"X shape:\", x.shape)\n",
    "    print(\"Y shape:\", y.shape)\n",
    "    break  # the first batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "zuXPlL3nol6U",
   "metadata": {
    "id": "zuXPlL3nol6U"
   },
   "source": [
    "train: 20 seen subjects × 4 session × 26 = 2080\n",
    "\n",
    "val: 20 × 1 × 26 = 520\n",
    "\n",
    "test1: 20 × 1 × 26 = 520\n",
    "\n",
    "test2: 15 unseen × 6 session × 26 = 2340"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tEjThr9xpong",
   "metadata": {
    "id": "tEjThr9xpong"
   },
   "source": [
    "random choose 25 subjects as seen, the rest 10 as unseen\n",
    "\n",
    "| Group Name | Subject Range   | Sessions Used         | Purpose                                  |\n",
    "|------------|------------------|------------------------|-------------------------------------------|\n",
    "| train      | seen subjects    | sessions [0, 1, 2, 3]  | Training (4 sessions)                     |\n",
    "| val        | seen subjects    | session [4]            | Validation                                |\n",
    "| test1      | seen subjects    | session [5]            | Test on unseen session of seen subjects  |\n",
    "| test2      | unseen subjects  | sessions [0–5] (all)   | Test on unseen subjects                   |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "D-8XtejnbSv2",
   "metadata": {
    "id": "D-8XtejnbSv2"
   },
   "source": [
    "#model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mV-wGFCs2hpy",
   "metadata": {
    "id": "mV-wGFCs2hpy"
   },
   "source": [
    "The overall model follows the original GitHub implementation, with a few necessary adjustments to align tensor time dimensions. Specifically, we aligned the temporal dimensions between up2 + temb and down1, up3 and x, and x_hat and x. These changes are clearly marked with comments in the code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "M67sftFNJGiL",
   "metadata": {
    "id": "M67sftFNJGiL"
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "```\n",
    "#transform from 1D to 2D:\n",
    "current input: [batch_size, channels, freq_bins, time_frames]\n",
    "original input: [batch_size, channels, time]\n",
    "\n",
    "Replace all instances of Conv1d, MaxPool1d, and AvgPool1d with Conv2d, MaxPool2d, and AvgPool2d, respectively.\n",
    "Also modify the AvgPool1d and MaxPool1d used in the Encoder and Decoder.\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "iIaSnHewQDNY",
   "metadata": {
    "id": "iIaSnHewQDNY"
   },
   "outputs": [],
   "source": [
    "def safe_cat_and_add(a, b, temb):\n",
    "    \"\"\"\n",
    "    Interpolates `a` to align with `b`, then adds `temb` (broadcasted) to `a`, and finally concatenates with `b`.\n",
    "\n",
    "    Inputs:\n",
    "        a: decoder upsampled output [B, C, F, T]\n",
    "        b: encoder skip connection [B, C, F, T]\n",
    "        temb: time embedding output, supports [B, C], [B, C, 1], etc., auto-adapts shape\n",
    "\n",
    "    Output:\n",
    "        Concatenated result [B, C_concat, F, T]\n",
    "    \"\"\"\n",
    "    # Align spatial dimensions (freq, time)\n",
    "    if a.shape[2:] != b.shape[2:]:\n",
    "        a = F.interpolate(a, size=b.shape[2:], mode='nearest')\n",
    "\n",
    "    # Normalize temb to shape [B, C, 1, 1]\n",
    "    while temb.dim() < 4:\n",
    "        temb = temb.unsqueeze(-1)\n",
    "    if temb.dim() > 4:\n",
    "        temb = temb.squeeze(-1)\n",
    "\n",
    "    # Broadcast to shape [B, C, F, T]\n",
    "    temb = temb.expand(-1, -1, a.shape[2], a.shape[3])\n",
    "\n",
    "    # Element-wise addition and concatenation\n",
    "    return torch.cat([a + temb, b], dim=1)\n",
    "\n",
    "def safe_align_2d(x1, x2):\n",
    "    \"\"\"\n",
    "    Align two 4D tensors in frequency and time dimensions (2D spatial alignment).\n",
    "\n",
    "    Inputs:\n",
    "        x1, x2: [B, C, F, T]\n",
    "\n",
    "    Returns:\n",
    "        Aligned x1, x2\n",
    "    \"\"\"\n",
    "    target_freq = min(x1.shape[2], x2.shape[2])\n",
    "    target_time = min(x1.shape[3], x2.shape[3])\n",
    "    x1 = F.interpolate(x1, size=(target_freq, target_time), mode='nearest')\n",
    "    x2 = F.interpolate(x2, size=(target_freq, target_time), mode='nearest')\n",
    "    return x1, x2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ceb479ce",
   "metadata": {
    "id": "ceb479ce"
   },
   "outputs": [],
   "source": [
    "#define diffE model\n",
    "def get_padding(kernel_size, dilation=1):\n",
    "    return int((kernel_size * dilation - dilation) / 2)\n",
    "\n",
    "\n",
    "# Swish activation function\n",
    "class Swish(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x * self.sigmoid(x)\n",
    "\n",
    "\n",
    "class SinusoidalPosEmb(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        device = x.device\n",
    "        half_dim = self.dim // 2\n",
    "        emb = math.log(10000) / (half_dim - 1)\n",
    "        emb = torch.exp(torch.arange(half_dim, device=device) * -emb)\n",
    "        emb = x[:, None] * emb[None, :]\n",
    "        emb = torch.cat((emb.sin(), emb.cos()), dim=-1)\n",
    "        return emb\n",
    "\n",
    "\n",
    "class WeightStandardizedConv2d(nn.Conv2d):\n",
    "    def forward(self, x):\n",
    "        eps = 1e-5 if x.dtype == torch.float32 else 1e-3\n",
    "        weight = self.weight\n",
    "        mean = weight.mean(dim=(1,2,3), keepdim=True)\n",
    "        var = weight.var(dim=(1,2,3), keepdim=True, unbiased=False)\n",
    "        weight = (weight - mean) / (var + eps).sqrt()\n",
    "        return F.conv2d(x, weight, self.bias, self.stride, self.padding, self.dilation, self.groups)\n",
    "\n",
    "\n",
    "class ResidualConvBlock(nn.Module):\n",
    "    def __init__(self, inc, outc, kernel_size, stride=1, gn=8):\n",
    "        super().__init__()\n",
    "        self.same_channels = inc == outc\n",
    "        padding = kernel_size // 2\n",
    "        self.conv = nn.Sequential(\n",
    "            WeightStandardizedConv2d(inc, outc, kernel_size, stride, padding),\n",
    "            nn.GroupNorm(gn, outc),\n",
    "            nn.PReLU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.conv(x)\n",
    "        return (x + x1) / 2 if self.same_channels else x1\n",
    "\n",
    "\n",
    "class UnetDown(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, gn=8, factor=2):\n",
    "        super().__init__()\n",
    "        self.pool = nn.MaxPool2d(kernel_size=factor)\n",
    "        self.layer = ResidualConvBlock(in_channels, out_channels, kernel_size, gn=gn)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer(x)\n",
    "        return self.pool(x)\n",
    "\n",
    "\n",
    "class UnetUp(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, gn=8, factor=2):\n",
    "        super().__init__()\n",
    "        self.upsample = nn.Upsample(scale_factor=factor, mode='nearest')\n",
    "        self.layer = ResidualConvBlock(in_channels, out_channels, kernel_size, gn=gn)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layer(self.upsample(x))\n",
    "\n",
    "\n",
    "class EmbedFC(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim):\n",
    "        super(EmbedFC, self).__init__()\n",
    "        \"\"\"\n",
    "        generic one layer FC NN for embedding things\n",
    "        \"\"\"\n",
    "        self.input_dim = input_dim\n",
    "        layers = [\n",
    "            nn.Linear(input_dim, emb_dim),\n",
    "            nn.PReLU(),\n",
    "            nn.Linear(emb_dim, emb_dim),\n",
    "        ]\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, self.input_dim)\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "class ConditionalUNet(nn.Module):\n",
    "    def __init__(self, in_channels, n_feat=256):\n",
    "        super(ConditionalUNet, self).__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.n_feat = n_feat\n",
    "\n",
    "        self.d1_out = n_feat * 1\n",
    "        self.d2_out = n_feat * 2\n",
    "        self.d3_out = n_feat * 3\n",
    "        self.d4_out = n_feat * 4\n",
    "\n",
    "        self.u1_out = n_feat\n",
    "        self.u2_out = n_feat\n",
    "        self.u3_out = n_feat\n",
    "        self.u4_out = in_channels\n",
    "\n",
    "        self.sin_emb = SinusoidalPosEmb(n_feat)\n",
    "        # self.timeembed1 = EmbedFC(n_feat, self.u1_out)\n",
    "        # self.timeembed2 = EmbedFC(n_feat, self.u2_out)\n",
    "        # self.timeembed3 = EmbedFC(n_feat, self.u3_out)\n",
    "\n",
    "        self.down1 = UnetDown(in_channels, self.d1_out, 1, gn=8, factor=2)\n",
    "        self.down2 = UnetDown(self.d1_out, self.d2_out, 1, gn=8, factor=2)\n",
    "        self.down3 = UnetDown(self.d2_out, self.d3_out, 1, gn=8, factor=2)\n",
    "\n",
    "        self.up2 = UnetUp(self.d3_out, self.u2_out, 1, gn=8, factor=2)\n",
    "        self.up3 = UnetUp(self.u2_out + self.d2_out, self.u3_out, 1, gn=8, factor=2)\n",
    "        self.up4 = UnetUp(self.u3_out + self.d1_out, self.u4_out, 1, gn=8, factor=2)\n",
    "        self.out = nn.Conv2d(self.u4_out + in_channels, in_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        down1 = self.down1(x)  # 2000 -> 1000\n",
    "        down2 = self.down2(down1)  # 1000 -> 500\n",
    "        down3 = self.down3(down2)  # 500 -> 250\n",
    "\n",
    "        temb = self.sin_emb(t).view(-1, self.n_feat, 1)  # [b, n_feat, 1]\n",
    "\n",
    "        up1 = self.up2(down3)  # 250 -> 500\n",
    "        #up2 = self.up3(torch.cat([up1 + temb, down2], 1))  # 500 -> 1000\n",
    "        up2 = self.up3(safe_cat_and_add(up1, down2, temb))\n",
    "\n",
    "        #up3 = self.up4(torch.cat([up2 + temb, down1], 1))  # 1000 -> 2000\n",
    "        up3 = self.up4(safe_cat_and_add(up2, down1, temb))\n",
    "\n",
    "        # Align the temporal dimension of up3 and x\n",
    "        if up3.shape[-1] != x.shape[-1]:\n",
    "            target_len = min(up3.shape[-1], x.shape[-1])\n",
    "            up3 = F.interpolate(up3, size=target_len)\n",
    "            x = F.interpolate(x, size=target_len)\n",
    "\n",
    "        #out = self.out(torch.cat([up3, x], 1))  # 2000 -> 2000\n",
    "        if x.shape[2:] != up3.shape[2:]:\n",
    "            x = F.interpolate(x, size=up3.shape[2:], mode='nearest')\n",
    "\n",
    "        out = self.out(torch.cat([up3, x], 1))\n",
    "\n",
    "        down = (down1, down2, down3)\n",
    "        up = (up1, up2, up3)\n",
    "        return out, down, up\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, in_channels, dim=512):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.e1_out = dim\n",
    "        self.e2_out = dim\n",
    "        self.e3_out = dim\n",
    "\n",
    "        self.down1 = UnetDown(in_channels, self.e1_out, 1, gn=8, factor=2)\n",
    "        self.down2 = UnetDown(self.e1_out, self.e2_out, 1, gn=8, factor=2)\n",
    "        self.down3 = UnetDown(self.e2_out, self.e3_out, 1, gn=8, factor=2)\n",
    "\n",
    "        self.avg_pooling = nn.AdaptiveAvgPool2d((1,1))\n",
    "        self.max_pooling = nn.AdaptiveMaxPool2d((1,1))\n",
    "        self.act = nn.Tanh()\n",
    "\n",
    "    def forward(self, x0):\n",
    "        # Down sampling\n",
    "        dn1 = self.down1(x0)  # 2048 -> 1024\n",
    "        dn2 = self.down2(dn1)  # 1024 -> 512\n",
    "        dn3 = self.down3(dn2)  # 512 -> 256\n",
    "        z = self.avg_pooling(dn3).view(-1, self.e3_out)  # [b, features]\n",
    "        down = (dn1, dn2, dn3)\n",
    "        out = (down, z)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, in_channels, n_feat=256, encoder_dim=512, n_classes=config['num_classes']):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.n_feat = n_feat\n",
    "        self.n_classes = n_classes\n",
    "        self.e1_out = encoder_dim\n",
    "        self.e2_out = encoder_dim\n",
    "        self.e3_out = encoder_dim\n",
    "        self.d1_out = n_feat\n",
    "        self.d2_out = n_feat * 2\n",
    "        self.d3_out = n_feat * 3\n",
    "        self.u1_out = n_feat\n",
    "        self.u2_out = n_feat\n",
    "        self.u3_out = n_feat\n",
    "        self.u4_out = in_channels\n",
    "\n",
    "        # self.sin_emb = SinusoidalPosEmb(n_feat)\n",
    "        # self.timeembed1 = EmbedFC(n_feat, self.e3_out)\n",
    "        # self.timeembed2 = EmbedFC(n_feat, self.u2_out)\n",
    "        # self.timeembed3 = EmbedFC(n_feat, self.u3_out)\n",
    "        # self.contextembed1 = EmbedFC(self.e3_out, self.e3_out)\n",
    "        # self.contextembed2 = EmbedFC(self.e3_out, self.u2_out)\n",
    "        # self.contextembed3 = EmbedFC(self.e3_out, self.u3_out)\n",
    "\n",
    "        # Unet up sampling\n",
    "        self.up1 = UnetUp(self.d3_out + self.e3_out, self.u2_out, 1, gn=8, factor=2)\n",
    "        self.up2 = UnetUp(self.d2_out + self.u2_out, self.u3_out, 1, gn=8, factor=2)\n",
    "        self.up3 = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2, mode=\"nearest\"),\n",
    "            nn.Conv2d(self.d1_out + self.u3_out + in_channels * 2, in_channels, kernel_size=1),\n",
    "        )\n",
    "\n",
    "        # self.out = nn.Conv1d(self.u4_out+in_channels, in_channels, 1)\n",
    "        self.pool = nn.AvgPool2d(kernel_size=2)\n",
    "\n",
    "    def forward(self, x0, encoder_out, diffusion_out):\n",
    "        # Encoder output\n",
    "        down, z = encoder_out\n",
    "        dn1, dn2, dn3 = down\n",
    "\n",
    "        # DDPM output\n",
    "        x_hat, down_ddpm, up, t = diffusion_out\n",
    "        dn11, dn22, dn33 = down_ddpm\n",
    "\n",
    "        # embed context, time step\n",
    "        # temb = self.sin_emb(t).view(-1, self.n_feat, 1) # [b, n_feat, 1]\n",
    "        # temb1 = self.timeembed1(temb).view(-1, self.e3_out, 1) # [b, features]\n",
    "        # temb2 = self.timeembed2(temb).view(-1, self.u2_out, 1) # [b, features]\n",
    "        # temb3 = self.timeembed3(temb).view(-1, self.u3_out, 1) # [b, features]\n",
    "        # ct2 = self.contextembed2(z).view(-1, self.u2_out, 1) # [b, n_feat, 1]\n",
    "        # ct3 = self.contextembed3(z).view(-1, self.u3_out, 1) # [b, n_feat, 1]\n",
    "\n",
    "        # Up sampling\n",
    "        up1 = self.up1(torch.cat([dn3, dn33.detach()], 1))\n",
    "        up2 = self.up2(torch.cat([up1, dn22.detach()], 1))\n",
    "        out = self.up3(\n",
    "            torch.cat([self.pool(x0), self.pool(x_hat.detach()), up2, dn11.detach()], 1)\n",
    "        )\n",
    "        return out\n",
    "\n",
    "\n",
    "class DiffE(nn.Module):\n",
    "    def __init__(self, encoder, decoder, fc):\n",
    "        super(DiffE, self).__init__()\n",
    "\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.fc = fc\n",
    "\n",
    "    def forward(self, x0, ddpm_out):\n",
    "        encoder_out = self.encoder(x0)\n",
    "        decoder_out = self.decoder(x0, encoder_out, ddpm_out)\n",
    "        fc_out = self.fc(encoder_out[1])\n",
    "        return decoder_out, fc_out\n",
    "\n",
    "\n",
    "class DecoderNoDiff(nn.Module):\n",
    "    def __init__(self, in_channels, n_feat=256, encoder_dim=512, n_classes=config['num_classes']):\n",
    "        super(DecoderNoDiff, self).__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.n_feat = n_feat\n",
    "        self.n_classes = n_classes\n",
    "        self.e1_out = encoder_dim\n",
    "        self.e2_out = encoder_dim\n",
    "        self.e3_out = encoder_dim\n",
    "        self.u1_out = n_feat\n",
    "        self.u2_out = n_feat\n",
    "        self.u3_out = n_feat\n",
    "        self.u4_out = n_feat\n",
    "\n",
    "        self.sin_emb = SinusoidalPosEmb(n_feat)\n",
    "        self.timeembed1 = EmbedFC(n_feat, self.e3_out)\n",
    "        self.timeembed2 = EmbedFC(n_feat, self.u2_out)\n",
    "        self.timeembed3 = EmbedFC(n_feat, self.u3_out)\n",
    "        self.contextembed1 = EmbedFC(self.e3_out, self.e3_out)\n",
    "        self.contextembed2 = EmbedFC(self.e3_out, self.u2_out)\n",
    "        self.contextembed3 = EmbedFC(self.e3_out, self.u3_out)\n",
    "\n",
    "        # Unet up sampling\n",
    "        self.up2 = UnetUp(self.e3_out, self.u2_out, 1, gn=8, factor=2)\n",
    "        self.up3 = UnetUp(self.e2_out + self.u2_out, self.u3_out, 1, gn=8, factor=2)\n",
    "        # self.up4 = UnetUp(self.e1_out+self.u3_out, self.u4_out, 1, 1, gn=in_channels, factor=2, is_res=True)\n",
    "        self.up4 = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=(2, 1), mode=\"nearest\"),  # freq\n",
    "            nn.Conv2d(self.u3_out + self.e1_out + in_channels, in_channels, kernel_size=1)\n",
    "        )\n",
    "\n",
    "        self.out = nn.Conv2d(self.u4_out, in_channels, kernel_size=1)\n",
    "        self.pool = nn.AvgPool2d(kernel_size=(2, 1))  #  freq\n",
    "\n",
    "    def forward(self, x0, x_hat, encoder_out, t):\n",
    "        down, z = encoder_out\n",
    "        dn1, dn2, dn3 = down\n",
    "        tembd = self.sin_emb(t).view(-1, self.n_feat, 1)  # [b, n_feat, 1]\n",
    "        tembd1 = self.timeembed1(self.sin_emb(t)).view(\n",
    "            -1, self.e3_out, 1\n",
    "        )  # [b, n_feat, 1]\n",
    "        tembd2 = self.timeembed2(self.sin_emb(t)).view(\n",
    "            -1, self.u2_out, 1\n",
    "        )  # [b, n_feat, 1]\n",
    "        tembd3 = self.timeembed3(self.sin_emb(t)).view(\n",
    "            -1, self.u3_out, 1\n",
    "        )  # [b, n_feat, 1]\n",
    "\n",
    "        # Up sampling\n",
    "        ddpm_loss = F.l1_loss(x0, x_hat, reduction=\"none\")\n",
    "\n",
    "        up2 = self.up2(dn3)  # 256 -> 512\n",
    "        up3 = self.up3(torch.cat([up2, dn2], 1))  # 512 -> 1024\n",
    "        out = self.up4(\n",
    "            torch.cat([self.pool(x0), self.pool(x_hat), up3, dn1], 1)\n",
    "        )  # 1024 -> 2048\n",
    "        # out = self.out(torch.cat([out, x_hat], 1)) # 2048 -> 2048\n",
    "        # out = self.out(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class LinearClassifier(nn.Module):\n",
    "    def __init__(self, in_dim, latent_dim, emb_dim):\n",
    "        super().__init__()\n",
    "        self.linear_out = nn.Sequential(\n",
    "            nn.Linear(in_features=in_dim, out_features=latent_dim),\n",
    "            nn.GroupNorm(4, latent_dim),\n",
    "            nn.PReLU(),\n",
    "            nn.Linear(in_features=latent_dim, out_features=latent_dim),\n",
    "            nn.GroupNorm(4, latent_dim),\n",
    "            nn.PReLU(),\n",
    "            nn.Linear(in_features=latent_dim, out_features=emb_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear_out(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def cosine_beta_schedule(timesteps, s=0.008):\n",
    "    \"\"\"\n",
    "    cosine schedule\n",
    "    as proposed in https://openreview.net/forum?id=-NEXDKk8gZ\n",
    "    \"\"\"\n",
    "    steps = timesteps + 1\n",
    "    t = torch.linspace(0, timesteps, steps, dtype=torch.float64) / timesteps\n",
    "    alphas_cumprod = torch.cos((t + s) / (1 + s) * math.pi * 0.5) ** 2\n",
    "    alphas_cumprod = alphas_cumprod / alphas_cumprod[0]\n",
    "    betas = 1 - (alphas_cumprod[1:] / alphas_cumprod[:-1])\n",
    "    return torch.clip(betas, 0, 0.999)\n",
    "\n",
    "\n",
    "def sigmoid_beta_schedule(timesteps, start=-3, end=3, tau=1, clamp_min=1e-5):\n",
    "    \"\"\"\n",
    "    sigmoid schedule\n",
    "    proposed in https://arxiv.org/abs/2212.11972 - Figure 8\n",
    "    \"\"\"\n",
    "    steps = timesteps + 1\n",
    "    t = torch.linspace(0, timesteps, steps, dtype=torch.float64) / timesteps\n",
    "    v_start = torch.tensor(start / tau).sigmoid()\n",
    "    v_end = torch.tensor(end / tau).sigmoid()\n",
    "    alphas_cumprod = (-((t * (end - start) + start) / tau).sigmoid() + v_end) / (\n",
    "        v_end - v_start\n",
    "    )\n",
    "    alphas_cumprod = alphas_cumprod / alphas_cumprod[0]\n",
    "    betas = 1 - (alphas_cumprod[1:] / alphas_cumprod[:-1])\n",
    "    return torch.clip(betas, 0, 0.999)\n",
    "\n",
    "\n",
    "def ddpm_schedules(beta1, beta2, T):\n",
    "    # assert beta1 < beta2 < 1.0, \"beta1 and beta2 must be in (0, 1)\"\n",
    "    # beta_t = (beta2 - beta1) * torch.arange(0, T + 1, dtype=torch.float32) / T + beta1\n",
    "    beta_t = cosine_beta_schedule(T, s=0.008).float()\n",
    "    # beta_t = sigmoid_beta_schedule(T).float()\n",
    "\n",
    "    alpha_t = 1 - beta_t\n",
    "\n",
    "    log_alpha_t = torch.log(alpha_t)\n",
    "    alphabar_t = torch.cumsum(log_alpha_t, dim=0).exp()\n",
    "\n",
    "    sqrtab = torch.sqrt(alphabar_t)\n",
    "\n",
    "    sqrtmab = torch.sqrt(1 - alphabar_t)\n",
    "\n",
    "    return {\n",
    "        \"sqrtab\": sqrtab,  # \\sqrt{\\bar{\\alpha_t}}\n",
    "        \"sqrtmab\": sqrtmab,  # \\sqrt{1-\\bar{\\alpha_t}}\n",
    "    }\n",
    "\n",
    "\n",
    "class DDPM(nn.Module):\n",
    "    def __init__(self, nn_model, betas, n_T, device):\n",
    "        super(DDPM, self).__init__()\n",
    "        self.nn_model = nn_model.to(device)\n",
    "\n",
    "        for k, v in ddpm_schedules(betas[0], betas[1], n_T).items():\n",
    "            self.register_buffer(k, v)\n",
    "\n",
    "        self.n_T = n_T\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, x):\n",
    "        _ts = torch.randint(1, self.n_T, (x.shape[0],)).to(\n",
    "            self.device\n",
    "        )  # t ~ Uniform(0, n_T)\n",
    "        noise = torch.randn_like(x)  # eps ~ N(0, 1)\n",
    "        #x_t = self.sqrtab[_ts, None, None] * x + self.sqrtmab[_ts, None, None] * noise\n",
    "        x_t = self.sqrtab[_ts].view(-1, 1, 1, 1) * x + self.sqrtmab[_ts].view(-1, 1, 1, 1) * noise\n",
    "        times = _ts / self.n_T\n",
    "        output, down, up = self.nn_model(x_t, times)\n",
    "        return output, down, up, noise, times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "T4i5FvbBauXV",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T4i5FvbBauXV",
    "outputId": "e81a162a-3775-4a63-b0fd-53a02611b01c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample input shape: torch.Size([64, 64, 250])  → Channels: 64, Timepoints: 250\n"
     ]
    }
   ],
   "source": [
    "for x, y in train_loader:\n",
    "    channels = x.shape[1]\n",
    "    timepoints = x.shape[2]\n",
    "    print(f\"Sample input shape: {x.shape}  → Channels: {channels}, Timepoints: {timepoints}\")\n",
    "    break\n",
    "\n",
    "num_classes = config['num_classes']\n",
    "ddpm_dim = config['ddpm_dim']\n",
    "encoder_dim = config['encoder_dim']\n",
    "fc_dim = config['fc_dim']\n",
    "n_T = config['n_T']\n",
    "\n",
    "ddpm_model = ConditionalUNet(in_channels=channels, n_feat=ddpm_dim).to(device)\n",
    "ddpm = DDPM(nn_model=ddpm_model, betas=(1e-6, 1e-2), n_T=n_T, device=device).to(device)\n",
    "\n",
    "encoder = Encoder(in_channels=channels, dim=encoder_dim).to(device)\n",
    "decoder = Decoder(in_channels=channels, n_feat=ddpm_dim, encoder_dim=encoder_dim).to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "42acd14a-7754-4fba-ba6a-8f7a4f609e17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Model initialized with:\n",
      " - Input channels       : 64\n",
      " - Timepoints           : 250\n",
      " - ddpm total params    : 265478\n",
      " - encoder total params : 149763\n",
      " - decoder total params : 156482\n",
      " - classifier params    : 409628\n",
      " - total DiffE params   : 715873\n"
     ]
    }
   ],
   "source": [
    "fc = LinearClassifier(encoder_dim, fc_dim, emb_dim=num_classes).to(device)\n",
    "\n",
    "diffe = DiffE(encoder, decoder, fc).to(device)\n",
    "\n",
    "print(\" Model initialized with:\")\n",
    "print(\" - Input channels       :\", channels)\n",
    "print(\" - Timepoints           :\", timepoints)\n",
    "print(\" - ddpm total params    :\", sum(p.numel() for p in ddpm.parameters()))\n",
    "print(\" - encoder total params :\", sum(p.numel() for p in encoder.parameters()))\n",
    "print(\" - decoder total params :\", sum(p.numel() for p in decoder.parameters()))\n",
    "print(\" - classifier params    :\", sum(p.numel() for p in fc.parameters()))\n",
    "print(\" - total DiffE params   :\", sum(p.numel() for p in diffe.parameters()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gNFmH1U1cUfr",
   "metadata": {
    "id": "gNFmH1U1cUfr"
   },
   "source": [
    "#train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e6b9kOFsmNm2",
   "metadata": {
    "id": "e6b9kOFsmNm2"
   },
   "outputs": [],
   "source": [
    "# Criterion\n",
    "criterion = nn.L1Loss()\n",
    "criterion_class = nn.MSELoss()\n",
    "\n",
    "# Define optimizer\n",
    "base_lr, lr = config['AE_base_lr'], config['AE_max_lr']\n",
    "optim1 = optim.RMSprop(ddpm.parameters(), lr=base_lr)\n",
    "optim2 = optim.RMSprop(diffe.parameters(), lr=base_lr)\n",
    "\n",
    "# EMAs\n",
    "fc_ema = EMA(diffe.fc, beta=0.95, update_after_step=100, update_every=10,)\n",
    "\n",
    "step_size = 150\n",
    "scheduler1 = optim.lr_scheduler.CyclicLR(\n",
    "    optimizer=optim1,\n",
    "    base_lr=base_lr,\n",
    "    max_lr=lr,\n",
    "    step_size_up=step_size,\n",
    "    mode=\"exp_range\",\n",
    "    cycle_momentum=False,\n",
    "    gamma=0.9998,\n",
    ")\n",
    "scheduler2 = optim.lr_scheduler.CyclicLR(\n",
    "    optimizer=optim2,\n",
    "    base_lr=base_lr,\n",
    "    max_lr=lr,\n",
    "    step_size_up=step_size,\n",
    "    mode=\"exp_range\",\n",
    "    cycle_momentum=False,\n",
    "    gamma=0.9998,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "DEi7SkZfvNbe",
   "metadata": {
    "id": "DEi7SkZfvNbe"
   },
   "outputs": [],
   "source": [
    "# Evaluate function\n",
    "def evaluate(encoder, fc, generator, device):\n",
    "    labels = np.arange(0, 26)\n",
    "    Y = []\n",
    "    Y_hat = []\n",
    "    for x, y in generator:\n",
    "        x, y = x.to(device), y.type(torch.LongTensor).to(device)\n",
    "        encoder_out = encoder(x)\n",
    "        y_hat = fc(encoder_out[1])\n",
    "        y_hat = F.softmax(y_hat, dim=1)\n",
    "\n",
    "        Y.append(y.detach().cpu())\n",
    "        Y_hat.append(y_hat.detach().cpu())\n",
    "\n",
    "    # List of tensors to tensor to numpy\n",
    "    Y = torch.cat(Y, dim=0).numpy()  # (N, )\n",
    "    Y_hat = torch.cat(Y_hat, dim=0).numpy()  # (N, 13): has to sum to 1 for each row\n",
    "\n",
    "    # Accuracy and Confusion Matrix\n",
    "    accuracy = top_k_accuracy_score(Y, Y_hat, k=1, labels=labels)\n",
    "    f1 = f1_score(Y, Y_hat.argmax(axis=1), average=\"macro\", labels=labels)\n",
    "    recall = recall_score(Y, Y_hat.argmax(axis=1), average=\"macro\", labels=labels)\n",
    "    precision = precision_score(Y, Y_hat.argmax(axis=1), average=\"macro\", labels=labels, zero_division=0)\n",
    "    auc = roc_auc_score(Y, Y_hat, average=\"macro\", multi_class=\"ovo\", labels=labels)\n",
    "\n",
    "    metrics = {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"f1\": f1,\n",
    "        \"recall\": recall,\n",
    "        \"precision\": precision,\n",
    "        \"auc\": auc,\n",
    "    }\n",
    "    # df_cm = pd.DataFrame(confusion_matrix(Y, Y_hat.argmax(axis=1)))\n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "635ff91f-7ac3-44aa-8054-d6f9d0e038d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_2\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_2\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">250</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)          │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">250</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)         │           <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ batch_normalization_6                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">250</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)         │              <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ depthwise_conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">250</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │           <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ batch_normalization_7                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">250</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ activation_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">250</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ average_pooling2d_4                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">62</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)           │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">AveragePooling2D</span>)                   │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">62</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)           │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ separable_conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SeparableConv2D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">62</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)           │           <span style=\"color: #00af00; text-decoration-color: #00af00\">1,536</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ batch_normalization_8                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">62</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ activation_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">62</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)           │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ average_pooling2d_5                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)            │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">AveragePooling2D</span>)                   │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)            │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>)                  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">9,000</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ softmax (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_2 (\u001b[38;5;33mInputLayer\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m250\u001b[0m, \u001b[38;5;34m1\u001b[0m)          │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m250\u001b[0m, \u001b[38;5;34m16\u001b[0m)         │           \u001b[38;5;34m1,024\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ batch_normalization_6                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m250\u001b[0m, \u001b[38;5;34m16\u001b[0m)         │              \u001b[38;5;34m64\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ depthwise_conv2d_2 (\u001b[38;5;33mDepthwiseConv2D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m250\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │           \u001b[38;5;34m2,048\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ batch_normalization_7                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m250\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │             \u001b[38;5;34m128\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ activation_4 (\u001b[38;5;33mActivation\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m250\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ average_pooling2d_4                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m62\u001b[0m, \u001b[38;5;34m32\u001b[0m)           │               \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mAveragePooling2D\u001b[0m)                   │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_4 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m62\u001b[0m, \u001b[38;5;34m32\u001b[0m)           │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ separable_conv2d_2 (\u001b[38;5;33mSeparableConv2D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m62\u001b[0m, \u001b[38;5;34m32\u001b[0m)           │           \u001b[38;5;34m1,536\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ batch_normalization_8                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m62\u001b[0m, \u001b[38;5;34m32\u001b[0m)           │             \u001b[38;5;34m128\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ activation_5 (\u001b[38;5;33mActivation\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m62\u001b[0m, \u001b[38;5;34m32\u001b[0m)           │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ average_pooling2d_5                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m32\u001b[0m)            │               \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mAveragePooling2D\u001b[0m)                   │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_5 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m32\u001b[0m)            │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m40\u001b[0m)                  │           \u001b[38;5;34m9,000\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ softmax (\u001b[38;5;33mActivation\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m40\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">13,928</span> (54.41 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m13,928\u001b[0m (54.41 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">13,768</span> (53.78 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m13,768\u001b[0m (53.78 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">160</span> (640.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m160\u001b[0m (640.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "eegnet_model = EEGNet(nb_classes=config['num_classes'], \n",
    "                      Chans=config['eeg_channels'], \n",
    "                      Samples=config['eeg_samples'], \n",
    "                      dropoutRate=config['eeg_droupout'], \n",
    "                      kernLength=config['eeg_kernel_length'], \n",
    "                      F1=config['eeg_F1'], \n",
    "                      D=config['eeg_D'], \n",
    "                      F2=config['eeg_F2'], \n",
    "                      dropoutType='Dropout')\n",
    "\n",
    "eegnet_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "eegnet_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ec9a3b47-5c70-4e89-94d8-a1155fa3541f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4800, 64, 250)\n",
      "(4800, 40)\n"
     ]
    }
   ],
   "source": [
    "def x_transform(x):\n",
    "    return x.reshape(x.shape[0], 64, -1)\n",
    "\n",
    "\n",
    "def dataloader_to_numpy(dataloader):\n",
    "    X_all, y_all = [], []\n",
    "    for X_batch, y_batch in dataloader:\n",
    "        X_all.append(X_batch.numpy())\n",
    "        y_all.append(y_batch.numpy())\n",
    "    X_all = np.concatenate(X_all, axis=0)\n",
    "    y_all = np.concatenate(y_all, axis=0)\n",
    "    #X_eegnet = x_transform(X_all)\n",
    "    return X_all, y_all\n",
    "\n",
    "\n",
    "X_train_np,  y_train_np  = dataloader_to_numpy(train_loader)\n",
    "X_val_np,    y_val_np    = dataloader_to_numpy(val_loader)\n",
    "X_test_np,  y_test_np    = dataloader_to_numpy(test_loader)\n",
    "#X_test2_np,  y_test2_np  = dataloader_to_numpy(test2_loader)\n",
    "\n",
    "print(X_train_np.shape)\n",
    "print(y_train_np.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "669995da-058b-4428-a873-bb8c3721c948",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Initiate Wandb.'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Initiate Wandb.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "410fb9bd-de77-46e3-a781-75ed17251e7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Calling wandb.login() after wandb.init() has no effect.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Ben_project_base_line_EEGNet_with_raw_data</strong> at: <a href='https://wandb.ai/yucheng_shao-carnegie-mellon-university/project_ssvep-ablations/runs/7x4dry8l' target=\"_blank\">https://wandb.ai/yucheng_shao-carnegie-mellon-university/project_ssvep-ablations/runs/7x4dry8l</a><br> View project at: <a href='https://wandb.ai/yucheng_shao-carnegie-mellon-university/project_ssvep-ablations' target=\"_blank\">https://wandb.ai/yucheng_shao-carnegie-mellon-university/project_ssvep-ablations</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250418_135259-7x4dry8l\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\LTI 11785 Introduction to deep learning\\project\\SSVEP\\wandb\\run-20250418_135710-q0rqzwg2</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/yucheng_shao-carnegie-mellon-university/project_ssvep-ablations/runs/q0rqzwg2' target=\"_blank\">Ben_project_base_line_EEGNet_with_raw_data</a></strong> to <a href='https://wandb.ai/yucheng_shao-carnegie-mellon-university/project_ssvep-ablations' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/yucheng_shao-carnegie-mellon-university/project_ssvep-ablations' target=\"_blank\">https://wandb.ai/yucheng_shao-carnegie-mellon-university/project_ssvep-ablations</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/yucheng_shao-carnegie-mellon-university/project_ssvep-ablations/runs/q0rqzwg2' target=\"_blank\">https://wandb.ai/yucheng_shao-carnegie-mellon-university/project_ssvep-ablations/runs/q0rqzwg2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "# Use wandb? Resume Training?\n",
    "USE_WANDB = True\n",
    "\n",
    "RESUME_LOGGING = False # Set this to true if you are resuming training from a previous run\n",
    "\n",
    "# Create your wandb run\n",
    "\n",
    "run_name = '{}_project_base_line_EEGNet_with_raw_data'.format('Ben')\n",
    "\n",
    "# If you are resuming an old run\n",
    "if USE_WANDB:\n",
    "\n",
    "    wandb.login(key=\"207d89e5e9cdfc415370f05503c72a11d5065073\") #TODO\n",
    "\n",
    "    if RESUME_LOGGING:\n",
    "        run = wandb.init(\n",
    "            id     = \"diff-CAE(fc)-EEG\", ### Insert specific run id here if you want to resume a previous run\n",
    "            resume = \"must\", ### You need this to resume previous runs\n",
    "            project = \"project_ssvep-ablations\", ### Project should be created in your wandb\n",
    "            settings = wandb.Settings(_service_wait=300)\n",
    "        )\n",
    "\n",
    "\n",
    "    else:\n",
    "        run = wandb.init(\n",
    "            name    = run_name, ### Wandb creates random run names if you skip this field, we recommend you give useful names\n",
    "            reinit  = True, ### Allows reinitalizing runs when you re-run this cell\n",
    "            project = \"project_ssvep-ablations\", ### Project should be created in your wandb account\n",
    "            config  = config ### Wandb Config for your run\n",
    "        )\n",
    "\n",
    "        ### Save your model architecture as a string with str(model)\n",
    "        model_arch  = str([ddpm, diffe, eegnet_model])\n",
    "        ### Save it in a txt file\n",
    "        arch_file   = open(\"model_arch.txt\", \"w\")\n",
    "        file_write  = arch_file.write(model_arch)\n",
    "        arch_file.close()\n",
    "\n",
    "        ### log it in your wandb run with wandb.save()\n",
    "        wandb.save('model_arch.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4dcc774b-fb53-435e-8ca0-c13ab87559bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4646"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CLEAR RAM!!\n",
    "import gc\n",
    "\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "071a1163-15ab-47ff-a3ac-82f8bd51816a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, optimizer, scheduler, metrics, epoch, path):\n",
    "    torch.save(\n",
    "        {'model_state_dict'         : model.state_dict(),\n",
    "         'optimizer_state_dict'     : optimizer.state_dict(),\n",
    "         'scheduler_state_dict'     : scheduler.state_dict(),\n",
    "         'metric'                   : metrics,\n",
    "         'epoch'                    : epoch},\n",
    "         path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "NIjWCm5vUKx-",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "3a0de191bd4b4018a92ada5e6e4607cf",
      "244fb693e8174b82aaa38b9d943ca69b",
      "07acb422b2b34b9791881de2513b32c7",
      "c7524cccdeea422d9ef342b142df608d",
      "6afb371d18044cd3815fd96c9810c3a7",
      "ce5b46cc0a1848f0adcdd11a1e7f95a3",
      "6855930ba8b94aab9f64dbbdcddd51d2",
      "38a3424ad0d5460ba042960d1736b5aa",
      "0a160710838b45c3950a599f2dce7836",
      "713d6a7190db4a54a4cd9ec1e498b1be",
      "9d503d51771842af8d76af60586edd19"
     ]
    },
    "id": "NIjWCm5vUKx-",
    "outputId": "20608bb1-27a0-4dd8-b1e9-6a61ce3e2cba"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nargs = argparse.Namespace()\\nargs.device = device\\nargs.subject = \"ALL\"\\nargs.root_dir = subject_chars_dir\\nsubject = args.subject\\n\\nnum_epochs = config[\\'epochs_AEs\\']\\ntest_period = 1\\nstart_test = test_period\\nalpha = config[\\'alpha\\']\\nbeta = config[\\'beta\\']\\n\\nbest_acc = 0\\nbest_f1 = 0\\nbest_recall = 0\\nbest_precision = 0\\nbest_auc = 0\\n\\nhistory = {\\n    \"train_loss\": [],\\n    \"train_acc\": [],\\n    \"val_loss\": [],\\n    \"val_acc\": []\\n}\\n\\nwith tqdm(total=num_epochs, desc=f\"Method ALL - Joint Training on All Subjects - Autoencoders\") as pbar:\\n    for epoch in range(num_epochs):\\n        ddpm.train()\\n        diffe.train()\\n\\n        epoch_loss = 0\\n        num_batches = 0\\n        epoch_acc = 0\\n        total_samples = 0\\n\\n        ############################## Train ###########################################\\n        for x, y in train_loader:\\n            x, y = x.to(device), y.type(torch.LongTensor).to(device)\\n            y_cat = F.one_hot(y, num_classes=26).type(torch.FloatTensor).to(device)\\n            # Train DDPM\\n            optim1.zero_grad()\\n            x_hat, down, up, noise, t = ddpm(x)\\n\\n            # Align the temporal dimension of x_hat and x\\n            x_hat, x = safe_align_2d(x_hat, x)\\n\\n            # needed if we wish to use x_hat again after first backward call\\n            x_hat_detached = x_hat.detach()\\n\\n            loss_ddpm = F.l1_loss(x_hat, x, reduction=\"none\")\\n            loss_ddpm.mean().backward()\\n            optim1.step()\\n            ddpm_out = x_hat, down, up, t\\n\\n            # Train Diff-E\\n            optim2.zero_grad()\\n            decoder_out, fc_out = diffe(x, ddpm_out)\\n\\n            #loss_gap = criterion(decoder_out, loss_ddpm.detach())\\n            loss_gap = criterion(decoder_out, x_hat_detached)\\n            loss_c = criterion_class(fc_out, y_cat)\\n            loss = beta * loss_gap + alpha * loss_c\\n            loss.backward()\\n            optim2.step()\\n\\n            # Optimizer scheduler step\\n            scheduler1.step()\\n            scheduler2.step()\\n\\n            # EMA update\\n            fc_ema.update()\\n\\n            epoch_loss += loss.item()\\n            num_batches += 1\\n\\n            pred_labels = torch.argmax(fc_out, dim=1)\\n            correct = (pred_labels == y).sum().item()\\n            epoch_acc += correct\\n            total_samples += y.size(0)\\n\\n        history[\"train_loss\"].append(epoch_loss / num_batches)\\n        history[\"train_acc\"].append(epoch_acc / total_samples)\\n\\n        metrics = {\\n            \\'train_loss\\': history[\"train_loss\"][-1],\\n            \\'train_acc\\': history[\"train_acc\"][-1],\\n        }\\n\\n        ############################## validation ###########################################\\n        with torch.no_grad():\\n            if epoch > start_test:\\n                test_period = 1\\n            if epoch % test_period == 0:\\n                ddpm.eval()\\n                diffe.eval()\\n\\n                metrics_val = evaluate(diffe.encoder, fc_ema, val_loader, device)\\n\\n                val_acc = metrics_val[\"accuracy\"]\\n                history[\"val_acc\"].append(val_acc)\\n                f1 = metrics_val[\"f1\"]\\n                recall = metrics_val[\"recall\"]\\n                precision = metrics_val[\"precision\"]\\n                auc = metrics_val[\"auc\"]\\n\\n                val_loss = 0\\n                with torch.no_grad():\\n                    for x, y in val_loader:\\n                        x, y = x.to(device), y.type(torch.LongTensor).to(device)\\n                        y_cat = F.one_hot(y, num_classes=26).float().to(device)\\n\\n                        x_hat, down, up, noise, t = ddpm(x)\\n                        ddpm_out = x_hat, down, up, t\\n\\n                        # Align the temporal dimension of x_hat and x\\n                        x_hat, x = safe_align_2d(x_hat, x)\\n\\n                        loss_ddpm = F.l1_loss(x_hat, x, reduction=\"none\")\\n                        decoder_out, fc_out = diffe(x, ddpm_out)\\n\\n                        #loss_gap = criterion(decoder_out, loss_ddpm)\\n                        loss_gap = criterion(decoder_out, x_hat)\\n                        loss_c = criterion_class(fc_out, y_cat)\\n\\n                        val_loss += (beta * loss_gap + alpha * loss_c).item()\\n                history[\"val_loss\"].append(val_loss / len(val_loader))\\n\\n                metrics.update({\\n                    \\'valid_loss\\': history[\"val_loss\"][-1],\\n                    \\'valid_acc\\': val_acc,\\n                })\\n\\n                best_acc_bool = val_acc > best_acc\\n                best_f1_bool = f1 > best_f1\\n                best_recall_bool = recall > best_recall\\n                best_precision_bool = precision > best_precision\\n                best_auc_bool = auc > best_auc\\n\\n                save_model(ddpm, optim1, scheduler1, metrics, epoch, os.path.join(checkpoint_dir, \\'ddpm_all_subjects.pth\\'))\\n                save_model(diffe, optim2, scheduler2, metrics, epoch, os.path.join(checkpoint_dir, \\'diffe_all_subjects.pth\\'))\\n\\n                if best_acc_bool:\\n                    best_acc = val_acc\\n                    #torch.save(diffe.state_dict(), f\\'/content/drive/MyDrive/project/model/ssvep/diffe_{subject}.pth\\')\\n                    #torch.save(ddpm.state_dict(), os.path.join(checkpoint_dir, \\'ddpm_all_subjects.pth\\'))\\n                    save_model(ddpm, optim1, scheduler1, metrics, epoch, os.path.join(checkpoint_dir, \\'best_ddpm_all_subjects.pth\\'))\\n                    save_model(diffe, optim2, scheduler2, metrics, epoch, os.path.join(checkpoint_dir, \\'best_diffe_all_subjects.pth\\'))\\n                    #torch.save(diffe.state_dict(), r\\'C:\\\\LTI 11785 Introduction to deep learning\\\\project\\\\SSVEP\\\\checkpoints_separated\\\\diffe_all_subjects.pth\\')\\n\\n                    wandb.save(os.path.join(checkpoint_dir, \\'best_ddpm_all_subjects.pth\\'))\\n                    wandb.save(os.path.join(checkpoint_dir, \\'best_diffe_all_subjects.pth\\'))\\n                    \\n                if best_f1_bool:\\n                    best_f1 = f1\\n                if best_recall_bool:\\n                    best_recall = recall\\n                if best_precision_bool:\\n                    best_precision = precision\\n                if best_auc_bool:\\n                    best_auc = auc\\n\\n                description = f\"Val Accuracy: {val_acc*100:.2f}% | Best: {best_acc*100:.2f}%\"\\n                pbar.set_description(f\"Method ALL – Processing subject {subject} – {description}\"\\n                )\\n        #print(f\"[Epoch {epoch+1}/{num_epochs}] Train Loss: {history[\\'train_loss\\'][-1]:.4f} | Val Acc: {val_acc*100:.2f}%\")\\n        print(f\"[Epoch {epoch+1}/{num_epochs}]\")\\n        print(f\"Train Loss: {history[\\'train_loss\\'][-1]:.4f} | Train Acc: {history[\\'train_acc\\'][-1]*100:.2f}%\")\\n        print(f\"Valid Loss: {history[\\'val_loss\\'][-1]:.4f} | Valid Acc: {val_acc*100:.2f}%\")\\n        pbar.update(1)\\n\\n        if run is not None:\\n            run.log(metrics)\\n'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train & Evaluate\n",
    "checkpoint_dir = config['checkpoint_dir']\n",
    "'''\n",
    "args = argparse.Namespace()\n",
    "args.device = device\n",
    "args.subject = \"ALL\"\n",
    "args.root_dir = subject_chars_dir\n",
    "subject = args.subject\n",
    "\n",
    "num_epochs = config['epochs_AEs']\n",
    "test_period = 1\n",
    "start_test = test_period\n",
    "alpha = config['alpha']\n",
    "beta = config['beta']\n",
    "\n",
    "best_acc = 0\n",
    "best_f1 = 0\n",
    "best_recall = 0\n",
    "best_precision = 0\n",
    "best_auc = 0\n",
    "\n",
    "history = {\n",
    "    \"train_loss\": [],\n",
    "    \"train_acc\": [],\n",
    "    \"val_loss\": [],\n",
    "    \"val_acc\": []\n",
    "}\n",
    "\n",
    "with tqdm(total=num_epochs, desc=f\"Method ALL - Joint Training on All Subjects - Autoencoders\") as pbar:\n",
    "    for epoch in range(num_epochs):\n",
    "        ddpm.train()\n",
    "        diffe.train()\n",
    "\n",
    "        epoch_loss = 0\n",
    "        num_batches = 0\n",
    "        epoch_acc = 0\n",
    "        total_samples = 0\n",
    "\n",
    "        ############################## Train ###########################################\n",
    "        for x, y in train_loader:\n",
    "            x, y = x.to(device), y.type(torch.LongTensor).to(device)\n",
    "            y_cat = F.one_hot(y, num_classes=26).type(torch.FloatTensor).to(device)\n",
    "            # Train DDPM\n",
    "            optim1.zero_grad()\n",
    "            x_hat, down, up, noise, t = ddpm(x)\n",
    "\n",
    "            # Align the temporal dimension of x_hat and x\n",
    "            x_hat, x = safe_align_2d(x_hat, x)\n",
    "\n",
    "            # needed if we wish to use x_hat again after first backward call\n",
    "            x_hat_detached = x_hat.detach()\n",
    "\n",
    "            loss_ddpm = F.l1_loss(x_hat, x, reduction=\"none\")\n",
    "            loss_ddpm.mean().backward()\n",
    "            optim1.step()\n",
    "            ddpm_out = x_hat, down, up, t\n",
    "\n",
    "            # Train Diff-E\n",
    "            optim2.zero_grad()\n",
    "            decoder_out, fc_out = diffe(x, ddpm_out)\n",
    "\n",
    "            #loss_gap = criterion(decoder_out, loss_ddpm.detach())\n",
    "            loss_gap = criterion(decoder_out, x_hat_detached)\n",
    "            loss_c = criterion_class(fc_out, y_cat)\n",
    "            loss = beta * loss_gap + alpha * loss_c\n",
    "            loss.backward()\n",
    "            optim2.step()\n",
    "\n",
    "            # Optimizer scheduler step\n",
    "            scheduler1.step()\n",
    "            scheduler2.step()\n",
    "\n",
    "            # EMA update\n",
    "            fc_ema.update()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            num_batches += 1\n",
    "\n",
    "            pred_labels = torch.argmax(fc_out, dim=1)\n",
    "            correct = (pred_labels == y).sum().item()\n",
    "            epoch_acc += correct\n",
    "            total_samples += y.size(0)\n",
    "\n",
    "        history[\"train_loss\"].append(epoch_loss / num_batches)\n",
    "        history[\"train_acc\"].append(epoch_acc / total_samples)\n",
    "\n",
    "        metrics = {\n",
    "            'train_loss': history[\"train_loss\"][-1],\n",
    "            'train_acc': history[\"train_acc\"][-1],\n",
    "        }\n",
    "\n",
    "        ############################## validation ###########################################\n",
    "        with torch.no_grad():\n",
    "            if epoch > start_test:\n",
    "                test_period = 1\n",
    "            if epoch % test_period == 0:\n",
    "                ddpm.eval()\n",
    "                diffe.eval()\n",
    "\n",
    "                metrics_val = evaluate(diffe.encoder, fc_ema, val_loader, device)\n",
    "\n",
    "                val_acc = metrics_val[\"accuracy\"]\n",
    "                history[\"val_acc\"].append(val_acc)\n",
    "                f1 = metrics_val[\"f1\"]\n",
    "                recall = metrics_val[\"recall\"]\n",
    "                precision = metrics_val[\"precision\"]\n",
    "                auc = metrics_val[\"auc\"]\n",
    "\n",
    "                val_loss = 0\n",
    "                with torch.no_grad():\n",
    "                    for x, y in val_loader:\n",
    "                        x, y = x.to(device), y.type(torch.LongTensor).to(device)\n",
    "                        y_cat = F.one_hot(y, num_classes=26).float().to(device)\n",
    "\n",
    "                        x_hat, down, up, noise, t = ddpm(x)\n",
    "                        ddpm_out = x_hat, down, up, t\n",
    "\n",
    "                        # Align the temporal dimension of x_hat and x\n",
    "                        x_hat, x = safe_align_2d(x_hat, x)\n",
    "\n",
    "                        loss_ddpm = F.l1_loss(x_hat, x, reduction=\"none\")\n",
    "                        decoder_out, fc_out = diffe(x, ddpm_out)\n",
    "\n",
    "                        #loss_gap = criterion(decoder_out, loss_ddpm)\n",
    "                        loss_gap = criterion(decoder_out, x_hat)\n",
    "                        loss_c = criterion_class(fc_out, y_cat)\n",
    "\n",
    "                        val_loss += (beta * loss_gap + alpha * loss_c).item()\n",
    "                history[\"val_loss\"].append(val_loss / len(val_loader))\n",
    "\n",
    "                metrics.update({\n",
    "                    'valid_loss': history[\"val_loss\"][-1],\n",
    "                    'valid_acc': val_acc,\n",
    "                })\n",
    "\n",
    "                best_acc_bool = val_acc > best_acc\n",
    "                best_f1_bool = f1 > best_f1\n",
    "                best_recall_bool = recall > best_recall\n",
    "                best_precision_bool = precision > best_precision\n",
    "                best_auc_bool = auc > best_auc\n",
    "\n",
    "                save_model(ddpm, optim1, scheduler1, metrics, epoch, os.path.join(checkpoint_dir, 'ddpm_all_subjects.pth'))\n",
    "                save_model(diffe, optim2, scheduler2, metrics, epoch, os.path.join(checkpoint_dir, 'diffe_all_subjects.pth'))\n",
    "\n",
    "                if best_acc_bool:\n",
    "                    best_acc = val_acc\n",
    "                    #torch.save(diffe.state_dict(), f'/content/drive/MyDrive/project/model/ssvep/diffe_{subject}.pth')\n",
    "                    #torch.save(ddpm.state_dict(), os.path.join(checkpoint_dir, 'ddpm_all_subjects.pth'))\n",
    "                    save_model(ddpm, optim1, scheduler1, metrics, epoch, os.path.join(checkpoint_dir, 'best_ddpm_all_subjects.pth'))\n",
    "                    save_model(diffe, optim2, scheduler2, metrics, epoch, os.path.join(checkpoint_dir, 'best_diffe_all_subjects.pth'))\n",
    "                    #torch.save(diffe.state_dict(), r'C:\\LTI 11785 Introduction to deep learning\\project\\SSVEP\\checkpoints_separated\\diffe_all_subjects.pth')\n",
    "\n",
    "                    wandb.save(os.path.join(checkpoint_dir, 'best_ddpm_all_subjects.pth'))\n",
    "                    wandb.save(os.path.join(checkpoint_dir, 'best_diffe_all_subjects.pth'))\n",
    "                    \n",
    "                if best_f1_bool:\n",
    "                    best_f1 = f1\n",
    "                if best_recall_bool:\n",
    "                    best_recall = recall\n",
    "                if best_precision_bool:\n",
    "                    best_precision = precision\n",
    "                if best_auc_bool:\n",
    "                    best_auc = auc\n",
    "\n",
    "                description = f\"Val Accuracy: {val_acc*100:.2f}% | Best: {best_acc*100:.2f}%\"\n",
    "                pbar.set_description(f\"Method ALL – Processing subject {subject} – {description}\"\n",
    "                )\n",
    "        #print(f\"[Epoch {epoch+1}/{num_epochs}] Train Loss: {history['train_loss'][-1]:.4f} | Val Acc: {val_acc*100:.2f}%\")\n",
    "        print(f\"[Epoch {epoch+1}/{num_epochs}]\")\n",
    "        print(f\"Train Loss: {history['train_loss'][-1]:.4f} | Train Acc: {history['train_acc'][-1]*100:.2f}%\")\n",
    "        print(f\"Valid Loss: {history['val_loss'][-1]:.4f} | Valid Acc: {val_acc*100:.2f}%\")\n",
    "        pbar.update(1)\n",
    "\n",
    "        if run is not None:\n",
    "            run.log(metrics)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "29ead19d-ace6-4d99-a3d8-7c449452343d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#val_losses = []\n",
    "#val_accuracies = []\n",
    "#initializer = RandomNormal(mean=0.0, stddev=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "cb13fd4d-ae68-4d9c-9b30-42e3fc082b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate2(diffe, ddpm, EEGNet, generator, device):\n",
    "    labels = np.arange(0, 26)\n",
    "    Y = []\n",
    "    Y_hat = []\n",
    "    for x, y in generator:\n",
    "        x, y = x.to(device), y.type(torch.LongTensor).to(device)\n",
    "        y_cat = F.one_hot(y, num_classes=26).type(torch.FloatTensor).to(device)\n",
    "\n",
    "        #x_hat, down, up, noise, t = ddpm(x)\n",
    "\n",
    "        # Align the temporal dimension of x_hat and x\n",
    "        #x_hat, x = safe_align_2d(x_hat, x)\n",
    "\n",
    "        #ddpm_out = x_hat, down, up, t\n",
    "        #ddpm_out = x, down, up, t\n",
    "\n",
    "        #decoder_out, fc_out = diffe(x, ddpm_out)\n",
    "        \n",
    "        #X_eegnet = decoder_out.view(decoder_out.size(0), 64, -1)  # (samples, 64, 396)\n",
    "        X_eegnet = x.view(x.size(0), 64, -1)\n",
    "        y_hat = EEGNet(X_eegnet)\n",
    "        \n",
    "        y_hat = F.softmax(y_hat, dim=1)\n",
    "\n",
    "        Y.append(y.detach().cpu())\n",
    "        Y_hat.append(y_hat.detach().cpu())\n",
    "\n",
    "    # List of tensors to tensor to numpy\n",
    "    Y = torch.cat(Y, dim=0).numpy()  # (N, )\n",
    "    Y_hat = torch.cat(Y_hat, dim=0).numpy()  # (N, 13): has to sum to 1 for each row\n",
    "\n",
    "    # Accuracy and Confusion Matrix\n",
    "    accuracy = top_k_accuracy_score(Y, Y_hat, k=1, labels=labels)\n",
    "    f1 = f1_score(Y, Y_hat.argmax(axis=1), average=\"macro\", labels=labels)\n",
    "    recall = recall_score(Y, Y_hat.argmax(axis=1), average=\"macro\", labels=labels)\n",
    "    precision = precision_score(Y, Y_hat.argmax(axis=1), average=\"macro\", labels=labels, zero_division=0)\n",
    "    auc = roc_auc_score(Y, Y_hat, average=\"macro\", multi_class=\"ovo\", labels=labels)\n",
    "\n",
    "    metrics = {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"f1\": f1,\n",
    "        \"recall\": recall,\n",
    "        \"precision\": precision,\n",
    "        \"auc\": auc,\n",
    "    }\n",
    "    # df_cm = pd.DataFrame(confusion_matrix(Y, Y_hat.argmax(axis=1)))\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "49dc8713-5e49-4662-bb9f-3920b1ad343d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 26s/step - accuracy: 0.0258 - loss: 3.76 ━━━━━━━━━━━━━━━━━━━━ 27s 27s/step - accuracy: 0.0258 - loss: 3.7621 - val_accuracy: 0.0317 - val_loss: 3.6889\n",
      "Epoch 2/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.0252 - loss: 3.68 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.0252 - loss: 3.6896 - val_accuracy: 0.0333 - val_loss: 3.6888\n",
      "Epoch 3/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.0244 - loss: 3.68 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.0244 - loss: 3.6859 - val_accuracy: 0.0417 - val_loss: 3.6887\n",
      "Epoch 4/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.0352 - loss: 3.68 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.0352 - loss: 3.6813 - val_accuracy: 0.0417 - val_loss: 3.6887\n",
      "Epoch 5/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.0375 - loss: 3.67 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.0375 - loss: 3.6764 - val_accuracy: 0.0433 - val_loss: 3.6886\n",
      "Epoch 6/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.0471 - loss: 3.67 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.0471 - loss: 3.6710 - val_accuracy: 0.0383 - val_loss: 3.6885\n",
      "Epoch 7/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.0490 - loss: 3.66 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.0490 - loss: 3.6682 - val_accuracy: 0.0350 - val_loss: 3.6884\n",
      "Epoch 8/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.0517 - loss: 3.66 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.0517 - loss: 3.6636 - val_accuracy: 0.0308 - val_loss: 3.6882\n",
      "Epoch 9/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.0575 - loss: 3.65 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.0575 - loss: 3.6584 - val_accuracy: 0.0308 - val_loss: 3.6881\n",
      "Epoch 10/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19s/step - accuracy: 0.0615 - loss: 3.65 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.0615 - loss: 3.6529 - val_accuracy: 0.0275 - val_loss: 3.6879\n",
      "Epoch 11/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.0679 - loss: 3.64 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.0679 - loss: 3.6471 - val_accuracy: 0.0283 - val_loss: 3.6876\n",
      "Epoch 12/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.0692 - loss: 3.64 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.0692 - loss: 3.6402 - val_accuracy: 0.0267 - val_loss: 3.6874\n",
      "Epoch 13/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.0771 - loss: 3.63 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.0771 - loss: 3.6318 - val_accuracy: 0.0283 - val_loss: 3.6871\n",
      "Epoch 14/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.0800 - loss: 3.62 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.0800 - loss: 3.6257 - val_accuracy: 0.0292 - val_loss: 3.6867\n",
      "Epoch 15/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.0835 - loss: 3.61 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.0835 - loss: 3.6155 - val_accuracy: 0.0300 - val_loss: 3.6864\n",
      "Epoch 16/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.0752 - loss: 3.60 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.0752 - loss: 3.6083 - val_accuracy: 0.0325 - val_loss: 3.6859\n",
      "Epoch 17/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.0908 - loss: 3.59 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.0908 - loss: 3.5972 - val_accuracy: 0.0350 - val_loss: 3.6854\n",
      "Epoch 18/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.0883 - loss: 3.58 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.0883 - loss: 3.5886 - val_accuracy: 0.0383 - val_loss: 3.6849\n",
      "Epoch 19/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.0879 - loss: 3.57 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.0879 - loss: 3.5784 - val_accuracy: 0.0400 - val_loss: 3.6843\n",
      "Epoch 20/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.0969 - loss: 3.56 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.0969 - loss: 3.5672 - val_accuracy: 0.0408 - val_loss: 3.6837\n",
      "Epoch 21/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.0927 - loss: 3.55 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.0927 - loss: 3.5561 - val_accuracy: 0.0400 - val_loss: 3.6830\n",
      "Epoch 22/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.1040 - loss: 3.54 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.1040 - loss: 3.5441 - val_accuracy: 0.0392 - val_loss: 3.6822\n",
      "Epoch 23/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.1071 - loss: 3.53 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.1071 - loss: 3.5330 - val_accuracy: 0.0400 - val_loss: 3.6814\n",
      "Epoch 24/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.1035 - loss: 3.52 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.1035 - loss: 3.5213 - val_accuracy: 0.0400 - val_loss: 3.6806\n",
      "Epoch 25/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.1042 - loss: 3.50 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.1042 - loss: 3.5095 - val_accuracy: 0.0392 - val_loss: 3.6797\n",
      "Epoch 26/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.1052 - loss: 3.49 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.1052 - loss: 3.4969 - val_accuracy: 0.0383 - val_loss: 3.6788\n",
      "Epoch 27/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.1027 - loss: 3.48 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.1027 - loss: 3.4864 - val_accuracy: 0.0400 - val_loss: 3.6779\n",
      "Epoch 28/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.1112 - loss: 3.47 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.1112 - loss: 3.4732 - val_accuracy: 0.0400 - val_loss: 3.6769\n",
      "Epoch 29/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.1044 - loss: 3.46 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.1044 - loss: 3.4641 - val_accuracy: 0.0417 - val_loss: 3.6759\n",
      "Epoch 30/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.1098 - loss: 3.45 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.1098 - loss: 3.4502 - val_accuracy: 0.0442 - val_loss: 3.6749\n",
      "Epoch 31/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.1121 - loss: 3.43 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.1121 - loss: 3.4397 - val_accuracy: 0.0458 - val_loss: 3.6739\n",
      "Epoch 32/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.1090 - loss: 3.42 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.1090 - loss: 3.4287 - val_accuracy: 0.0467 - val_loss: 3.6729\n",
      "Epoch 33/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.1169 - loss: 3.41 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.1169 - loss: 3.4173 - val_accuracy: 0.0475 - val_loss: 3.6719\n",
      "Epoch 34/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.1248 - loss: 3.40 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.1248 - loss: 3.4028 - val_accuracy: 0.0458 - val_loss: 3.6709\n",
      "Epoch 35/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.1094 - loss: 3.39 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.1094 - loss: 3.3954 - val_accuracy: 0.0458 - val_loss: 3.6698\n",
      "Epoch 36/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.1206 - loss: 3.38 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.1206 - loss: 3.3824 - val_accuracy: 0.0450 - val_loss: 3.6688\n",
      "Epoch 37/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.1185 - loss: 3.37 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.1185 - loss: 3.3755 - val_accuracy: 0.0450 - val_loss: 3.6678\n",
      "Epoch 38/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19s/step - accuracy: 0.1229 - loss: 3.36 ━━━━━━━━━━━━━━━━━━━━ 20s 20s/step - accuracy: 0.1229 - loss: 3.3633 - val_accuracy: 0.0458 - val_loss: 3.6667\n",
      "Epoch 39/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.1242 - loss: 3.35 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.1242 - loss: 3.3531 - val_accuracy: 0.0467 - val_loss: 3.6657\n",
      "Epoch 40/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.1250 - loss: 3.34 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.1250 - loss: 3.3450 - val_accuracy: 0.0458 - val_loss: 3.6646\n",
      "Epoch 41/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.1319 - loss: 3.33 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.1319 - loss: 3.3357 - val_accuracy: 0.0450 - val_loss: 3.6635\n",
      "Epoch 42/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19s/step - accuracy: 0.1283 - loss: 3.32 ━━━━━━━━━━━━━━━━━━━━ 20s 20s/step - accuracy: 0.1283 - loss: 3.3257 - val_accuracy: 0.0450 - val_loss: 3.6624\n",
      "Epoch 43/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19s/step - accuracy: 0.1383 - loss: 3.31 ━━━━━━━━━━━━━━━━━━━━ 20s 20s/step - accuracy: 0.1383 - loss: 3.3136 - val_accuracy: 0.0458 - val_loss: 3.6612\n",
      "Epoch 44/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19s/step - accuracy: 0.1381 - loss: 3.30 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.1381 - loss: 3.3044 - val_accuracy: 0.0475 - val_loss: 3.6601\n",
      "Epoch 45/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.1333 - loss: 3.29 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.1333 - loss: 3.2953 - val_accuracy: 0.0475 - val_loss: 3.6589\n",
      "Epoch 46/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.1371 - loss: 3.28 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.1371 - loss: 3.2854 - val_accuracy: 0.0483 - val_loss: 3.6578\n",
      "Epoch 47/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.1452 - loss: 3.27 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.1452 - loss: 3.2719 - val_accuracy: 0.0492 - val_loss: 3.6567\n",
      "Epoch 48/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.1513 - loss: 3.26 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.1513 - loss: 3.2649 - val_accuracy: 0.0500 - val_loss: 3.6557\n",
      "Epoch 49/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.1510 - loss: 3.25 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.1510 - loss: 3.2545 - val_accuracy: 0.0492 - val_loss: 3.6546\n",
      "Epoch 50/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.1485 - loss: 3.24 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.1485 - loss: 3.2436 - val_accuracy: 0.0483 - val_loss: 3.6536\n",
      "Epoch 51/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19s/step - accuracy: 0.1544 - loss: 3.23 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.1544 - loss: 3.2316 - val_accuracy: 0.0467 - val_loss: 3.6527\n",
      "Epoch 52/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.1542 - loss: 3.21 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.1542 - loss: 3.2188 - val_accuracy: 0.0450 - val_loss: 3.6518\n",
      "Epoch 53/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.1600 - loss: 3.20 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.1600 - loss: 3.2075 - val_accuracy: 0.0433 - val_loss: 3.6510\n",
      "Epoch 54/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.1669 - loss: 3.19 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.1669 - loss: 3.1966 - val_accuracy: 0.0442 - val_loss: 3.6502\n",
      "Epoch 55/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.1710 - loss: 3.18 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.1710 - loss: 3.1818 - val_accuracy: 0.0417 - val_loss: 3.6495\n",
      "Epoch 56/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.1669 - loss: 3.17 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.1669 - loss: 3.1701 - val_accuracy: 0.0417 - val_loss: 3.6489\n",
      "Epoch 57/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.1700 - loss: 3.15 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.1700 - loss: 3.1589 - val_accuracy: 0.0408 - val_loss: 3.6484\n",
      "Epoch 58/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.1660 - loss: 3.14 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.1660 - loss: 3.1462 - val_accuracy: 0.0383 - val_loss: 3.6479\n",
      "Epoch 59/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.1719 - loss: 3.13 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.1719 - loss: 3.1321 - val_accuracy: 0.0375 - val_loss: 3.6475\n",
      "Epoch 60/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.1681 - loss: 3.12 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.1681 - loss: 3.1218 - val_accuracy: 0.0375 - val_loss: 3.6471\n",
      "Epoch 61/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.1775 - loss: 3.10 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.1775 - loss: 3.1090 - val_accuracy: 0.0367 - val_loss: 3.6468\n",
      "Epoch 62/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.1808 - loss: 3.09 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.1808 - loss: 3.0983 - val_accuracy: 0.0350 - val_loss: 3.6464\n",
      "Epoch 63/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19s/step - accuracy: 0.1731 - loss: 3.08 ━━━━━━━━━━━━━━━━━━━━ 20s 20s/step - accuracy: 0.1731 - loss: 3.0863 - val_accuracy: 0.0367 - val_loss: 3.6460\n",
      "Epoch 64/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19s/step - accuracy: 0.1900 - loss: 3.07 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.1900 - loss: 3.0735 - val_accuracy: 0.0367 - val_loss: 3.6456\n",
      "Epoch 65/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.1873 - loss: 3.06 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.1873 - loss: 3.0635 - val_accuracy: 0.0358 - val_loss: 3.6451\n",
      "Epoch 66/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.1875 - loss: 3.05 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.1875 - loss: 3.0512 - val_accuracy: 0.0350 - val_loss: 3.6446\n",
      "Epoch 67/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.1952 - loss: 3.03 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.1952 - loss: 3.0368 - val_accuracy: 0.0350 - val_loss: 3.6440\n",
      "Epoch 68/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.1960 - loss: 3.02 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.1960 - loss: 3.0223 - val_accuracy: 0.0350 - val_loss: 3.6434\n",
      "Epoch 69/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.1969 - loss: 3.01 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.1969 - loss: 3.0150 - val_accuracy: 0.0358 - val_loss: 3.6427\n",
      "Epoch 70/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.2058 - loss: 3.00 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.2058 - loss: 3.0044 - val_accuracy: 0.0333 - val_loss: 3.6421\n",
      "Epoch 71/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.2052 - loss: 2.99 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.2052 - loss: 2.9932 - val_accuracy: 0.0325 - val_loss: 3.6416\n",
      "Epoch 72/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.2042 - loss: 2.98 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.2042 - loss: 2.9807 - val_accuracy: 0.0325 - val_loss: 3.6411\n",
      "Epoch 73/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.2081 - loss: 2.96 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.2081 - loss: 2.9687 - val_accuracy: 0.0325 - val_loss: 3.6406\n",
      "Epoch 74/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.2167 - loss: 2.95 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.2167 - loss: 2.9556 - val_accuracy: 0.0325 - val_loss: 3.6400\n",
      "Epoch 75/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.2198 - loss: 2.94 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.2198 - loss: 2.9473 - val_accuracy: 0.0317 - val_loss: 3.6396\n",
      "Epoch 76/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.2223 - loss: 2.93 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.2223 - loss: 2.9326 - val_accuracy: 0.0308 - val_loss: 3.6391\n",
      "Epoch 77/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.2204 - loss: 2.91 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.2204 - loss: 2.9199 - val_accuracy: 0.0308 - val_loss: 3.6388\n",
      "Epoch 78/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.2338 - loss: 2.91 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.2338 - loss: 2.9116 - val_accuracy: 0.0300 - val_loss: 3.6385\n",
      "Epoch 79/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.2242 - loss: 2.89 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.2242 - loss: 2.8977 - val_accuracy: 0.0292 - val_loss: 3.6380\n",
      "Epoch 80/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.2385 - loss: 2.88 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.2385 - loss: 2.8895 - val_accuracy: 0.0300 - val_loss: 3.6376\n",
      "Epoch 81/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.2323 - loss: 2.87 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.2323 - loss: 2.8765 - val_accuracy: 0.0300 - val_loss: 3.6372\n",
      "Epoch 82/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.2419 - loss: 2.86 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.2419 - loss: 2.8647 - val_accuracy: 0.0275 - val_loss: 3.6368\n",
      "Epoch 83/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.2540 - loss: 2.85 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.2540 - loss: 2.8524 - val_accuracy: 0.0275 - val_loss: 3.6364\n",
      "Epoch 84/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.2435 - loss: 2.84 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.2435 - loss: 2.8407 - val_accuracy: 0.0275 - val_loss: 3.6359\n",
      "Epoch 85/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.2500 - loss: 2.83 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.2500 - loss: 2.8331 - val_accuracy: 0.0267 - val_loss: 3.6354\n",
      "Epoch 86/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.2575 - loss: 2.82 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.2575 - loss: 2.8218 - val_accuracy: 0.0267 - val_loss: 3.6348\n",
      "Epoch 87/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.2623 - loss: 2.80 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.2623 - loss: 2.8081 - val_accuracy: 0.0267 - val_loss: 3.6341\n",
      "Epoch 88/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.2654 - loss: 2.79 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.2654 - loss: 2.7980 - val_accuracy: 0.0267 - val_loss: 3.6335\n",
      "Epoch 89/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.2710 - loss: 2.79 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.2710 - loss: 2.7917 - val_accuracy: 0.0267 - val_loss: 3.6328\n",
      "Epoch 90/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.2654 - loss: 2.77 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.2654 - loss: 2.7766 - val_accuracy: 0.0267 - val_loss: 3.6322\n",
      "Epoch 91/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.2738 - loss: 2.76 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.2738 - loss: 2.7664 - val_accuracy: 0.0267 - val_loss: 3.6315\n",
      "Epoch 92/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.2796 - loss: 2.75 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.2796 - loss: 2.7540 - val_accuracy: 0.0258 - val_loss: 3.6308\n",
      "Epoch 93/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.2837 - loss: 2.74 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.2837 - loss: 2.7446 - val_accuracy: 0.0258 - val_loss: 3.6303\n",
      "Epoch 94/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.2767 - loss: 2.73 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.2767 - loss: 2.7360 - val_accuracy: 0.0258 - val_loss: 3.6295\n",
      "Epoch 95/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.3010 - loss: 2.72 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.3010 - loss: 2.7221 - val_accuracy: 0.0258 - val_loss: 3.6285\n",
      "Epoch 96/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.2977 - loss: 2.71 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.2977 - loss: 2.7179 - val_accuracy: 0.0258 - val_loss: 3.6275\n",
      "Epoch 97/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.2950 - loss: 2.70 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.2950 - loss: 2.7042 - val_accuracy: 0.0258 - val_loss: 3.6265\n",
      "Epoch 98/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.2996 - loss: 2.69 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.2996 - loss: 2.6989 - val_accuracy: 0.0258 - val_loss: 3.6257\n",
      "Epoch 99/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.2996 - loss: 2.68 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.2996 - loss: 2.6879 - val_accuracy: 0.0267 - val_loss: 3.6247\n",
      "Epoch 100/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.3035 - loss: 2.67 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.3035 - loss: 2.6793 - val_accuracy: 0.0267 - val_loss: 3.6237\n",
      "Epoch 101/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.3069 - loss: 2.66 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.3069 - loss: 2.6686 - val_accuracy: 0.0267 - val_loss: 3.6224\n",
      "Epoch 102/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.3175 - loss: 2.66 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.3175 - loss: 2.6615 - val_accuracy: 0.0267 - val_loss: 3.6210\n",
      "Epoch 103/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.3240 - loss: 2.64 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.3240 - loss: 2.6458 - val_accuracy: 0.0267 - val_loss: 3.6194\n",
      "Epoch 104/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.3244 - loss: 2.63 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.3244 - loss: 2.6397 - val_accuracy: 0.0267 - val_loss: 3.6180\n",
      "Epoch 105/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.3194 - loss: 2.63 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.3194 - loss: 2.6305 - val_accuracy: 0.0267 - val_loss: 3.6167\n",
      "Epoch 106/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.3383 - loss: 2.61 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.3383 - loss: 2.6163 - val_accuracy: 0.0258 - val_loss: 3.6154\n",
      "Epoch 107/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.3458 - loss: 2.60 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.3458 - loss: 2.6041 - val_accuracy: 0.0258 - val_loss: 3.6137\n",
      "Epoch 108/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.3533 - loss: 2.59 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.3533 - loss: 2.5944 - val_accuracy: 0.0267 - val_loss: 3.6113\n",
      "Epoch 109/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.3529 - loss: 2.58 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.3529 - loss: 2.5872 - val_accuracy: 0.0275 - val_loss: 3.6088\n",
      "Epoch 110/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.3619 - loss: 2.57 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.3619 - loss: 2.5738 - val_accuracy: 0.0292 - val_loss: 3.6070\n",
      "Epoch 111/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.3648 - loss: 2.56 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.3648 - loss: 2.5657 - val_accuracy: 0.0300 - val_loss: 3.6056\n",
      "Epoch 112/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.3729 - loss: 2.55 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.3729 - loss: 2.5518 - val_accuracy: 0.0325 - val_loss: 3.6038\n",
      "Epoch 113/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.3752 - loss: 2.53 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.3752 - loss: 2.5397 - val_accuracy: 0.0408 - val_loss: 3.6015\n",
      "Epoch 114/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.3798 - loss: 2.52 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.3798 - loss: 2.5282 - val_accuracy: 0.0533 - val_loss: 3.5988\n",
      "Epoch 115/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.3758 - loss: 2.51 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.3758 - loss: 2.5152 - val_accuracy: 0.0567 - val_loss: 3.5960\n",
      "Epoch 116/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.4021 - loss: 2.50 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.4021 - loss: 2.5028 - val_accuracy: 0.0558 - val_loss: 3.5933\n",
      "Epoch 117/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.3988 - loss: 2.49 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.3988 - loss: 2.4976 - val_accuracy: 0.0558 - val_loss: 3.5908\n",
      "Epoch 118/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.4133 - loss: 2.48 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.4133 - loss: 2.4811 - val_accuracy: 0.0575 - val_loss: 3.5885\n",
      "Epoch 119/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.4146 - loss: 2.46 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.4146 - loss: 2.4681 - val_accuracy: 0.0567 - val_loss: 3.5860\n",
      "Epoch 120/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.4252 - loss: 2.45 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.4252 - loss: 2.4551 - val_accuracy: 0.0575 - val_loss: 3.5828\n",
      "Epoch 121/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.4404 - loss: 2.43 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.4404 - loss: 2.4392 - val_accuracy: 0.0650 - val_loss: 3.5791\n",
      "Epoch 122/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.4373 - loss: 2.42 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.4373 - loss: 2.4278 - val_accuracy: 0.0667 - val_loss: 3.5749\n",
      "Epoch 123/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.4387 - loss: 2.42 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.4387 - loss: 2.4209 - val_accuracy: 0.0692 - val_loss: 3.5711\n",
      "Epoch 124/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.4525 - loss: 2.40 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.4525 - loss: 2.4044 - val_accuracy: 0.0767 - val_loss: 3.5683\n",
      "Epoch 125/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.4590 - loss: 2.39 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.4590 - loss: 2.3939 - val_accuracy: 0.0758 - val_loss: 3.5655\n",
      "Epoch 126/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.4575 - loss: 2.38 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.4575 - loss: 2.3869 - val_accuracy: 0.0808 - val_loss: 3.5621\n",
      "Epoch 127/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.4642 - loss: 2.37 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.4642 - loss: 2.3773 - val_accuracy: 0.0992 - val_loss: 3.5580\n",
      "Epoch 128/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.4656 - loss: 2.36 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.4656 - loss: 2.3639 - val_accuracy: 0.1117 - val_loss: 3.5539\n",
      "Epoch 129/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.4787 - loss: 2.35 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.4787 - loss: 2.3541 - val_accuracy: 0.1225 - val_loss: 3.5499\n",
      "Epoch 130/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.4890 - loss: 2.34 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.4890 - loss: 2.3466 - val_accuracy: 0.1258 - val_loss: 3.5463\n",
      "Epoch 131/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.4894 - loss: 2.33 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.4894 - loss: 2.3345 - val_accuracy: 0.1242 - val_loss: 3.5425\n",
      "Epoch 132/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.4931 - loss: 2.32 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.4931 - loss: 2.3263 - val_accuracy: 0.1233 - val_loss: 3.5385\n",
      "Epoch 133/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.5017 - loss: 2.31 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.5017 - loss: 2.3128 - val_accuracy: 0.1200 - val_loss: 3.5345\n",
      "Epoch 134/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.5056 - loss: 2.30 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.5056 - loss: 2.3081 - val_accuracy: 0.1200 - val_loss: 3.5306\n",
      "Epoch 135/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.5146 - loss: 2.29 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.5146 - loss: 2.2984 - val_accuracy: 0.1192 - val_loss: 3.5262\n",
      "Epoch 136/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.5108 - loss: 2.29 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.5108 - loss: 2.2924 - val_accuracy: 0.1208 - val_loss: 3.5216\n",
      "Epoch 137/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.5269 - loss: 2.27 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.5269 - loss: 2.2786 - val_accuracy: 0.1217 - val_loss: 3.5174\n",
      "Epoch 138/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.5362 - loss: 2.26 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.5362 - loss: 2.2699 - val_accuracy: 0.1217 - val_loss: 3.5132\n",
      "Epoch 139/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.5248 - loss: 2.26 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.5248 - loss: 2.2678 - val_accuracy: 0.1233 - val_loss: 3.5088\n",
      "Epoch 140/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.5465 - loss: 2.25 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.5465 - loss: 2.2557 - val_accuracy: 0.1242 - val_loss: 3.5043\n",
      "Epoch 141/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.5529 - loss: 2.24 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.5529 - loss: 2.2493 - val_accuracy: 0.1275 - val_loss: 3.4995\n",
      "Epoch 142/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.5585 - loss: 2.24 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.5585 - loss: 2.2407 - val_accuracy: 0.1275 - val_loss: 3.4949\n",
      "Epoch 143/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.5631 - loss: 2.23 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.5631 - loss: 2.2326 - val_accuracy: 0.1267 - val_loss: 3.4903\n",
      "Epoch 144/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.5581 - loss: 2.22 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.5581 - loss: 2.2225 - val_accuracy: 0.1267 - val_loss: 3.4858\n",
      "Epoch 145/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.5733 - loss: 2.21 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.5733 - loss: 2.2147 - val_accuracy: 0.1292 - val_loss: 3.4807\n",
      "Epoch 146/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.5825 - loss: 2.20 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.5825 - loss: 2.2096 - val_accuracy: 0.1342 - val_loss: 3.4754\n",
      "Epoch 147/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.5827 - loss: 2.19 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.5827 - loss: 2.1996 - val_accuracy: 0.1367 - val_loss: 3.4701\n",
      "Epoch 148/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.5854 - loss: 2.19 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.5854 - loss: 2.1924 - val_accuracy: 0.1358 - val_loss: 3.4659\n",
      "Epoch 149/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.5979 - loss: 2.18 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.5979 - loss: 2.1870 - val_accuracy: 0.1333 - val_loss: 3.4616\n",
      "Epoch 150/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.5881 - loss: 2.17 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.5881 - loss: 2.1785 - val_accuracy: 0.1342 - val_loss: 3.4565\n",
      "Epoch 151/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.5967 - loss: 2.17 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.5967 - loss: 2.1753 - val_accuracy: 0.1383 - val_loss: 3.4509\n",
      "Epoch 152/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.6098 - loss: 2.16 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.6098 - loss: 2.1666 - val_accuracy: 0.1442 - val_loss: 3.4452\n",
      "Epoch 153/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.6150 - loss: 2.15 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.6150 - loss: 2.1547 - val_accuracy: 0.1492 - val_loss: 3.4397\n",
      "Epoch 154/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.6075 - loss: 2.14 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.6075 - loss: 2.1480 - val_accuracy: 0.1533 - val_loss: 3.4347\n",
      "Epoch 155/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.6223 - loss: 2.13 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.6223 - loss: 2.1388 - val_accuracy: 0.1533 - val_loss: 3.4301\n",
      "Epoch 156/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.6219 - loss: 2.13 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.6219 - loss: 2.1367 - val_accuracy: 0.1575 - val_loss: 3.4255\n",
      "Epoch 157/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.6273 - loss: 2.12 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.6273 - loss: 2.1279 - val_accuracy: 0.1600 - val_loss: 3.4204\n",
      "Epoch 158/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19s/step - accuracy: 0.6392 - loss: 2.11 ━━━━━━━━━━━━━━━━━━━━ 20s 20s/step - accuracy: 0.6392 - loss: 2.1172 - val_accuracy: 0.1667 - val_loss: 3.4148\n",
      "Epoch 159/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.6365 - loss: 2.11 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.6365 - loss: 2.1154 - val_accuracy: 0.1758 - val_loss: 3.4083\n",
      "Epoch 160/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.6417 - loss: 2.10 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.6417 - loss: 2.1085 - val_accuracy: 0.1825 - val_loss: 3.4026\n",
      "Epoch 161/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.6556 - loss: 2.09 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.6556 - loss: 2.0988 - val_accuracy: 0.1833 - val_loss: 3.3975\n",
      "Epoch 162/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.6494 - loss: 2.09 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.6494 - loss: 2.0968 - val_accuracy: 0.1850 - val_loss: 3.3932\n",
      "Epoch 163/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.6604 - loss: 2.08 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.6604 - loss: 2.0872 - val_accuracy: 0.1892 - val_loss: 3.3884\n",
      "Epoch 164/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.6596 - loss: 2.07 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.6596 - loss: 2.0798 - val_accuracy: 0.1967 - val_loss: 3.3828\n",
      "Epoch 165/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.6660 - loss: 2.07 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.6660 - loss: 2.0743 - val_accuracy: 0.2058 - val_loss: 3.3765\n",
      "Epoch 166/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.6550 - loss: 2.07 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.6550 - loss: 2.0707 - val_accuracy: 0.2142 - val_loss: 3.3696\n",
      "Epoch 167/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.6687 - loss: 2.06 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.6687 - loss: 2.0632 - val_accuracy: 0.2225 - val_loss: 3.3629\n",
      "Epoch 168/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19s/step - accuracy: 0.6717 - loss: 2.05 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.6717 - loss: 2.0559 - val_accuracy: 0.2300 - val_loss: 3.3571\n",
      "Epoch 169/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.6750 - loss: 2.05 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.6750 - loss: 2.0516 - val_accuracy: 0.2350 - val_loss: 3.3519\n",
      "Epoch 170/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.6756 - loss: 2.04 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.6756 - loss: 2.0442 - val_accuracy: 0.2425 - val_loss: 3.3469\n",
      "Epoch 171/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.6787 - loss: 2.03 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.6787 - loss: 2.0364 - val_accuracy: 0.2550 - val_loss: 3.3413\n",
      "Epoch 172/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.6825 - loss: 2.03 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.6825 - loss: 2.0319 - val_accuracy: 0.2567 - val_loss: 3.3348\n",
      "Epoch 173/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.6840 - loss: 2.02 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.6840 - loss: 2.0285 - val_accuracy: 0.2700 - val_loss: 3.3268\n",
      "Epoch 174/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.6858 - loss: 2.02 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.6858 - loss: 2.0214 - val_accuracy: 0.2800 - val_loss: 3.3188\n",
      "Epoch 175/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.6862 - loss: 2.01 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.6862 - loss: 2.0145 - val_accuracy: 0.2942 - val_loss: 3.3116\n",
      "Epoch 176/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.6873 - loss: 2.01 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.6873 - loss: 2.0113 - val_accuracy: 0.3008 - val_loss: 3.3052\n",
      "Epoch 177/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.6917 - loss: 2.00 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.6917 - loss: 2.0071 - val_accuracy: 0.3142 - val_loss: 3.2991\n",
      "Epoch 178/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.6940 - loss: 1.99 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.6940 - loss: 1.9980 - val_accuracy: 0.3225 - val_loss: 3.2926\n",
      "Epoch 179/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.6971 - loss: 1.99 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.6971 - loss: 1.9972 - val_accuracy: 0.3292 - val_loss: 3.2852\n",
      "Epoch 180/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.7067 - loss: 1.98 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.7067 - loss: 1.9894 - val_accuracy: 0.3442 - val_loss: 3.2776\n",
      "Epoch 181/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.7060 - loss: 1.98 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.7060 - loss: 1.9836 - val_accuracy: 0.3500 - val_loss: 3.2698\n",
      "Epoch 182/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.7050 - loss: 1.98 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.7050 - loss: 1.9822 - val_accuracy: 0.3567 - val_loss: 3.2620\n",
      "Epoch 183/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.7042 - loss: 1.97 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.7042 - loss: 1.9766 - val_accuracy: 0.3683 - val_loss: 3.2543\n",
      "Epoch 184/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.7092 - loss: 1.97 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.7092 - loss: 1.9706 - val_accuracy: 0.3833 - val_loss: 3.2463\n",
      "Epoch 185/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.7129 - loss: 1.96 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.7129 - loss: 1.9652 - val_accuracy: 0.3883 - val_loss: 3.2398\n",
      "Epoch 186/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19s/step - accuracy: 0.7119 - loss: 1.96 ━━━━━━━━━━━━━━━━━━━━ 20s 20s/step - accuracy: 0.7119 - loss: 1.9626 - val_accuracy: 0.4008 - val_loss: 3.2331\n",
      "Epoch 187/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.7179 - loss: 1.95 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.7179 - loss: 1.9560 - val_accuracy: 0.4125 - val_loss: 3.2250\n",
      "Epoch 188/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.7154 - loss: 1.95 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.7154 - loss: 1.9548 - val_accuracy: 0.4233 - val_loss: 3.2164\n",
      "Epoch 189/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.7237 - loss: 1.95 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.7237 - loss: 1.9500 - val_accuracy: 0.4308 - val_loss: 3.2078\n",
      "Epoch 190/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.7194 - loss: 1.94 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.7194 - loss: 1.9483 - val_accuracy: 0.4392 - val_loss: 3.1994\n",
      "Epoch 191/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.7262 - loss: 1.94 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.7262 - loss: 1.9421 - val_accuracy: 0.4533 - val_loss: 3.1903\n",
      "Epoch 192/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.7350 - loss: 1.93 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.7350 - loss: 1.9350 - val_accuracy: 0.4650 - val_loss: 3.1827\n",
      "Epoch 193/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.7290 - loss: 1.93 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.7290 - loss: 1.9323 - val_accuracy: 0.4750 - val_loss: 3.1750\n",
      "Epoch 194/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.7371 - loss: 1.92 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.7371 - loss: 1.9298 - val_accuracy: 0.4833 - val_loss: 3.1668\n",
      "Epoch 195/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.7346 - loss: 1.92 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.7346 - loss: 1.9233 - val_accuracy: 0.4858 - val_loss: 3.1578\n",
      "Epoch 196/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.7333 - loss: 1.91 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.7333 - loss: 1.9166 - val_accuracy: 0.4992 - val_loss: 3.1487\n",
      "Epoch 197/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.7346 - loss: 1.91 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.7346 - loss: 1.9158 - val_accuracy: 0.5100 - val_loss: 3.1406\n",
      "Epoch 198/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.7258 - loss: 1.91 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.7258 - loss: 1.9176 - val_accuracy: 0.5125 - val_loss: 3.1329\n",
      "Epoch 199/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.7329 - loss: 1.91 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.7329 - loss: 1.9113 - val_accuracy: 0.5292 - val_loss: 3.1243\n",
      "Epoch 200/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.7277 - loss: 1.90 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.7277 - loss: 1.9085 - val_accuracy: 0.5458 - val_loss: 3.1145\n",
      "Epoch 201/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.7362 - loss: 1.90 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.7362 - loss: 1.9043 - val_accuracy: 0.5525 - val_loss: 3.1056\n",
      "Epoch 202/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.7448 - loss: 1.89 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.7448 - loss: 1.8982 - val_accuracy: 0.5600 - val_loss: 3.1003\n",
      "Epoch 203/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.7400 - loss: 1.89 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.7400 - loss: 1.8981 - val_accuracy: 0.5592 - val_loss: 3.0942\n",
      "Epoch 204/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19s/step - accuracy: 0.7371 - loss: 1.89 ━━━━━━━━━━━━━━━━━━━━ 20s 20s/step - accuracy: 0.7371 - loss: 1.8933 - val_accuracy: 0.5725 - val_loss: 3.0844\n",
      "Epoch 205/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.7471 - loss: 1.88 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.7471 - loss: 1.8872 - val_accuracy: 0.5858 - val_loss: 3.0721\n",
      "Epoch 206/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.7513 - loss: 1.88 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.7513 - loss: 1.8861 - val_accuracy: 0.5900 - val_loss: 3.0626\n",
      "Epoch 207/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.7381 - loss: 1.88 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.7381 - loss: 1.8814 - val_accuracy: 0.5967 - val_loss: 3.0562\n",
      "Epoch 208/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.7369 - loss: 1.88 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.7369 - loss: 1.8821 - val_accuracy: 0.6025 - val_loss: 3.0497\n",
      "Epoch 209/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.7446 - loss: 1.87 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.7446 - loss: 1.8793 - val_accuracy: 0.6008 - val_loss: 3.0428\n",
      "Epoch 210/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.7479 - loss: 1.87 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.7479 - loss: 1.8745 - val_accuracy: 0.6042 - val_loss: 3.0331\n",
      "Epoch 211/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.7479 - loss: 1.86 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.7479 - loss: 1.8687 - val_accuracy: 0.6108 - val_loss: 3.0223\n",
      "Epoch 212/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.7550 - loss: 1.86 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.7550 - loss: 1.8688 - val_accuracy: 0.6192 - val_loss: 3.0128\n",
      "Epoch 213/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.7567 - loss: 1.86 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.7567 - loss: 1.8667 - val_accuracy: 0.6267 - val_loss: 3.0040\n",
      "Epoch 214/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.7538 - loss: 1.86 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.7538 - loss: 1.8606 - val_accuracy: 0.6275 - val_loss: 2.9955\n",
      "Epoch 215/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.7546 - loss: 1.86 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.7546 - loss: 1.8616 - val_accuracy: 0.6300 - val_loss: 2.9867\n",
      "Epoch 216/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.7525 - loss: 1.85 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.7525 - loss: 1.8570 - val_accuracy: 0.6325 - val_loss: 2.9767\n",
      "Epoch 217/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.7673 - loss: 1.85 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.7673 - loss: 1.8529 - val_accuracy: 0.6375 - val_loss: 2.9664\n",
      "Epoch 218/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.7652 - loss: 1.84 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.7652 - loss: 1.8480 - val_accuracy: 0.6483 - val_loss: 2.9572\n",
      "Epoch 219/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.7596 - loss: 1.84 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.7596 - loss: 1.8476 - val_accuracy: 0.6600 - val_loss: 2.9479\n",
      "Epoch 220/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.7690 - loss: 1.84 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.7690 - loss: 1.8413 - val_accuracy: 0.6600 - val_loss: 2.9375\n",
      "Epoch 221/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.7613 - loss: 1.83 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.7613 - loss: 1.8395 - val_accuracy: 0.6608 - val_loss: 2.9290\n",
      "Epoch 222/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.7723 - loss: 1.83 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.7723 - loss: 1.8367 - val_accuracy: 0.6550 - val_loss: 2.9216\n",
      "Epoch 223/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.7740 - loss: 1.83 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.7740 - loss: 1.8354 - val_accuracy: 0.6658 - val_loss: 2.9136\n",
      "Epoch 224/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.7660 - loss: 1.83 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.7660 - loss: 1.8308 - val_accuracy: 0.6717 - val_loss: 2.9035\n",
      "Epoch 225/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.7679 - loss: 1.82 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.7679 - loss: 1.8274 - val_accuracy: 0.6742 - val_loss: 2.8933\n",
      "Epoch 226/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.7742 - loss: 1.82 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.7742 - loss: 1.8268 - val_accuracy: 0.6758 - val_loss: 2.8844\n",
      "Epoch 227/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.7640 - loss: 1.82 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.7640 - loss: 1.8276 - val_accuracy: 0.6767 - val_loss: 2.8742\n",
      "Epoch 228/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.7627 - loss: 1.81 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.7627 - loss: 1.8197 - val_accuracy: 0.6875 - val_loss: 2.8638\n",
      "Epoch 229/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.7746 - loss: 1.81 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.7746 - loss: 1.8118 - val_accuracy: 0.6900 - val_loss: 2.8537\n",
      "Epoch 230/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.7729 - loss: 1.81 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.7729 - loss: 1.8167 - val_accuracy: 0.6883 - val_loss: 2.8444\n",
      "Epoch 231/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.7692 - loss: 1.81 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.7692 - loss: 1.8143 - val_accuracy: 0.6833 - val_loss: 2.8342\n",
      "Epoch 232/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.7750 - loss: 1.80 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.7750 - loss: 1.8081 - val_accuracy: 0.6867 - val_loss: 2.8252\n",
      "Epoch 233/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.7748 - loss: 1.80 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.7748 - loss: 1.8070 - val_accuracy: 0.6967 - val_loss: 2.8172\n",
      "Epoch 234/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.7735 - loss: 1.80 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.7735 - loss: 1.8061 - val_accuracy: 0.7083 - val_loss: 2.8091\n",
      "Epoch 235/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.7740 - loss: 1.80 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.7740 - loss: 1.8016 - val_accuracy: 0.7083 - val_loss: 2.7982\n",
      "Epoch 236/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19s/step - accuracy: 0.7727 - loss: 1.79 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.7727 - loss: 1.7982 - val_accuracy: 0.6958 - val_loss: 2.7849\n",
      "Epoch 237/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.7783 - loss: 1.79 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.7783 - loss: 1.7965 - val_accuracy: 0.7058 - val_loss: 2.7721\n",
      "Epoch 238/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.7810 - loss: 1.79 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.7810 - loss: 1.7915 - val_accuracy: 0.7125 - val_loss: 2.7614\n",
      "Epoch 239/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.7792 - loss: 1.79 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.7792 - loss: 1.7909 - val_accuracy: 0.7183 - val_loss: 2.7532\n",
      "Epoch 240/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.7875 - loss: 1.78 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.7875 - loss: 1.7864 - val_accuracy: 0.7200 - val_loss: 2.7467\n",
      "Epoch 241/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19s/step - accuracy: 0.7794 - loss: 1.78 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.7794 - loss: 1.7842 - val_accuracy: 0.7125 - val_loss: 2.7395\n",
      "Epoch 242/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.7960 - loss: 1.77 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.7960 - loss: 1.7785 - val_accuracy: 0.7175 - val_loss: 2.7301\n",
      "Epoch 243/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.7937 - loss: 1.77 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.7937 - loss: 1.7747 - val_accuracy: 0.7192 - val_loss: 2.7186\n",
      "Epoch 244/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.7806 - loss: 1.77 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.7806 - loss: 1.7778 - val_accuracy: 0.7242 - val_loss: 2.7054\n",
      "Epoch 245/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.7894 - loss: 1.77 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.7894 - loss: 1.7740 - val_accuracy: 0.7258 - val_loss: 2.6922\n",
      "Epoch 246/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.7865 - loss: 1.77 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.7865 - loss: 1.7714 - val_accuracy: 0.7225 - val_loss: 2.6821\n",
      "Epoch 247/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.7885 - loss: 1.76 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.7885 - loss: 1.7663 - val_accuracy: 0.7200 - val_loss: 2.6728\n",
      "Epoch 248/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.7919 - loss: 1.76 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.7919 - loss: 1.7648 - val_accuracy: 0.7208 - val_loss: 2.6627\n",
      "Epoch 249/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.7900 - loss: 1.76 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.7900 - loss: 1.7628 - val_accuracy: 0.7292 - val_loss: 2.6527\n",
      "Epoch 250/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.7921 - loss: 1.76 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.7921 - loss: 1.7600 - val_accuracy: 0.7300 - val_loss: 2.6427\n",
      "Epoch 251/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.7919 - loss: 1.75 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.7919 - loss: 1.7597 - val_accuracy: 0.7342 - val_loss: 2.6327\n",
      "Epoch 252/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.7852 - loss: 1.75 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.7852 - loss: 1.7581 - val_accuracy: 0.7367 - val_loss: 2.6222\n",
      "Epoch 253/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.7865 - loss: 1.75 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.7865 - loss: 1.7538 - val_accuracy: 0.7358 - val_loss: 2.6149\n",
      "Epoch 254/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.7985 - loss: 1.75 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.7985 - loss: 1.7505 - val_accuracy: 0.7317 - val_loss: 2.6044\n",
      "Epoch 255/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.7894 - loss: 1.74 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.7894 - loss: 1.7484 - val_accuracy: 0.7392 - val_loss: 2.5906\n",
      "Epoch 256/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.7998 - loss: 1.74 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.7998 - loss: 1.7438 - val_accuracy: 0.7417 - val_loss: 2.5771\n",
      "Epoch 257/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.7933 - loss: 1.74 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.7933 - loss: 1.7433 - val_accuracy: 0.7450 - val_loss: 2.5657\n",
      "Epoch 258/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 19s/step - accuracy: 0.7952 - loss: 1.74 ━━━━━━━━━━━━━━━━━━━━ 20s 20s/step - accuracy: 0.7952 - loss: 1.7410 - val_accuracy: 0.7408 - val_loss: 2.5576\n",
      "Epoch 259/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.8012 - loss: 1.73 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.8012 - loss: 1.7367 - val_accuracy: 0.7317 - val_loss: 2.5496\n",
      "Epoch 260/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.8025 - loss: 1.73 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.8025 - loss: 1.7338 - val_accuracy: 0.7367 - val_loss: 2.5381\n",
      "Epoch 261/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.8017 - loss: 1.73 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.8017 - loss: 1.7331 - val_accuracy: 0.7375 - val_loss: 2.5266\n",
      "Epoch 262/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.8065 - loss: 1.72 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.8065 - loss: 1.7287 - val_accuracy: 0.7450 - val_loss: 2.5130\n",
      "Epoch 263/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.7967 - loss: 1.73 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.7967 - loss: 1.7304 - val_accuracy: 0.7467 - val_loss: 2.5012\n",
      "Epoch 264/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.8019 - loss: 1.72 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.8019 - loss: 1.7261 - val_accuracy: 0.7392 - val_loss: 2.4944\n",
      "Epoch 265/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.8056 - loss: 1.72 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.8056 - loss: 1.7218 - val_accuracy: 0.7350 - val_loss: 2.4876\n",
      "Epoch 266/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.7987 - loss: 1.72 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.7987 - loss: 1.7211 - val_accuracy: 0.7433 - val_loss: 2.4773\n",
      "Epoch 267/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.7969 - loss: 1.71 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.7969 - loss: 1.7165 - val_accuracy: 0.7475 - val_loss: 2.4647\n",
      "Epoch 268/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.7979 - loss: 1.71 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.7979 - loss: 1.7167 - val_accuracy: 0.7517 - val_loss: 2.4518\n",
      "Epoch 269/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.8000 - loss: 1.71 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.8000 - loss: 1.7110 - val_accuracy: 0.7425 - val_loss: 2.4402\n",
      "Epoch 270/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.7960 - loss: 1.71 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.7960 - loss: 1.7143 - val_accuracy: 0.7475 - val_loss: 2.4294\n",
      "Epoch 271/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.7996 - loss: 1.71 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.7996 - loss: 1.7142 - val_accuracy: 0.7542 - val_loss: 2.4176\n",
      "Epoch 272/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.8062 - loss: 1.70 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.8062 - loss: 1.7056 - val_accuracy: 0.7525 - val_loss: 2.4072\n",
      "Epoch 273/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.7996 - loss: 1.71 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.7996 - loss: 1.7100 - val_accuracy: 0.7492 - val_loss: 2.3983\n",
      "Epoch 274/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.8108 - loss: 1.70 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.8108 - loss: 1.7024 - val_accuracy: 0.7467 - val_loss: 2.3891\n",
      "Epoch 275/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.8060 - loss: 1.70 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.8060 - loss: 1.7000 - val_accuracy: 0.7442 - val_loss: 2.3785\n",
      "Epoch 276/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.8098 - loss: 1.69 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.8098 - loss: 1.6993 - val_accuracy: 0.7542 - val_loss: 2.3670\n",
      "Epoch 277/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.8067 - loss: 1.69 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.8067 - loss: 1.6950 - val_accuracy: 0.7525 - val_loss: 2.3559\n",
      "Epoch 278/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.8054 - loss: 1.69 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.8054 - loss: 1.6939 - val_accuracy: 0.7525 - val_loss: 2.3468\n",
      "Epoch 279/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.7987 - loss: 1.69 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.7987 - loss: 1.6911 - val_accuracy: 0.7475 - val_loss: 2.3379\n",
      "Epoch 280/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.8115 - loss: 1.68 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.8115 - loss: 1.6899 - val_accuracy: 0.7408 - val_loss: 2.3300\n",
      "Epoch 281/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.8065 - loss: 1.68 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.8065 - loss: 1.6867 - val_accuracy: 0.7450 - val_loss: 2.3205\n",
      "Epoch 282/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.8087 - loss: 1.68 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.8087 - loss: 1.6886 - val_accuracy: 0.7458 - val_loss: 2.3083\n",
      "Epoch 283/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.8019 - loss: 1.68 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.8019 - loss: 1.6852 - val_accuracy: 0.7533 - val_loss: 2.2957\n",
      "Epoch 284/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.8040 - loss: 1.68 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.8040 - loss: 1.6862 - val_accuracy: 0.7633 - val_loss: 2.2834\n",
      "Epoch 285/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.8054 - loss: 1.68 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.8054 - loss: 1.6821 - val_accuracy: 0.7708 - val_loss: 2.2728\n",
      "Epoch 286/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.8081 - loss: 1.68 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.8081 - loss: 1.6810 - val_accuracy: 0.7625 - val_loss: 2.2632\n",
      "Epoch 287/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.8125 - loss: 1.68 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.8125 - loss: 1.6813 - val_accuracy: 0.7642 - val_loss: 2.2512\n",
      "Epoch 288/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.8050 - loss: 1.67 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.8050 - loss: 1.6774 - val_accuracy: 0.7700 - val_loss: 2.2405\n",
      "Epoch 289/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.8125 - loss: 1.67 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.8125 - loss: 1.6739 - val_accuracy: 0.7658 - val_loss: 2.2324\n",
      "Epoch 290/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17s/step - accuracy: 0.8044 - loss: 1.67 ━━━━━━━━━━━━━━━━━━━━ 18s 18s/step - accuracy: 0.8044 - loss: 1.6726 - val_accuracy: 0.7600 - val_loss: 2.2258\n",
      "Epoch 291/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.8121 - loss: 1.66 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.8121 - loss: 1.6697 - val_accuracy: 0.7533 - val_loss: 2.2180\n",
      "Epoch 292/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.8108 - loss: 1.66 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.8108 - loss: 1.6678 - val_accuracy: 0.7717 - val_loss: 2.2045\n",
      "Epoch 293/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.8085 - loss: 1.66 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.8085 - loss: 1.6614 - val_accuracy: 0.7725 - val_loss: 2.1925\n",
      "Epoch 294/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.8094 - loss: 1.66 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.8094 - loss: 1.6634 - val_accuracy: 0.7750 - val_loss: 2.1816\n",
      "Epoch 295/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.8129 - loss: 1.65 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.8129 - loss: 1.6593 - val_accuracy: 0.7667 - val_loss: 2.1743\n",
      "Epoch 296/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.8085 - loss: 1.66 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.8085 - loss: 1.6633 - val_accuracy: 0.7658 - val_loss: 2.1664\n",
      "Epoch 297/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.8048 - loss: 1.66 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.8048 - loss: 1.6644 - val_accuracy: 0.7667 - val_loss: 2.1565\n",
      "Epoch 298/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.8092 - loss: 1.66 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.8092 - loss: 1.6607 - val_accuracy: 0.7683 - val_loss: 2.1459\n",
      "Epoch 299/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.8075 - loss: 1.65 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.8075 - loss: 1.6593 - val_accuracy: 0.7742 - val_loss: 2.1360\n",
      "Epoch 300/300\n",
      "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18s/step - accuracy: 0.8092 - loss: 1.65 ━━━━━━━━━━━━━━━━━━━━ 19s 19s/step - accuracy: 0.8092 - loss: 1.6546 - val_accuracy: 0.7750 - val_loss: 2.1267\n",
      "\n",
      "Final Epoch - Validation Loss: 2.1267, Validation Accuracy: 0.7750\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['C:\\\\LTI 11785 Introduction to deep learning\\\\project\\\\SSVEP\\\\wandb\\\\run-20250418_135710-q0rqzwg2\\\\files\\\\eegnet_model.keras']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from wandb.integration.keras import WandbCallback\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "\n",
    "\n",
    "wandb_cb = WandbCallback(save_graph=False, save_model=False)\n",
    "\n",
    "\n",
    "class TQDMProgressBar(Callback):\n",
    "    def on_train_begin(self, logs=None):\n",
    "        self.epochs = self.params['epochs']\n",
    "        self.pbar = tqdm(total=self.epochs, desc=\"Epochs\", ncols=100)\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        val_loss = logs.get('val_loss', None)\n",
    "        val_acc = logs.get('val_accuracy', None)\n",
    "        msg = f\" val_loss: {val_loss:.4f} - val_accuracy: {val_acc:.4f}\"\n",
    "        self.pbar.set_postfix_str(msg)\n",
    "        self.pbar.update(1)\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        self.pbar.close()\n",
    "\n",
    "\n",
    "num_epochs = config['epochs_EEG']\n",
    "test_period = 1\n",
    "start_test = test_period\n",
    "alpha = 1\n",
    "beta = 0\n",
    "\n",
    "\n",
    "# Train the model and store the training history\n",
    "history = eegnet_model.fit(\n",
    "    X_train_np, y_train_np,\n",
    "    batch_size=X_train_np.shape[0],\n",
    "    epochs=num_epochs,\n",
    "    validation_data=(X_val_np, y_val_np),\n",
    "    verbose=1,  #\n",
    "    callbacks=[wandb_cb]\n",
    "    #callbacks=[TQDMProgressBar(), wandb_cb]  # ✅ use custom progress\n",
    ")\n",
    "\n",
    "# Extract training and validation history for plotting later\n",
    "train_loss = history.history['loss']\n",
    "val_loss   = history.history['val_loss']\n",
    "train_acc  = history.history.get('accuracy') or history.history.get('acc')  # depends on TF version\n",
    "val_acc    = history.history.get('val_accuracy') or history.history.get('val_acc')\n",
    "\n",
    "# Print final results\n",
    "print(f\"\\nFinal Epoch - Validation Loss: {val_loss[-1]:.4f}, Validation Accuracy: {val_acc[-1]:.4f}\")\n",
    "\n",
    "\n",
    "plot_model(eegnet_model, show_shapes=True)\n",
    "wandb.log({\"model_architecture\": wandb.Image(\"model.png\")})\n",
    "\n",
    "eegnet_model.save(\"eegnet_model.keras\")\n",
    "wandb.save(\"eegnet_model.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4117c82a-d16c-415e-9005-2edd65456e20",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 354
    },
    "id": "BzCmk5Zy_96e",
    "outputId": "efdbb9f5-c9ca-4973-9161-61b30bb6f909"
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'History' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[48], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m plt\u001b[38;5;241m.\u001b[39msubplot(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m#plt.plot(history[\"train_loss\"], label=\"Training Loss\")\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(history[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m], label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining Loss\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     10\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(history[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m], label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidation Loss\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     11\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: 'History' object is not subscriptable"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.style.use('tableau-colorblind10')\n",
    "\n",
    "# Plot training & validation loss\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history[\"train_loss\"], label=\"Training Loss\")\n",
    "plt.plot(history[\"val_loss\"], label=\"Validation Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "\n",
    "# Plot training & validation accuracy\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot([v * 100 for v in history[\"train_acc\"]], label=\"Training Accuracy\")\n",
    "plt.plot([v * 100 for v in history[\"val_acc\"]], label=\"Validation Accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy (%)\")\n",
    "plt.legend()\n",
    "\n",
    "plt.suptitle(\"Training Progress (Loss & Accuracy)\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "LGmMW-mZVOrM",
   "metadata": {
    "id": "LGmMW-mZVOrM"
   },
   "outputs": [],
   "source": [
    "def evaluate_on_loader(test_loader, name=\"test\"):\n",
    "    diffe.eval()\n",
    "    labels = np.arange(0, 26)\n",
    "    Y, Y_hat = [], []\n",
    "    with torch.no_grad():\n",
    "        for x, y in test_loader:\n",
    "            x, y = x.to(device), y.type(torch.LongTensor).to(device)\n",
    "            y_cat = F.one_hot(y, num_classes=26).type(torch.FloatTensor).to(device)\n",
    "\n",
    "            x_hat, down, up, noise, t = ddpm(x)\n",
    "\n",
    "            # Align the temporal dimension of x_hat and x\n",
    "            #x_hat, x = safe_align_2d(x_hat, x)\n",
    "\n",
    "            #ddpm_out = x_hat, down, up, t\n",
    "            #ddpm_out = x, down, up, t\n",
    "            #decoder_out, fc_out = diffe(x, ddpm_out)\n",
    "\n",
    "            # EMA update\n",
    "            #fc_ema.update()\n",
    "            #print(decoder_out.shape)\n",
    "            #X_eegnet = decoder_out.view(decoder_out.size(0), 64, -1)  # (samples, 64, 396)\n",
    "            X_eegnet = x.view(x.size(0), 64, -1)\n",
    "            #print(X_eegnet.shape)\n",
    "            outputs = eegnet_model(X_eegnet)\n",
    "\n",
    "            #encoder_out = diffe.encoder(x)\n",
    "            #y_hat = diffe.fc(encoder_out[1])\n",
    "            y_hat = F.softmax(outputs, dim=1)\n",
    "\n",
    "            Y.append(y.detach().cpu())\n",
    "            Y_hat.append(y_hat.detach().cpu())\n",
    "\n",
    "    Y = torch.cat(Y, dim=0).numpy()\n",
    "    Y_hat = torch.cat(Y_hat, dim=0).numpy()\n",
    "\n",
    "    accuracy = top_k_accuracy_score(Y, Y_hat, k=1, labels=labels)\n",
    "    print(f\" {name} Accuracy: {accuracy:.2%}\")\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xs-0xKrFvCnJ",
   "metadata": {
    "id": "xs-0xKrFvCnJ"
   },
   "source": [
    "#Evaluate on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "KVr8fRRkVRf1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KVr8fRRkVRf1",
    "outputId": "72215c66-53fc-42f0-cbe6-64751cc5e33f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Test1 (Seen Subject) Accuracy: 58.46%\n",
      " Test2 (Unseen Subject) Accuracy: 57.78%\n"
     ]
    }
   ],
   "source": [
    "acc1 = evaluate_on_loader(test1_loader, name=\"Test1 (Seen Subject)\")\n",
    "acc2 = evaluate_on_loader(test2_loader, name=\"Test2 (Unseen Subject)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "db6e70df-d1ef-4aa4-8606-85e120a00914",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>▁▁▁▂▂▂▂▂▂▂▂▃▃▄▄▄▅▅▆▆▆▇▇▇▇▇▇▇████████████</td></tr><tr><td>epoch</td><td>▁▁▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇█</td></tr><tr><td>loss</td><td>████▇▆▆▆▆▆▅▅▅▅▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▂▂▂▂▂▂▂▃▃▄▅▆▆▇█████████</td></tr><tr><td>val_loss</td><td>█████████████▇▇▇▇▇▆▆▆▆▆▆▆▅▅▄▄▄▃▃▃▂▂▂▂▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>0.80917</td></tr><tr><td>best_epoch</td><td>299</td></tr><tr><td>best_val_loss</td><td>2.12674</td></tr><tr><td>epoch</td><td>299</td></tr><tr><td>loss</td><td>1.65463</td></tr><tr><td>val_accuracy</td><td>0.775</td></tr><tr><td>val_loss</td><td>2.12674</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Ben_project_base_line_EEGNet_with_raw_data</strong> at: <a href='https://wandb.ai/yucheng_shao-carnegie-mellon-university/project_ssvep-ablations/runs/q0rqzwg2' target=\"_blank\">https://wandb.ai/yucheng_shao-carnegie-mellon-university/project_ssvep-ablations/runs/q0rqzwg2</a><br> View project at: <a href='https://wandb.ai/yucheng_shao-carnegie-mellon-university/project_ssvep-ablations' target=\"_blank\">https://wandb.ai/yucheng_shao-carnegie-mellon-university/project_ssvep-ablations</a><br>Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 2 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250418_135710-q0rqzwg2\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if USE_WANDB:\n",
    "    run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20230b2-6f7c-491a-8481-f4e8b6b01325",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (pytorch_env_p11)",
   "language": "python",
   "name": "pytorch_env_p11"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "07acb422b2b34b9791881de2513b32c7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_38a3424ad0d5460ba042960d1736b5aa",
      "max": 500,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_0a160710838b45c3950a599f2dce7836",
      "value": 500
     }
    },
    "0a160710838b45c3950a599f2dce7836": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "244fb693e8174b82aaa38b9d943ca69b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ce5b46cc0a1848f0adcdd11a1e7f95a3",
      "placeholder": "​",
      "style": "IPY_MODEL_6855930ba8b94aab9f64dbbdcddd51d2",
      "value": "Method ALL – Processing subject ALL – Val Accuracy: 10.00% | Best: 12.50%: 100%"
     }
    },
    "38a3424ad0d5460ba042960d1736b5aa": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3a0de191bd4b4018a92ada5e6e4607cf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_244fb693e8174b82aaa38b9d943ca69b",
       "IPY_MODEL_07acb422b2b34b9791881de2513b32c7",
       "IPY_MODEL_c7524cccdeea422d9ef342b142df608d"
      ],
      "layout": "IPY_MODEL_6afb371d18044cd3815fd96c9810c3a7"
     }
    },
    "6855930ba8b94aab9f64dbbdcddd51d2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6afb371d18044cd3815fd96c9810c3a7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "713d6a7190db4a54a4cd9ec1e498b1be": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9d503d51771842af8d76af60586edd19": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c7524cccdeea422d9ef342b142df608d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_713d6a7190db4a54a4cd9ec1e498b1be",
      "placeholder": "​",
      "style": "IPY_MODEL_9d503d51771842af8d76af60586edd19",
      "value": " 500/500 [15:47&lt;00:00,  1.77s/it]"
     }
    },
    "ce5b46cc0a1848f0adcdd11a1e7f95a3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
